\chapter{Extending linear mixed models to high-dimensional phenotypes}
\label{chapter:limmbo}
Different strategies and challenges for multi-trait \gls{gwas} of high-dimensional phenotypes have been discussed in \cref{subsection:joint-analysis}. Phenotypes can either be transformed into a lower dimensional space prior to the association study or the summary statistics from single-trait \gls{gwas} can be combined post-hoc to obtain quasi multi-trait association results. In contrast, multivariate \glspl{lmm} can directly model the genotypic association across a moderate number of phenotypes. In the following chapter, I will describe the challenges of multivariate \glspl{lmm} for high-dimensional phenotypes and present LiMMBo, a new method for the genotype-phenotype mapping of high-dimensional datasets. 

\glspl{lmm} have become a workhorse in genetic association studies as they allow to control for complex sample-by-sample covariance structures that can reflect population structure and relatedness (discussed in detail in \cref{subsection:lmm-genetics}). In summary, \glspl{lmm} commonly describe the phenotype as a linear combination of fixed effects -- experimental and/or technical  covariates and the genotype marker of interest, and a random genetic effect and residual noise which capture the genetic and residual covariances between traits. The association of the genetic marker is evaluated by comparing the alternative hypothesis that the genotype has an effect on the phenotype which is unequal to zero to the null model of no effect (\cref{subsubsection:model-design}). In practice, this means estimating the effect size of the fixed genetic effects and the random effect covariance terms for the alternative model and the random effect covariance terms for the null model where the effect size of the genetic marker is zero. 

The first \gls{lmm} implementations estimated all variance components (genotype ef- fect size and random effect covariance terms, \cref{eq:lmm-mv-likelihood}) anew for each \gls{snp}-phe\-notype association. However, in human genetics effect sizes are generally assumed to be small compared to the overall phenotypic variance \citep{Kang2010,Zhang2010}. Consequently, estimates of the random effect covariance terms under the null model can serve as a good approximation. Based on these differences in the estimation of the random effect covariance terms, \glspl{lmm} can broadly be grouped into two categories. The exact methods with covariance estimates under the alternative model and approximate methods, where the random effect covariance terms are only estimated once under the null model of no fixed genetic effect and are then used as predefined random effects in the alternative models for all genome-wide associations. 

Within these two categories, one can further distinguish between methods only applicable as univariate tests or methods that allow for multivariate testing. \Cref{tab:lmmframeworks} summarises commonly used frameworks and describes their computational complexity\footnote{The computational complexity and algorithms for the GCTA implementations \citep{Yang2011} of multivariate genetic variance estimation \citep{Lee2012} and \gls{lmm} for association testing \citep{Yang2014} could not be found in the original publications and are therefore not listed}.
\\
% Table generated by Excel2LaTeX from sheet 'LMMOverview'
\begin{table}[h]
  \centering
  \caption[\textbf{Linear mixed model frameworks for genetic association studies.}]{\textbf{Linear mixed model frameworks for genetic association studies.} A list of popular \gls{lmm} frameworks, grouped by their usage of covariance estimates when fitting the alternative model (first column: E: exact, A: approximate). The complexity describes the complexity for fitting a single \gls{lmm} as specified in the original publication or summarised elsewhere, as indicated by the footnotes. \(P\) indicates the trait size that the model was designed for (according to the original publication). Models with specific parameters are described in more detail in the text (FaST-LMM-select and TASSEL). \(N\): number of samples;  \(s_c\): number of SNPs used for singular value decomposition; \(c\):  compression factor with \(c=\frac{N}{g}\) for \(g\) individuals per group; \(t, t_1 \text{and} t_2\): average number of iterations needed to find parameter estimates. GRAMMAR-Gamma, FaST-LMM-select: \(t\) steps of the Brent's algorithm;  GEMMA, MTMM: \(t_1\) steps of the EM algorithm, \(t_2\) steps of the NR algorithm; BOLT-LMM: \(t\) steps of the variational Bayes and conjugate gradients; TASSEL: \(t\) steps of the ProcMixed algorithm in SAS; mtSet: \(t\) steps of the L-FBGS.}
  \begin{small}
     \begin{tabular}{llrrr}
    \toprule
     & Framework & Complexity \(O\) & \(P\) & Reference \\
    \midrule
    \multirow{3}[1]{*}{E} & FastLMM-select & \(Ns_c^2 + N^2 + tN\) & \num{1} & \citep{Lippert2011} \\
          & \multicolumn{1}{l}{\multirow{2}[0]{*}{GEMMA}} & \(N^3 + N^2P  + \)&  \multirow{2}[0]{*}{\num{10}} & \citep{Zhou2014} \\
          \addlinespace[-.2ex]
          & & \( t_1NP^2 + t_2NP^6\) & &\citep{Zhou2014} \\
    \addlinespace[3ex]
    \multirow{9}[1]{*}{A} & \multicolumn{1}{l}{\multirow{2}[0]{*}{MTMM}}  & \(t_1N^3P^3 + t_2N^3P^7 \) &  \multirow{2}[0]{*}{\num{2}} &  \multirow{2}[0]{*}{\citep{Korte2012}}\footnotemark[2] \\
                    \addlinespace[-.2ex]
          &  & \(+ N^2P^2\) & & \\
    		& EMMAX & \(N^3 + tN + N^2\) & \num{1} & \citep{Kang2010} \\
          & TASSEL & \(\frac{1}{c^3}N^3\) & \num{1} & \citep{Zhang2010} \\
          & GRAMMAR- & \multirow{2}[0]{*}{\(N^3 + tN + N\)} & \multirow{2}[0]{*}{\num{1}} & \multicolumn{1}{r}{\multirow{2}[0]{*}{\citep{Svishcheva2012}}} \\
          \addlinespace[-.5ex]
          & Gamma &       &       &  \\
          & BOLT-LMM & \(tN\) & \num{1} & \citep{Loh2014} \\
          & mtSet & \(N^3 + t(NP^4 + P^5)\) & \num{10} & \citep{Casale2015} \\
    \bottomrule
    \end{tabular}
  \end{small}
  \label{tab:lmmframeworks}%
  \vspace{-0.2cm}
\end{table}%
%
Among the exact methods, FaST-LMM-select reduces the complexity best in terms of sample size by selecting the number of \glspl{snp} to use for the estimation of the \gls{rrm}.  However, it can only be applied in univariate analyses while MTMM and GEMMA extend to multivariate cases.  BOLT-LMM scales best with increasing samples sizes in the group of approximate tests, by directly using the genotypes and not computing or storing the \gls{rrm}. All other methods have an upfront \(O(N^3)\) operation for the eigendecomposition of the \gls{rrm}. TASSEL reduces this complexity based on grouping of the samples and thereby effectively reducing the size of the \gls{rrm}.
%\citet{Eu-ahsunthornwattana2014} analysed several LMM frameworks including FaST-LMM, GEMMA and EMMAX with respect to their control for type I errors and estimation of kinships and compared the results obtained from each method. They find that the results of most methods tested are in concordance and their performance similar in terms of power and calibration when applied to real and simulated data. In conclusion, they recommend to choose a framework based on complexity and data structure requirements. 

With the generation of ever-increasing cohort sizes in genetic association studies, most \gls{lmm} frameworks are optimised for the number of samples as described above for BOLT-LMM and TASSEL. While the remaining methods still have the upfront cubic computation of the  \gls{rrm}'s eigendecomposition, subsequent steps have been adapted to scale linearly or quadratically with the number of samples for the majority of the applications.

The reduced complexity in the sample term comes as a trade-off with the number of traits that can be analysed. 
Specifically, computations become prohibitive as soon as a few tens of traits  (\cref{tab:lmmframeworks}, column P) are considered, with computational complexities ranging from \(O(P^5)\) to up to \(O(P^7)\) for existing methods \citep{Casale2015,Korte2012}. In practice, this limits these models to moderate trait numbers\footnotetext[2]{Listed in \citep{Zhou2014}}.

To overcome this limitation, I developed a simple, but surprisingly effective heuristic to efficiently estimate large trait covariance matrices in \gls{limmbo}, thereby allowing for the analysis of datasets with a large number of phenotypic traits. \gls{limmbo} and its application (\cref{chapter:yeast}) is currently under revision and available in pre-print \citep{Meyer2018a}. 
I conducted all simulations and analyses and generated all results. I provide \gls{limmbo} as an open source Python package (\url{https://pypi.org/project/limmbo/}) with command line interface and its source code is available on github: \url{https://github.com/HannahVMeyer/limmbo}.

\section{LiMMBo: Linear mixed modeling with bootstapping}
\label{section:intro-limmbo}
To extend the range of \glspl{lmm} for high-dimensional phenotype sets, I chose to build on an approximate model in order to avoid the repeated estimation of the trait-by-trait covariance matrices. In that respect, the multivariate \gls{lmm} developed by Lippert, Casale and colleagues \citep{Lippert2014,Casale2015} harboured many advantages. It is computationally efficient for a moderate number of traits, has successfully  been used in multi-trait studies \citep{Cannavo2016,Schor2017} and collaboration with its developers was easily realisable. 
Their model is cast as
\begin{equation}
\mat{Y} = \mat{G} + \mat{\Psi},
\label{eq:vd-mn}
\end{equation}
%
where the \(N \times P\) phenotype matrix \tmat{Y} for \(N\) individuals and \(P\) traits is modelled as the sum of a genetic (or polygenic) component \tmat{G} and a noise component \tmat{\Psi} (I have omitted additional fixed effects for notational brevity). Here, \tmat{G} and \tmat{\Psi} are random effects following matrix normal distributions:
\begin{equation}
\begin{aligned}
\mat{G} &\sim\mathcal{MN}_{N, P}\left(0, \mat{R}, \matsub{C}{g}\right) \\
\mat{\Psi} &\sim\mathcal{MN}_{N, P}\left(0,  \matsub{I}{N}, \matsub{C}{n}\right),
\end{aligned}
\label{eq:mvn}
\end{equation}
%
where \tmat{R} denotes the \(N \times N\) genetic relationship matrix, \tmatsub{I}{N} is the \(N \times N\) identity matrix and \tmatsub{C}{g} and \tmatsub{C}{n} are the genetic and the residual \(P \times P\) trait covariance matrices, respectively. The marginal likelihood of the model in \cref{eq:vd-mn} can be expressed in terms of a multivariate normal distribution of the form
\begin{equation}
p\left(\mat{Y} \rvert \matsub{C}{g},  \mat{R}_N, \matsub{C}{g}\right) = \mathcal{N}\left(\text{vec}\left(\mat{Y}\right) \rvert 0,\matsub{C}{g} \otimes \mat{R}_N + \matsub{C}{n} \otimes \matsub{I}{N}\right), 
\label{eq:vd}
\end{equation}
%
where the covariance structure of the phenotypes (in shape of the \(N \times P\) phenotype vector \(\text{vec}\left(\mat{Y}\right) \) through stacking the columns of the phenotype matrix) is described by the sum of the Kronecker products $\otimes$ of the sample and trait covariance terms. This model enables efficient inference schemes by exploiting Kronecker identities for the eigendecomposition of the full covariance matrix~\citep{Lippert2014,Rakitsch2013,Zhou2014,Casale2015}. 
In particular, it allows for decoupling the decomposition of $\matsub{C}{g}$ and $\mat{R}_N$, which greatly increase the efficiency of the inference as $\matsub{R}{N}$ is constant.
The model in \cref{eq:vd-mn} also corresponds to the null model when using the multi-trait LMM for genetic association mapping. 

The complexity of this multivariate \gls{lmm} implementation (from now referred to as \textquote{standard REML}) is \(O(N^2 + t(NP^4 + P^5))\) with \(N\) the number of samples, \(P\) the number of traits, and \(t\) the number of iterations of Broyden's method, which uses an approximation of the second derivative for optimising the \gls{reml} of the parameter estimates. From this equation, it becomes evident that as the number of traits increases, the complexity increases steeply and explains why this \gls{lmm} set-up is not feasible for large trait sets (as is the case for other inference schemes \cref{tab:lmmframeworks}). To overcome the bottleneck of estimating the trait-by-trait covariance matrices, I developed a simple method that efficiently uses a subsampling approach to estimate \tmatsub{C}{g} and \tmatsub{C}{n}.