\section{Covariance estimation via bootstrapping}
\label{section:bootstrapping-limmbo}
The key innovation of LiMMBo is to perform the variance decomposition on \(b\) bootstrap samples of \(s\) traits instead of on the whole dataset, and use those bootstrap samples to reconstruct the full \tmatsub{C}{g} and \tmatsub{C}{n} matrices (\cref{fig:vd}). In detail, from the total phenotype set with \(P\) traits, \(b\) subset of \(s\) traits are randomly selected. \(b\) depends on the overall trait size \(P\) and the sampling size \(s\) and is chosen such that each two traits are drawn together at least \(c\) times (default: 3). For each subset, the variance decomposition is estimated via the null model of the mvLMM, i.e. without the fixed genetic effect \(\mathbf{x}\) (\cref{eq:mvLMM} and \cref{eq:vd}) and the $s \times s$ covariance matrices \tmatsupsub{C}{s}{g} and\tmatsupsub{C}{s}{n} recorded.  For each trait pair, their covariance estimate is averaged over the number of times they were drawn. The challenge lies in combining the bootstrap results in such a way, that the resulting \tmatsub{C}{g} and \tmatsub{C}{n} matrices are true covariance matrices i.e. positive semi-definite and serve as good estimators of the true covariance matrices. This is achieved by fitting (least-squares estimate) the covariance estimates of the \(b\) subsets to the closest positive-semidefinite matrices via the Boyden-Fletcher-Goldfarb-Shanno algorithm (BFGS)\citep{Byrd1995}, using the average estimates to initiate the matrices. 

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=\textwidth]{Chapter2/Figures/LiMMBoScheme.pdf}
	\caption[\textbf{Variance decomposition.}]{\textbf{Variance decomposition.} On the left-hand side, the phenotype set of \(P\) traits and \(N\) samples is decomposed into its \(P \times P\) trait-to-trait covariances \tmatsub{C}{g} and \tmatsub{C}{n}, based on the provided genetic sample-to-sample kinship estimate matrix \tmat{R}. The noise sample-to-sample matrix is assumed to be constant (Identity matrix). Standardly, this is done by restricted maximum likelihood estimation of the null model of the mvLMM (Eq.~\ref{eq:vd}). However, this direct variance decomposition (VD) via REML only works for moderate number of phenotype sizes. For higher trait-set sizes, LiMMBo serves as an alternative to the standard REML (right-hand side). Here, the phenotypes' variance components are estimated on \(b\) \(s\)-sized subsets of \(P\) which are subsequently combined into the overall \(P \times P\) covariance matrices \tmatsub{C}{g} and \tmatsub{C}{n}.} 
	 	\label{fig:vd}
\end{figure}


\section{Data simulation}
\label{section:data-limmbo}
I simulated a number of different phenotype datasets to evaluate LiMMBo in terms of scalability, model calibration and power. The datasets differed in their overall trait size \(P\), the percentage of variance explained by genetics \(h_2\) (sum of fixed and random genetic effects) and the number of different phenotype components simulated to create the final phenotype. The phenotypes were simulated as described in \cref{section:simulation}, based on the parameters and parameter values described in \cref{tab:pardescription} and \cref{tab:parvalues}. Parameter values were chosen based on \red{...}.

% Table generated by Excel2LaTeX from sheet 'SimulateDataParameters'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Parameters for phenotype simulation.}]{\textbf{Parameters for phenotype simulation.} The genetic and noise effects have both a random and fixed effect. For each, the total variance is the sum of their random and fixed effect variance and has to add to 1. Each fixed and random component has a certain percentage of its variance that is shared across traits, while the rest is independent.}
    \begin{tabular}{clrrr}
    \toprule
          &       & variance explained & shared & independent \\
    \midrule
    \multirow{3}[1]{*}{genetic effects} & total & \(h_2\) &       &  \\
          & fixed & \(h_2^s\) & \(\theta\) & 1-\(\theta\) \\
          & random & \(h_2^g\) & \(\eta\) & 1-\(\eta\) \\
   \addlinespace[1.5ex]
    \multirow{3}[1]{*}{noise effects} & total & (1-\(h_2\)) &       &  \\
          & fixed & (1-\(h_2\))\(\delta\) & \(\gamma\) & 1-\(\gamma\) \\
          & random & (1-\(h_2\))(1-\(\delta\)) & \(\alpha\) & 1-\(\alpha\) \\
    \bottomrule
    \end{tabular}%
  \label{tab:pardescription}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'SimulateDataValues'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Parameter values of simulated phenotypes for assessing scalability, calibration and power.}]{\textbf{Parameter values of simulated phenotypes for assessing scalability, calibration and power.} The `genotype' parameter specifies the simulated genotype cohort which was used to simulate fixed and random genetic effects (described in \cref{subsection:genotypes}). \(P\) are the different trait set sizes that were simulated. The parameters that follow are described in \cref{tab:pardescription} and specify the variance explained by each of the phenotype components. A variance explained equals zero means that this component was not simulated and corresponding non-applicable variance terms are designated with `-'.} 
    \begin{tabular}{lrr}
    \toprule
          & \multicolumn{2}{c}{Parameter values} \\
\cmidrule{2-3}    Parameter & Power  & Calibration \\
    \midrule
    \multicolumn{1}{c}{\multirow{3}[1]{*}{Genotypes}} & \multicolumn{1}{c}{\multirow{3}[1]{*}{relatedNoPopstructure}} & relatedNoPopstructure \\
          &       & unrelatedNoPopstructure \\
          &       & unrelatedPopstructure \\
          \addlinespace[1.5ex]
    \(P\) & 10, 50, 100 & 10, 20, \(\ldots\), 100 \\
   \addlinespace[1.5ex]
    \(h_2^s\) & 0.48, 0.3, 0.12 & 0 \\
    \(h_2^g\) & 0.32, 0.2, 0.08 & 0.8, 0.5, 0.2 \\
    \(h_2\) & 0.8, 0.5, 0.2 & 0.8, 0.5, 0.2 \\
    (1-\(h_2\))\(\delta\) & 0.08, 0.2, 0.32 & 0 \\
    (1-\(h_2\))(1-\(\delta\)) & 0.12, 0.3, 0.48 & 0.2, 0.5, 0.8 \\
    (1-\(h_2\)) & 0.2, 0.5, 0.8 & 0.2, 0.5, 0.8 \\
    \(\theta\) & 0.6   &  - \\
    \(\eta\) & 0.8   & 0.8 \\
    \(\gamma\) & 0.6   &  - \\
    \(\alpha\) & 0.8   & 0.8 \\
    \bottomrule
    \end{tabular}%
\label{tab:parvalues}%
\end{table}%

\section{Scalability of LiMMBo}
\label{section:scalability-limmbo}
The complexitiy of the variance decomposition of the LMM framework that LiMMBo builds on is \(O(N^2 + t(NP^2 + NP^4))\). The second term depends on the overall trait size and describes the complexity of estimating the trait-by-trait covariance matrices  \tmatsub{C}{g} and  \tmatsub{C}{n}. By bootstrapping \(s\)-sized samples from the overall trait size, this complexity term changes to \(bt(Ns^2 + Ns^4)\), with the covariance estimation carried out for \(b\) bootstraps. In addition to the estimation of the covariance terms, the overall complexity of LiMMBo also depends on the fitting the BFGS algorithm \(n\) times to the full traitset of size \(P\). LiMMBo makes use of a cholesky decomposition of the matrices to be fitted, resulting in $\frac{1}{2}P(P+1)$ model parameters to be fitted for both  \tmatsub{C}{g} and  \tmatsub{C}{n}. Thus, the overall complexity of LiMMBo is \(O(N^2 + bt(Ns^2 + Ns^4) + n(\frac{1}{2}P(P+1))\), which is the sum of the complexity of the bootstrap variance decompositions and the complexity of fitting the BFGS algorithm.  

In order to assess and compare how LiMMBo scales, I performed VD both with LiMMBo and the standard REML approach on phenotypes with trait sizes ranging from \numrange{10}{100} traits (parameters for phenotype simulation as described in \cref{tab:parvalues}, total of ten simulated datasets per setup). For phenotypes with \num{10} traits, the sampling datasize \(s\) was set to \num{5}, otherwise  \(s=10\).  \Cref{fig:proctime} shows the overall time taken by the standard REML approach, LiMMBo and its two main components, the bootstrapping and the combination of the bootstrap results. The majority of the run time of LiMMBo is taken by the VD of the bootstrapped subsets, which accounts for at least \num{85}\%  (\num{70} traits) and on average \num{97}\%  of the total runtime. As a comparison, the time taken by the standard REML approach quickly exceeds the time of LiMMBo and becomes unfeasable for more than \num{30} traits. 

I implemented LiMMBo as a python module where the individual variance decomposition estimations can be distributed across multiple cores on a single computer or multiple nodes on a computing network via the \textit{Parallel Python Software} \citep{PPSoftware}, allowing for the parallelisation of the most time-consuming step. LiMMBo can be freely accessed via \red{my github/PMB github/python repo?}.


\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1\textwidth]{Chapter2/Figures/proctime.pdf}
	\caption[\textbf{Scalability of LiMMBo  compared to standard REML.}]{\textbf{Scalability of LiMMBo  compared to standard REML}. Empirical run times for LiMMBo and the standard REML approach on three simulated datasets per phenotype size, with \(N=1000\) individuals each and different amount of variance explained by the genetic background signal (\num{0.2}, \num{0.5}, \num{0.8}). The boxplots summarise the results across the different set-ups. Lines were fitted for the bootstrapping step (orange): \(n(s^2 + s^4)\); the combination of the bootstrapping (blue): \(\frac{1}{2}P(P+1)\) and their combined runtime (turquoise):  \(n(s^2 + s^4) + \frac{1}{2}P(P+1)\). \(b\): number of boostraps, \(s\): bootstrap size, \(P\): phenotype size. The majority of the runtime is required for the bootstrapping. The runtime for the standard REML results (red) are only depicted up to \(P=40\) when they already exceed the runtimes for \(P=100\) in the LiMMBo approach. The complexity of the REML and LiMMBo algorithm (or their reseptive parts) were used for fitting the lines to the observed data: sum across boostraps: \(bt(Ns^2 + Ns^4)\); combining of bootstraps: \(n(\frac{1}{2}P(P+1))\). total runtime of LiMMBo: \(O(N^2 + bt(Ns^2 + Ns^4) + n(\frac{1}{2}P(P+1))\); REML: \(O(N^2 + t(NP^2 + NP^4))\). \red{Need to doublecheck with Paolo, since the line fit for the REML is perfect when assuming \(O(N^2 + t(NP^2 + NP^6))\) but does not work when fitting to the power of \num{4}.}}
	 	\label{fig:proctime}
\end{figure}

\section{LiMMBo yields covariance estimates consistent with REML estimates for moderate trait numbers}
\label{section:covariance-limmbo}
I evaluated the suitability of LiMMBo for covariance estimation of \tmatsub{C}{g} and \tmatsub{C}{n} on simulated datasets with different strength of background genetic effects. I choose to simulate traitset sizes between ten and thirty traits, as this is the regime where the standard REML approach is still feasible and allows for a comparison of the two methods. I simulated phenotype sets composed of background genetic effects \tmat{G} and noise effects \tmat{\Psi} only, omitting any specific genetic effects (additional parameters as described in \cref{tab:parvalues}) and estimated these variance components subsequently with LiMMBo and standard REML. Variance estimation on simulated datasets allows for the comparison of the estimated covariance matrices to the true covariance matrices based on which the phenotypes were simulated. By computing the root mean squared deviation (RMSD) between the true and estimated covariance matrices from both methods, I obtain a measure that is directly comparable and is independent of the traitset size: 
\begin{equation}
\text{RMSD}=\sqrt{\frac{\sum_{t=1}^n (C_{\text{true}} - C_{\text{estimate}})^2}{n}}
\label{eq:rmsd}
\end{equation}
\Cref{fig:covsimilarity} shows the comparison of both standard REML and LiMMBo-derived covariance matrices compared to the simulated, true covariance matrices. Both methods provide consistent estimates across trait sizes with little difference between the methods.

\begin{figure}[hbtp]
	\centering	
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.6\textwidth]{Chapter2/Figures/covarianceSummary.pdf}\\
	\caption[\textbf{Comparison of trait-by-trait covariance estimates derived from standard REML and LiMMBo.}]{\textbf{Comparison of trait-by-trait covariance estimates derived from standard REML and LiMMBo.} For moderate trait set sizes ranging from \num{10} to \num{30} traits, phenotypes with different percentage of variance explained by genetics were simulated. The genetic and noise trait-to-trait covariance matrices \tmatsub{C}{g} and \tmatsub{C}{n} were then estimated both via LiMMBo and standard REML. These estimates were compared to the true (simulated) covariance matrix by computing their root mean squated deviation (RMSD; \cref{eq:rmsd}). The boxplots summarize the RMSD across different variance levels. Across all trait sizes, LiMMBo yields covariance estimates with RMSD consistent to the REML approach.}
	  \label{fig:covsimilarity}%
\end{figure}

\section{mtGWAS with LiMMBo-derived covariance matrices are well calibrated across all phenotype sizes}
\label{section:calibration-limmbo}
One key aspect in statistical method development is to ensure that the method is well-calibrated under the null model. Apart from gaining knowledge about the genetic and noise trait-to-trait covariance structure of a phenotype, variance decompostion into different random effect components yields estimates that can be supplied as known parameters to approximate mvLMM methods and multi-trait GWAS (mtGWAS). As introduced by Jiang and colleagues \citeyear{Jiang1995} and adapted by Korte and colleagues \citeyear{Korte2012}, there are different model designs for mvLMM, depending on the underlying biological hypothesis regarding the effect of the genetic variant. In the most simple case, one can test if the genetic variant has an effect on any of the traits \(P\) (any effect test) i.e. the effect size of the fixed effect is unequal to zero for at least one trait : \(H_\text{A}: \mat{\beta} \ne \mat{0}_P\).  In this \(P\)-degrees of freedom (df) test, the corresponding null hypthesis of no association is that the effect size of the fixed effect is equal to zero: \(H_0:\mat{\beta}  = \mat{0}_P\). In the common effect model, the variant has the same effect size across all traits (\(\mat{\beta}  = \mat{1}_P\beta)\) and is tested for significance in a \num{1} df model versus the null hypothesis of no association (\(\mat{\beta}  = \mat{0}_P\)). A more complicated model allows to test for specific effects of the variant on a given trait \(p\). This can be tested with a \num{1} df test where a model containing a common effect across all traits and a specific effect for trait \(p\) is compared against the common effect model. Both, for the calibration and power analysis I chose to apply an any effect test.

In order to test if LiMMBo-derived covariance estimates yield well calibrated test statistics, I simulated phenotype sets composed of random genetic and noise effects only with \num{10}, \num{20}, \num{30}, \num{50} and \num{100} traits and parameters \cref{tab:parvalues}. For trait sizes of up to thirty traits, I compared the calibration of mtGWAS for LiMMBo- and REML-derived covariance matrices. As shown in \cref{fig:calibration}, both methods show sufficient calibration across all phenotype sizes and variance explained by genetics. 

For higher trait sizes, I compared the calibration of mtGWAS using a mvLMM to a simple multi-variate linear model (mvLM, Eq.~\ref{eq:mvLM}). The mvLM does not require the variance decompostion into different random effects, i.e. avoids the computational bottleneck, but simply uses pricipal components of the genotypes as fixed effects to adjust for population structure. I assessed the calibration of the mvLM and mvLMM in mtGWAS by testing all simulated SNPs against the different phenotypes and subsequently estimating the type I error rates. For each mtGWAS, I counted the number of tests that exceeded a given threshold divided by the overall number of tests conducted, hence the number of genome-wide SNPs. I tested for calibration at two significance thresholds, \num{5e-5} and \num{5e-8}.  The latter is the common GWAS significance threshold, which is based on the number of independent variants in the genome. It was first proposed in the scope of the HapMap project \citeyear{HapMap2005} who found about \num{150} independent, common variants per \num{500}~kb region. At a significance threshold of \(p < 0.05\) and extrapolated to the genome size of \(\sim 3.3\)~Gb, by Bonferroni correction this yields  \(0.05/(\frac{150}{500\text{kb}} \times 3.3 \text{Gb}= 5.05 \times 10^{-8}\). This estimate was later confirmed in a study using different methods for estimating the number of independent variants \citep{Fadista2016}. \Cref{tab:calibration} shows the type I error estimates for both mtGWAS approaches. The mvLMM performs well across all trait sizes and thresholds of significance. In contrast, the mvLM is poorly calibrated and clearly demonstrates the difficulty of adjusting for population structure via fixed effects in highly structured populations.  
 
\begin{figure}[hbtp]
	\centering	
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=\textwidth]{Chapter2/Figures/calibrationSummaryQQAll}\\
	\caption[\textbf{Calibration of mtGWAS based on covariance estimates from standard REML and LiMMBo.}]{\textbf{Calibration of mtGWAS based on covariance estimates from standard REML and LiMMBo.} Phenotypes  ranging from \num{10} to \num{30} traits with different percentage of variance explained by genetics were simulated. The genetic and noise trait-to-trait covariance matrices \tmatsub{C}{g} and \tmatsub{C}{n} were then estimated both via LiMMBo and standard REML and used as estimates for mvLMM across all genome-wide SNPs. Both methods show a unifrom distribution of observed p-values across all phenotype sizes and variance explained by genetics }
	  \label{fig:calibration}%
\end{figure}


% Table generated by Excel2LaTeX from sheet 'CalibrationSummary'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Type I error estimates for mtGWAS.}]{\textbf{Type I error estimates for mtGWAS.} Type I errors estimates for mvLMM and mvLM across all genome-wide SNPs for three trait set sizes assesed at two different levels of significance.  For the mvLMM,  covariance estimates were derived via LiMMBo. In the mvLM, population structure was adjusted for via the first \num{10} PCs of the genotype data. The mvLMM controls well for Type I errors at both thresholds, while the mvLM leads to inflated test statistics.}
    \begin{tabular}{crrr}
    \toprule
          &       & \multicolumn{2}{c}{Type I Error estimates} \\
\cmidrule{3-4}    \multicolumn{1}{r}{Traits} & Significance level & mvLM & mvLMM \\
    \midrule
    \multirow{2}[1]{*}{10} & \num{5e-05} & \num{2.17e-03} & \num{4.51e-05} \\
          & \num{5.00e-08} & \num{2.32e-05} & \(<\)\num{5e-08} \\
             \addlinespace[0.5ex]
    \multirow{2}[0]{*}{50} & \num{5.00e-05} & \num{1.09e-02} & \num{1.93e-05} \\
          & \num{5.00e-08} & \num{2.08e-04} & \(<\)\num{5e-08} \\
             \addlinespace[0.5ex]
    \multirow{2}[1]{*}{100} & \num{5.00e-05} & \num{3.17e-02} & \num{2.28e-05} \\
          & \num{5.00e-08} & \num{1.01e-03} & \num{4.56e-08} \\
    \bottomrule
    \end{tabular}%
  \label{tab:calibration}%
\end{table}%

\section{Multi-trait genotype to phenotype mapping increases power for high-dimensional phenotypes}
\label{section:power-limmbo}

Multi-trait LMM models for low to moderate phenotype sizes have been shown to improve power by leveraging correlated background structure and trait-trait correlations resulting thereof \cite{Casale2015}. For assessing the significance of the genotype-phenotype association via likelihood-ratio (LLR) test statistics where the likelihood of the full model is compared to the likelihood of the null model i.e. without the fixed genetic effect, the LLR statistic are translated into p-values via the appropriate \(\chi^2\) distribution with \(P\) degrees of freedom \cite{Wilks1938}.  In order to test if there is still a gain in power for a mvLMM with high-dimensional phenotypes, i.e. large number of degrees of freedom, I simulated phenotypes where I varied key parameters whose influence on power I wanted to investigate. 

I varied trait numbers (\(P\=\)\numlist{10;50;100}, genetic background contribution to the phenotypic variance (\(h_2=\)\numlist{0.2;0.5;0.8} and proportion of traits that are affected by the fixed genetic effects (\(a=\)\numrange{0.2}{1}). I simulated the phenotypes composed of random genetic and fixed and random noise effects with parameters described in \cref{tab:pardescription} and \cref{tab:parvalues}. For each of these phenotype sets, I added \num{20} fixed genetic effects to a subset of traits, creating phenotypes with different proportions of traits affected by the fixed effect genetics.  For each set-up, I simulated \num{50} independent phenotypes (a total of \( 2,250 \text{ phenotypes} = 3\text{ }h_2 \times 3 \text{ trait sizes} \times 50 \text{ permutations} \times 5 \text{subset sizes}\)) and estimated the trait-trait covariance matrices \tmatsub{C}{g} and \tmatsub{C}{n} via LiMMBo. I used these estimates in a mvLMM to test the association between the known causal SNPs (from the simulation) and the phenotypes. In addition, I determined the association of the causal SNPs for each trait independently via uvLMMs. The significance of the associations was assessed by comparing the p-values of these original associations to p-values obtained from mvLMM and uvLMM on \num{1000} permutation of the genotypes.  For the uvLMM,  the p-values were adjusted for multiple testing by the number of traits that were tested and the minimum adjusted p-value across all traits for a given SNP recorded. For each SNP, the number of times the (adjusted) p-value of the permutation was less or equal to the observed p-value was recorded and divided by the total number of permutations, yielding an empirical p-value per SNP.

I compared the results of the uni-variate and multi-variate models to evaluate \num{2} key differences in the models. First, I can test which burden of multiple association testing weights heavier, the correction for multiple testing in the uvLMM or the increased degrees of freedom in the mvLMM. This effect can be analysed by varying the number of traits in the phenotypes and keeping the other parameters constant. As depicted in \cref{fig:power}\subfig{A}, for the highest number of phenotypes tested, both models are comparable in the number of causal SNPs they detect. For the other trait sizes testes, the multi-variate model out-performs the uni-variate model by far. For these comparisons, an ideal scenario was assumed and all traits were affected by the fixed genetics affects (\(a=1\) and the total genetic variance was low (\(h_2=0.2\)). The influence of the proportion of traits affected by the causal SNPs on the power to detect these is depicted in \cref{fig:power}\subfig{B}. This analysis allows for the evaluation of the second key difference in the models. The multi-variate model can exploit correlated background structure and allows for the detection of pleiotropic effects, while the uni-variate model can only detect simple SNP-trait associations. This advantage becomes clear in \cref{fig:power}\subfig{B}, where the median numer of dected true SNPs increases with increasing proportions of traits affected by the causal SNPs. \red{The number of SNPs detected in the uni-variate model remains constant} Here, the number of traits and total genetic variance were kept constant at \(P=50\) and \(h_2=0.2\). The influence of the proportion of phenotypic variance explained by all genetic, i.e. fixed and random genetic effects is shown in \cref{fig:power}\subfig{C}. For both models, the number of detected SNPs decreases with increasing \(h_2\), as the effect sizes of the SNPs become negligable compared to the overall genetic variance. However, the multi-variate model is still able to exploit the correlation of the fixed effects across traits and detectes more SNPs in regimes of high  \(h_2\). An overview of all parameter comparisons can be found in \red{Supp fig}. 
 
\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.9\textwidth]{Chapter2/Figures/power.pdf}
	\caption[\textbf{Power comparison for mvLMM and uvLMMs of high-dimensional phenotypes.}]{\textbf{Power comparison for mvLMM and uvLMMs of high-dimensional phenotypes.} Each panels show the influence of one simulation parameter on the power to detect the causal SNPs. When investigatin one parameter, the other parameters were fixed at a certain value. A. Influence of the number of traits: proportion of traits affected and the total genetic variance fixed at \(a=1\) and \(h_2=0.2\), respectively. B. Influence of proportion of traits affected: trait size and total genetic variance fixed to \(P=50\) and \(h_2=0.2\) repectively. C. Influence of total genetic variance:  trait size and  proportion of traits affected fixed to \(P=100\) and \(a=0.6\).} 
 	\label{fig:power}
\end{figure}

