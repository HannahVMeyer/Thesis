\chapter{Extracting low-dimensional representations from very high-dimensional data}
\label{chapter:DimReduction}
In biological and medical research, samples are often phenotyped for more than one trait. In simple cases, multiple phenotypes are related measurements such as length, width and circumference of plant leafs or measurements commonly regarded as covariates for instance height or weight. In molecular phenotyping such as gene expression or metabolite profiling, several hundred or thousand measurements are collected simultaneously.  For image-based phenotype methods like CT scans, functional MRI or high-resolution microscopy, each pixel or voxel can be considered a different meassurement. One property that such very high-dimensional datasets often have in common is that they occupy an intrinsically lower-dimensional space or manifold which is embedded in the observed, high-dimensional space. Low dimensional representations of gene expression meassurements might reflect common pathways or transcriptional profiles and image-derived phenotypes could reflect organ shape variation, disease status or activity scores (in fMRI).  Dimensionality reduction techniques aim to provide a meaningful low-dimensional representation \tmat{Z} of the high-dimensional data \tmat{X} while only losing minor amounts of information:
\begin{equation}
X \inR N P \xrightarrow{\text{DimReduction}} Z \inR N K,
\end{equation}
while using only a small number of free parameters and preserving the quantities of interest in the data. Depending on the algorithm employed, these might be local proximities or global structure. There are two key questions when applying dimensionality reduction techniques to very high-dimensional datasets: i) the type of technique to use and ii) the criterion to classify the low-dimensional space as a good representation of the original data. 

In this chapter, I will first present a small review of current dimensionality reduction methods. I will use these methods to demonstrate the application of dimensionality reduction for visualisation on small datasets with known structure and compare the visual results to two criteria for measuring the quality of dimensionality reduction. I will then describe the results of the different dimensionality reduction techniques on simulated high-dimensional datasets and show that low-dimensional representations of a phenotype can capture underlying genetic structure. 

\section{Review of dimensionality reduction methods}
\label{section:DimReduction-methods}
The earliest dimensionality reduction techniques were two linear methods based on spectral decomposition: principal component analysis (PCA) and classical multi-dimensional scaling (MDS). 

The general concept of PCA was initially described by Pearson in 1901 \citep{Pearson1901}. In 1933, Hotelling was the first to describe it as a method for dimensionality reduction \citep{Hoteling1933}. In PCA, the components of the new phenotype representation are called principal components and are the eigenvectors \tmat{W} of the empirical covariance matrix \tmat{C}:  \(\mat{C}=\mat{X}\mat{X}^T = \mat{W}\mat{\Lambda}\mat{W}^T\). The eigenvalues in the diagonal matrix \(\mat{\Lambda}\) corresponding to the principal components are equivalent to the variance explained by their components. The transformation of the phenotype data into principal components leads to a projection where the highest amount of phenotypic variance explained lies in the first component, the second highest variance in the second component and so forth. The dimensionality reduction is achieved by using the first \(K\) principal components until the cumulative sum of the eigenvalues reaches a predefined threshold of total phenotypic variance that should be retained: \(\mat{Z}={\matsub{W}{1}, \dots, \matsub{W}{K}}\). 
 
MDS was introduced by Gower in 1966, motivated by his dissatisfaction about the overuse of PCA in biology  \citeyear{Gower1966}. MDS is based on the spectral decomposition of a dissimilarity matrix \tmat{D} between the samples in \tmat{X}. Classical MDS finds the low-dimensional representation \tmat{Z} whose pairwise distance matches the dissimilarity \(d_{ij}\) of the original data: \(d_{ij} \approx \hat{d_{ij}} = \lVert \matsub{z}{i} - \matsub{z}{j}\rVert \). \tmat{Z} can be found by the eigendecomposition of the squared dissimilarity matrix \(\matsup{D}{2} = \mat{V}\mat{\Lambda}\matsup{V}{T}\), where \(\mat{Z}=\mat{\Lambda}^\frac{1}{2}\mat{V}^T\). As in PCA, \tmat{Z} will be ordered with the components explaining most variance ranked first and dimensionality reduction can be achieved by selecting the first \(K\) vectors \citep{Gower1966}. MDS finds an embedding that preserves the interâ€point distances and is equivalent to PCA when those distances are Euclidean.

Several decades after the introduction of PCA as a means of linear dimensionality reduction, Schoelkopf and colleagues proposed its non-linear extension based on the transformation of \tmat{X} into a feature space \tmat{F} via the mapping function  \(\Phi\). Instead of finding the eigendecomposition of the covariance matrix of \tmat{X}, the aim is the diagonalisation of the covariance \tmat{K} of the features of the data \(\Phi(\mat{X})\): \(\mat{K} = \Phi(\matsub{x}{i})\Phi(\matsub{x}{i})^T\). Using the kernel representation \(k(\matsub{x}{i}, \matsub{x}{i}^T) = (\Phi(\matsub{x}{i})\Phi(\matsub{x}{i})^T)\) to compute the dot products of \(\Phi(\matsub{x}{i})\Phi(\matsub{x}{i})^T\), allows to compute the dot product in \tmat{F} without having to carry out the map \(\Phi\). This technique is commonly refered to as the kernel trick and yields the feature covariance matrix \(\matsub{K}{ij} = k(\matsub{x}{i}, \matsub{x}{i})_{ij}\). The normalised eigenvectors of \tmat{K} are used to extract the principal components. This kernel PCA (kPCA) approach allows for non-linear feature extraction and the possibility to select different kernels (e.g. gaussian or sigmoid) makes it applicable for a wide range of non-linearity assumptions \citep{Schoelkopf1998}.

PCA, kPCA and MDS build the basis for many other dimensionality reduction techniques. Notably, Ham and colleagues show that the class of kernel-eigenmap-based dimensionality reduction methods, such as Isomap, Local Linear Embedding (LLE) and Laplacian Eigenmaps can be understood as a variant of kPCA with different kernel matrices. Methods of this class are described in detail later, but common to all these mehods is the aim to obtain a global representation of the data \tmat{X} by using information about local interactions between the data points in \tmat{X}. The data points are represented as the nodes of a symmetric graph, whose kernel function \(k\) describes a local geometry of \tmat{X}. The graph specified by \((\mat{X}, k)\) is used to construct a square matrix \tmat{M}, which describes the transitions on the graph as a markow chain. Using this Markov matrix \tmat{M}, one can map the data into a lower dimensional Euclidean space. The difference of the algorithms lies in the definition of the neighborhood structure and the means to find a global embedding. \Cref{tab:dimMethods} summarises these and other commonly used linear and non-linear techniques and the list below gives a short summary of the mathematical principles. 

% Table generated by Excel2LaTeX from sheet 'HighDimMethods'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Dimensionality reduction methods.}]{\textbf{Dimensionality reduction methods.} The different dimensionality reduction techniques can distinctly be classified into linear and non-linear types. The methods column broadly groups techniques based on their main mathematical concept and parameters gives the number of parameters that need to be specified for the mathematical model.}
    \begin{tabular}{lllr}
    \toprule
    Type  & Method & Name  & \multicolumn{1}{l}{Parameters} \\
    \midrule
    \multicolumn{1}{c}{\multirow{4}[1]{*}{linear}} & \multirow{2}[1]{*}{spectral} & PCA   & 0 \\
          &       & MDS   & 0 \\
    \addlinespace[1.5ex]
          & Factor analysis & PEER  & 1 \\
          & Generative model & ICA   & 2 \\
    \addlinespace[0.5ex]
    \midrule
    \addlinespace[1ex]
    \multirow{8}[1]{*}{non-linear} & rank-based & nMDS  & 2 \\
          & PCA-based & DRR   & >1 \\
    \addlinespace[1.5ex]
          & spectral & kPCA  & 0 \\
          &  \multirow{4}[0]{*}{Kernel eigenmap}      & Isomap & 1 \\
          &       & LLE   & 1 \\
          &       & Laplacian Eigenmaps & 2 \\
          &       & DiffusionMaps & >2 \\
    \addlinespace[1.5ex]
          & Probability distributions & tSNE  & 4 \\
    \bottomrule
    \end{tabular}%
\label{tab:dimMethods}%
\end{table}%

\begin{enumerate}
\item\textbf{PEER}: PEER implements factor analysis methods to estimate variance components in \tmat{X}. The model assumes additive effects from independent sources that influence \tmat{X} and aims at estimating these effects in a joint Bayesian inference model.  By specifying the only source of variation to be due to unknown effects, PEER can be used to extract latent variables from high-dimensional datasets, where the latent variables are modeled based on a standard normal distribution and are initiated based on PCA of \tmat{X}. The model specifications are complex and the interested reader is referred to the paper describing the details of the methodology \citep{Stegle2010}. In the most simple scenario, only the parameter of the maximum number of unobserved latent factors, i.e. the column dimensionality \(K\) of \tmat{Z} have to be specified \citep{Stegle2012}. 

\item\textbf{Independent Component Analysis}:  ICA belongs to the class of generative models, which describes how the data \tmat{X} could have been generated by a process of mixing independent components \tmat{S} according to a mixing scheme \tmat{A}: \(\mat{X} = \mat{SA}\). Both the independent components and the mixing matrix are unknown. The key to `un-mixing' the signals are the underlying assumptions that the latent components are independent and have a non-Gaussian distribution. In order to find \tmat{A} and \tmat{S}, ICA finds the un-mixing matrix \tmat{U} which maximises the non-gaussianity of \(\mat{S}\): \(\mat{XU}=\mat{S}\). Non-gaussianity can be quantified by approximating the negentropy of  \tmat{S}, i.e. the difference in entropy between a Gaussian random variable of the same covariance matrix as \tmat{S} and the entropy of \tmat{S} itself. The parameters to be specified are the threshold for the tolerance at which the un-mixing matrix is considered to have converged and the number of components to be modeled.  ICA was first described by Herault and Jutten \citeyear{Herault1983} and has seen many implementations for finding the maximum of the non-gaussianity (reviewed in \citep{Common1994}), including FastICA \citep{Hyvarinen2000}. ICA often includes a pre-processing step to make the columns of \tmat{X} uncorrelated and scale their variances to unity. This process is termed  `whitening' and is achieved through PCA of \tmat{X}. 

\item\textbf{non-metric MDS}: Extensions of the classical MDS described above relax the matching criterion of dissimilarities and distances to finding the closest match of a monotonic function of the distances to the dissimilarities: \(f(d_{ij}) \approx \hat{d_{ij}} = \lVert \matsub{z}{i} - \matsub{z}{j}\rVert \). The closest match is determined by minimising a stress function \citep{Kruskal1964a,Kruskal1964b}. In the non-metric version of these extensions, \(f(d_{ij})\) simply considers the rank order of the input dissimilarities such that the rank order agreement between the distances and the dissimilarities is maximized \citep{Minchin1987}. The parameters to be specified are the threshold for minimum stress at which the distances and dissimilarities are considered to have converged and the number of components to model.

\item\textbf{Dimensionality Reduction via Regression}: DRR is a PCA-based regression technique which aims at removing redundant information present in the principal components \tmat{W} of \tmat{X}. While standard PCA yields decorrelated dimensions, complete independence of its components is only certain if the high-dimensional data had a Gaussian probability density function \citep{Laparra2015}. The main idea in DRR is to remove the redundant information contained in partially dependent components and only keep the remaining, non-predictable information in the low-dimensional representation. The removal of the redundant information is achieved in a step-wise manner by starting at the lowest variance component (i.e.smallest eigenvalue) and using it as the response variable for a multi-variate non-linear regression function \(f\) with all higher variance components as predictors. This process is repeated for each principal component until the component with the second highest eigenvalue is reached and all redundant information has been regressed out.  Formally, this iterative prediction scheme can be described as\(z_i = \mat{w}{i} - f_i(\mat{w}{1},\mat{w}{2} \dots , \mat{w}{i-1})\), where \(z_i\) is the non-predictable information. As in PCA, the first components account for the highest variance. The number of parameters depends on the function \(f\) specified for the non-linear regression. The standard method described in the original paper uses Kernel Ridge regression with a Gaussian kernel function, i.e. one free parameter for the band width of the kernel \citep{Laparra2015}. 

\item\textbf{Isomap}: Isomap builds on classical MDS for the dimensionality reduction and kernel eigenmaps to find the required dissimilarity matrix of \tmat{X}. The dissimilarities are definded as the geodesic manifold distances between all pairs of datapoints. Isomap constructs a graph of all datapoints and sets the edge length between neighbouring points to the geodesic distance. For datapoints in proximity (based on \(n\) nearest neighbours or threshold on the distance measure), the euclidean distance in input space serves a good approximation. The geodesic distance for points outside the proximity criterion is approximated by adding up a sequence of `short hops' jumps  between neighboring points. The shortest distances between points of the graph are a measure for the dissimilarity between data points and serve as the input data for classical MDS \citep{Tenebaum2000}. The proximity threshold is the parameter to specify. 

\item\textbf{Locally Linear Embedding}: LLE uses kernel eigenmaps based on the local structure in the data to recover the non-linear global data structure. It assumes that any datapoint in \tmat{X} lies on a close to linear patch with its neighbours and can be be reconstructed through linear recombination of these neighbours. The linear recombination is described in the weight matrix \tmat{H}. The objective of the algorithm is to find \tmat{H} which minimises the reconstrution error between all datapoints and their reconstructions. Based on the optimised \tmat{H}, the datapoints \tmat{X} can be transformed into lower dimensional space \tmat{Z} by solving the eigendecomposition of \((\matsub{I}{N} -\mat{W})^T(\matsub{I}{N} -\mat{W})\) \citep{Roweis2000}. LLE requires the specification of the local neighbourhood size \(n\). 

\item\textbf{Laplacian Eigenmaps}: The basis for Laplacian Eigenmaps is an adjacency graph representing \tmat{X}. For adjacent datapoints (proximity measures as in Isomap), the edges of the graph are weighted based on a heat kernel of the euclidean distance: \(\matsub{H}{i,j}=exp(-\frac{\lVert \matsub{x}{i} - \matsub{x}{j}\rVert_2}{n})\). Edges for points that do not fall within the proximity threshold \(n\) are set to zero.  Based on the weight matrix \tmat{H}, a diagonal matrix \tmat{D} is constructed by \(D = \sum_{j} \matsub{H}{i,j}\) and the positive, semi-definite Laplacian matrix \tmat{L} computed as: \(\mat{L} = \mat{D} - |mat{H}\). The eigendecomposition of \(\mat{LV} = \mat{\lambda DV}\) and selection of the first \(K\) eigenvectors \(V\) yields the \(K\)-dimensional embedding of \tmat{X} in \tmat{Z} \citep{Belkin2003}. For dimensionality reduction via Laplacian Eigenmaps, the threshold for the proximity criterion and \(n\), the free parameter in the heat-kernel have to be specified. Large values of \(n\) yield less weight to differences in distance, with \(n=\infty\) setting all non-zero distances to one. 
 
\item\textbf{DiffusionMaps}: As for all kernel eigenmap methods, DiffusionMaps first constructs a graph representation of \tmat{X} which is turned into the Markow matrix \tmat{M}, used for the low-dimensional embedding. The length of the edges between points on the graph are computed by a kernel \(k(\matsub{x}{i},\matsub{x}{i})\) normalised to the local connectivity of the graph, and in such capture the local geometry in the data. This normalised kernel can be interpreted as the transition kernel of \tmat{M}, representing the transition probability from point \tmatsub{x}{i} to \tmatsub{x}{j} in one time step. Based on the eigenvalues and eigenvectors of \tmat{M}, diffusion distances and maps between the datapoints can be computed. These are in turn used to map the data into a Euclidean space, where the distance describes the relationship beween data points in terms of their connectivity. The dimensionality of the re-mapped data depends on the number of eigenvectors used for the embedding into Euclidean space. These are chosen based on the number of transitions \(t\) on \tmat{M} and an accuracy term \(\epsilon\), which specify the maximum eigenvalue considered informative in the mapping \citep{Coifman2005,Coifman2006}. Depending on the kernel function, additional parameters might have to be specified. Typical kernel functions are the Gaussian kernel and  heat kernels.

\item\textbf{t-Distributed Stochastic Neighborhood Embedding}: In tSNE, the Euclidean distance of the data points in \tmat{X} are converted into joined probabilites \(p_{i,j}\). Similarily, for  a low-dimensional representation \tmat{Z} of \tmat{X}, the distance  \(\lVert \matsub{z}{i} -\matsub{z}{j}\rVert_2 \) is converted into the joined probabilites \(q_{i,j}\). The objective of tSNE is to find the configuration of \tmat{Z} which minimises the Kullback-Leibler divergence \(KL\) between the probability distributions \(P\) and \(Q\): \(KL(P\lVert Q) = \sum_i \sum_j p_{ij}\log\frac{pij}{qij}\). \(KL\) in general is a measure for how much one probability distribution diverges from another \citep{Kullback1951} and serves in tSNE as the criterion for finding a good low-dimensional representation. The mapping of similarities to probabilities in the low-dimensional space are based on a Student t-dristibution with one degree of freedom, whereas the mappings in high-dimensional space are converted using a gaussian distribution. Depending on the data density around each \tmatsub{x}{i}, the standard deviation is adjusted for each gaussian \(p_i\) based on the specified perplexity, a smooth measure of the effective number of neighbours. In addition, parameters for the gradient descent function used to find the minimum \(KL\) have to be specified:  the number of iterations, the learning rate and the momentum. For details of these parameters refer to \citep{Maaten2008}.
\end{enumerate}

Despite the diversity of the dimensionality reduction techniques, there are a number of underlying features which define common properties and can give an indication for their applicability. Methods directly based on PCA, i.e PCA, MDS and DRR are easy to apply and the extracted features are interpretable (directions of variance). While PCA and MDS mainly work well for linear manifolds, DRR extends the applicability to non-linear manifolds. The ability to learn non-linear manifold structures in the data is also shared by the kernel eigenmap methods, nMDS and DRR \citep{Coifman2006}. However, non-linear models introduce a number of free parameters, whose choice requires prior assumptions about the manifold characteristics. Dimensionality reduction via kernel eigenmaps and tSNE depend on the assumption that distances of points for apart in the global space do not contain information and need not be preserved. Hence, these techniques are simply based on local neighbourhoods and preserve these in the low-dimensional space. This in turn requires dense data points in the low-dimensional space for this strategy to be a good estimation. 

There are two main purposes for dimensionality reduction, visualisation and feature selection.  For visualisation, \(K\) is commonly chosen to be two or three such that the data can be presented in a two or three dimensional graphic. The choice of dimensionality for feature selection is less trivial, as the dimension of the low-dimensional manifold is unknown. In general, choosing the dimensionality is easiest for PCA and PCA-based methods, where the principal components that cumulatively explain a certain fraction of the variance in the data define the dimensionality. For other methods, the task is less straight forward and different strategies have to be developed.  In the next section, I will show the results of applying the techniques described above for the visualisation of two small datasets with known structure.
