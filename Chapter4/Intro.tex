\chapter{Low-dimensional representations of very high-dimensional data}
\label{chapter:DimReduction}
In \cref{chapter:limmbo} and \cref{chapter:yeast}, I developed and applied methods for the multivariate analyses of hundreds of traits. When evaluation the suitability of \gls{limmbo} on the simulated datasets, I considered each trait as a separate, but correlated measures and used all traits for the multi-trait genotype to phenotype mapping. I used the same strategy for the application of \gls{limmbo} to the growth traits of yeast. However, as described in \cref{chapter:simulation}, the simulated phenotypes are generated by adding different phenotype components, and one could argue that depending on the analysis, it might proof useful to extract relevant features representing different phenotype components prior to the multivariate analyses across all traits. For instance, given very large numbers of measurements or traits, feature extracting will reduce the number of traits and therefore the degrees of freedom for the multivariate analyses. In the following chapter I will describe different methods to achieve the feature extraction by dimensionality reduction approaches. I will present two case studies that show how these approaches can be used for visualisation of high-dimensional data. Beyond that, I will demonstrate in simulations that they can not only be used for visualisation but also as valid proxy phenotypes for genetic association studies. These simulation results build the basis for the genetic association study on 3D heart phenotypes described in \cref{chapter:GWAS-3Dheart}. 

In biological and medical research, samples are often phenotyped for more than one trait. These traits can either be different attributes of the same underling phenotype or more independent features. In the former case, multiple phenotypes can be related measurements such as length, width and circumference of plant leafs or measurements commonly regarded as covariates such as sample height and weight. For image-based and molecular phenotyping methods, the measured traits can be a mixture of independent features and attributes of the same phenotype. 
For instance, in computed tomography scans, functional \gls{mri} or high-resolution microscopy, each pixel or voxel can be considered a different measurement. Groups of these describe different morphologies (features) and pixels/voxels within each group can be considered attributes of that feature. In molecular phenotyping such as gene expression or metabolite profiling, several hundred or thousand measurements are collected simultaneously. Here, the classification into features and attributes is more difficult, considering the complex structure of gene expression networks and gene regulation. In many of these cases, neither the number of attributes nor the number of independent features are know. However, when analysing these large datasets, one is often interested in extracting meaningful variables from the data or compressing the data into a more tractable number of variables. These approaches rely on the assumption that the lower number of variables are a good representation of the true complexity of the dataset. In other words, one assumes that the high-dimensional datasets occupy an intrinsically lower-dimensional space (manifold) which is embedded in the observed, high-dimensional space. Low dimensional representations of gene expression measurements might reflect common pathways or transcriptional profiles and image-derived phenotypes could reflect organ shape variation, disease status or functional MRI activity scores.  For a high-dimensional dataset \tmat{X} with \(N\) samples and \(P\) dimensions (traits), dimensionality reduction techniques aim i) to provide a meaningful low-dimensional representation \tmat{Z} of \(K\) dimensions while only losing minor amounts of information:
\begin{equation}
\mat{X} \inR N P \xrightarrow{\text{DimReduction}} \mat{Z} \inR N K ,
\end{equation}
ii) to use only a small number of free parameters and iii) to preserve the quantities of interest in the data. Depending on the algorithm employed, these might be local proximity or global structure. 

There are a variety of approaches for dimensionality reduction with different underlying mathematical concepts and parameters and choosing the most appropriate method for a given dataset is not trivial. Fundamentally, the problem is finding an objective criterion of what a good dimensionality reduction method is. 

In the following, I will first present a small review of current dimensionality reduction methods. I will use these methods to demonstrate the application of dimensionality reduction for visualisation on small datasets with known structure. I will compare the visual results to two published criteria for measuring the quality of dimensionality reduction in terms of neighbourhood-similarities in the low- and high-dimensional space. Then, I will describe the results of the different dimensionality reduction techniques on simulated high-dimensional datasets and propose an additional stability criterion which aids in choosing the dimensionality of the lower-dimensional phenotype space. Finally, I show that low-dimensional representations of the phenotypes can capture underlying genetic structure. The methods and criteria used in this chapter will be applied on clinically relevant high-dimensional heart morphology data in \cref{chapter:GWAS-3Dheart}.

\section{Review of dimensionality reduction methods}
\label{section:DimReduction-methods}
The earliest dimensionality reduction techniques were two linear methods based on spectral decomposition: \gls{pca} and \gls{mds}. 

The general concept of \gls{pca} was described by Pearson in 1901 \citep{Pearson1901}. In  \parencite*{Hotelling1933}, Hotelling was the first to describe it as a method for dimensionality reduction. In \gls{pca} the components of the new phenotype representation are the \glspl{pc} and are the eigenvectors \tmat{W} of the empirical covariance matrix \tmat{C}:  \(\mat{C}=\mat{X}\mat{X}^T = \mat{W}\mat{\Lambda}\mat{W}^T\). The eigenvalues in the diagonal matrix \(\mat{\Lambda}\) corresponding to the \glspl{pc} are equivalent to the variance explained by their components. The transformation of the phenotype data into \glspl{pc} leads to a projection where the highest amount of phenotypic variance explained lies in the first component, the second highest variance in the second component and so on. The dimensionality reduction is achieved by using the first \(K\) \glspl{pc} until the cumulative sum of the eigenvalues reaches a predefined threshold of total phenotypic variance that should be retained: \(\mat{Z}={\matsub{W}{1}, \dots, \matsub{W}{K}}\). 
 
\gls{mds} was introduced by \citet{Gower1966}, motivated by his dissatisfaction about the overuse of \gls{pca} in biology. \gls{mds} is based on the spectral decomposition of a dissimilarity matrix \tmat{D} between the samples in \tmat{X}. Classical \gls{mds} finds the low-dimensional representation \tmat{Z} whose pairwise distance matches the dissimilarity \(d_{ij}\) of the original data: \(d_{ij} \approx \hat{d_{ij}} = \lVert \matsub{z}{i} - \matsub{z}{j}\rVert \). \tmat{Z} can be found by the eigendecomposition of the squared dissimilarity matrix \(\matsup{D}{2} = \mat{V}\mat{\Lambda}\matsup{V}{T}\), where \(\mat{Z}=\mat{\Lambda}^\frac{1}{2}\mat{V}^T\). As in \gls{pca}, \tmat{Z} will be ordered with the components explaining most variance ranked first and dimensionality reduction can be achieved by selecting the first \(K\) vectors \citep{Gower1966}. \gls{mds} finds an embedding that preserves the interâ€point distances and is equivalent to \gls{pca} when those distances are Euclidean.

Several decades after the introduction of \gls{pca} as a means of linear dimensionality reduction, \citet{Schoelkopf1998} and colleagues proposed its non-linear extension based on the transformation of \tmat{X} into a feature space \tmat{F} via the mapping function  \(\Phi\). Instead of finding the eigendecomposition of the covariance matrix of \tmat{X}, the aim is the diagonalisation of the covariance \tmat{K} of the features of the data \(\Phi(\mat{X})\): \(\mat{K} = \Phi(\mat{X})\Phi(\mat{X})^T\). Using the kernel representation \(k(\matsub{x}{i}, \matsub{x}{j}) = (\Phi(\matsub{x}{i})\Phi(\matsub{x}{j}))\) to compute the dot products of \(\Phi(\matsub{x}{i})\Phi(\matsub{x}{j})\) allows computation of the dot product in \tmat{F} without having to carry out the map \(\Phi\). This technique is commonly referred to as the kernel trick and yields the feature covariance matrix \(\matsub{K}{ij} = k(\matsub{x}{i}, \matsub{x}{j})_{ij}\). The normalised eigenvectors of \tmat{K} are used to extract the \glspl{pc}. This \gls{kpca} approach allows for non-linear feature extraction, whilst the possibility to select different kernels (e.g. gaussian or sigmoid) makes it applicable for a wide range of cases when non-linearity is assumed \citep{Schoelkopf1998}.

\gls{pca}, \gls{kpca} and \gls{mds} build the basis for many other dimensionality reduction techniques. Notably, Ham and colleagues show that the class of kernel-eigenmap-based dimensionality reduction methods, such as Isomap, \gls{lle} and Laplacian Eigenmaps can be understood as a variant of \gls{kpca} with different kernel matrices. Methods of this class are described in detail later, but common to all these methods is the aim to obtain a global representation of the data \tmat{X} by using information about local interactions between the data points in \tmat{X}. The data points are represented as the nodes of a symmetric graph, whose kernel function \(k\) describes a local geometry of \tmat{X}. The graph specified by \((\mat{X}, k)\) is used to construct a square matrix \tmat{M}, which describes the transitions on the graph as a Markow chain. Using this Markov matrix \tmat{M}, one can map the data into a lower dimensional Euclidean space. The difference of the algorithms lies in the definition of the neighbourhood structure and the means to find a global embedding. \Cref{tab:dimMethods} summarises these and other commonly used linear and non-linear techniques and the list below gives a short summary of the mathematical principles. 

% Table generated by Excel2LaTeX from sheet 'HighDimMethods'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Dimensionality reduction methods.}]{\textbf{Dimensionality reduction methods.} The different dimensionality reduction techniques can distinctly be classified into linear and non-linear types. The methods column broadly groups techniques based on their main mathematical concept and parameters gives the number of parameters that need to be specified for the mathematical model.}
    \begin{tabular}{lllr}
    \toprule
    Type  & Method & Name  & \multicolumn{1}{l}{Parameters} \\
    \midrule
    \multicolumn{1}{c}{\multirow{4}[1]{*}{linear}} & \multirow{2}[1]{*}{spectral} & \gls{pca}   & 0 \\
          &       & \gls{mds}   & 0 \\
    \addlinespace[1.5ex]
          & Factor analysis & \glsdisp{peer}{PEER}  & 1 \\
          & Generative model & \glsdisp{ica}{ICA}  & 2 \\
    \addlinespace[0.5ex]
    \midrule
    \addlinespace[1ex]
    \multirow{8}[1]{*}{non-linear} & rank-based &  \glsdisp{nmds}{nMDS}  & 2 \\
          & \gls{pca}-based & \glsdisp{drr}{DRR}   & >1 \\
    \addlinespace[1.5ex]
          & spectral & \glsdisp{kpca}{kPCA}  & 0 \\
          &  \multirow{4}[0]{*}{Kernel eigenmap}      & Isomap & 1 \\
          &       & \glsdisp{lle}{LLE}   & 1 \\
          &       & Laplacian Eigenmaps & 2 \\
          &       & DiffusionMaps & >2 \\
    \addlinespace[1.5ex]
          & Probability distributions & \glsdisp{tsne}{tSNE}  & 4 \\
    \bottomrule
    \end{tabular}%
\label{tab:dimMethods}%
\end{table}%

\begin{enumerate}
\item\textbf{Probabilistic estimation of expression residuals}: \glsdisp{peer}{PEER} implements factor analysis methods to estimate variance components in \tmat{X}. The model assumes additive effects from independent sources that influence \tmat{X} and aims at estimating these effects in a joint Bayesian inference model.  By specifying the only source of variation to be due to unknown effects, \gls{peer} can be used to extract latent variables from high-dimensional datasets, where the latent variables are modelled based on a standard normal distribution and are initiated based on \gls{pca} of \tmat{X}. The model specifications are complex and the interested reader is referred to the paper describing the details of the methodology \citep{Stegle2010}. In the most simple scenario, only the parameter of the maximum number of unobserved latent factors, i.e. the column dimensionality \(K\) of \tmat{Z} have to be specified \citep{Stegle2012}. 

\item\textbf{Independent Component Analysis}:  \glsdisp{ica}{ICA} belongs to the class of generative models, which describes how the data \tmat{X} could have been generated by a process of mixing independent components \tmat{S} according to a mixing scheme \tmat{A}: \(\mat{X} = \mat{SA}\). Both the independent components and the mixing matrix are unknown. The key to \textquote{un-mixing} the signals are the underlying assumptions that the latent components are independent and have a non-Gaussian distribution. In order to find \tmat{A} and \tmat{S}, \gls{ica} finds the un-mixing matrix \tmat{U} which maximises the non-gaussianity of \(\mat{S}\): \(\mat{XU}=\mat{S}\). Non-gaussianity can be quantified by approximating the negentropy of \tmat{S}, i.e. the difference in entropy between a Gaussian random variable of the same covariance matrix as \tmat{S} and the entropy of \tmat{S} itself. The parameters to be specified are the threshold for the tolerance at which the un-mixing matrix is considered to have converged and the number of components to be modelled. \gls{ica} was first described by \citet{Herault1986} and has seen many implementations for finding the maximum of the non-gaussianity (reviewed in \citep{Comon1994}), including FastICA \citep{Hyvarinen2000}. \gls{ica} often includes a pre-processing step to make the columns of \tmat{X} uncorrelated and scale their variances to unity. This process is termed \textquote{whitening} and is achieved through \gls{pca} of \tmat{X}. 

\item\textbf{non-metric MDS}: Extensions of the classical \gls{mds} described above relax the matching criterion of dissimilarities and distances to finding the closest match of a monotonic function of the distances to the dissimilarities: \(f(d_{ij}) \approx \hat{d_{ij}} = \lVert \matsub{z}{i} - \matsub{z}{j}\rVert \). The closest match is determined by minimising a stress function \citep{Kruskal1964a,Kruskal1964b}. In the non-metric version of these extensions, \(f(d_{ij})\) simply considers the rank order of the input dissimilarities such that the rank order agreement between the distances and the dissimilarities is maximized \citep{Minchin1987}. The parameters to be specified are the threshold for minimum stress at which the distances and dissimilarities are considered to have converged and the number of components to model.

\item\textbf{Dimensionality reduction via regression}: \glsdisp{drr}{DRR} is a \gls{pca}-based regression technique which aims to remove redundant information present in the \glspl{pc} \tmat{W} of \tmat{X}. While standard \gls{pca} yields decorrelated dimensions, complete independence of its components is only certain if the high-dimensional data had a Gaussian probability density function \citep{Laparra2015}. The main idea in \gls{drr} is to remove the redundant information contained in partially dependent components and only keep the remaining, non-predictable information in the low-dimensional representation. The removal of the redundant information is achieved in a step-wise manner by starting at the lowest variance component (i.e. smallest eigenvalue) and using it as the response variable for a multivariate non-linear regression function \(f\) with all higher variance components as predictors. This process is repeated for each \gls{pc} until the component with the second highest eigenvalue is reached and all redundant information has been regressed out.  Formally, this iterative prediction scheme can be described as \(z_i = \mat{w}_{i} - f_i(\mat{w}_{1},\mat{w}_{2}, \dots ,\allowbreak \mat{w}_{ i-1})\), where \(z_i\) is the non-predictable information. As in \gls{pca}, the first components account for the highest variance. The number of parameters depends on the function \(f\) specified for the non-linear regression. The standard method described in the original paper uses Kernel Ridge regression with a Gaussian kernel function, i.e. one free parameter for the band width of the kernel \citep{Laparra2015}. 

\item\textbf{Isomap}: Isomap builds on classical \gls{mds} for the dimensionality reduction and kernel eigenmaps to find the required dissimilarity matrix of \tmat{X}. The dissimilarities are defined as the geodesic manifold distances between all pairs of data points. Isomap constructs a graph of all data points and sets the edge length between neighbouring points to the geodesic distance. For data points in proximity (based on \(n\) nearest neighbours or threshold on the distance measure), the euclidean distance in input space serves a good approximation. The geodesic distance for points outside the proximity criterion is approximated by adding up a sequence of `short hops' jumps  between neighbouring points. The shortest distances between points of the graph are a measure for the dissimilarity between data points and serve as the input data for classical \gls{mds} \citep{Tenenbaum2000}. The proximity threshold is the parameter to specify. 

\item\textbf{Local linear embedding}: \glsdisp{lle}{LLE} uses kernel eigenmaps based on the local structure in the data to recover the non-linear global data structure. It assumes that any data point in \tmat{X} lies on a close to linear patch with its neighbours and can be reconstructed through linear recombination of these neighbours. The linear recombination is described in the weight matrix \tmat{H}. The objective of the algorithm is to find \tmat{H} which minimises the reconstruction error between all data points and their reconstructions. Based on the optimised \tmat{H}, the data points \tmat{X} can be transformed into lower dimensional space \tmat{Z} by solving the eigendecomposition of \((\matsub{I}{N} -\mat{W})^T(\matsub{I}{N} -\mat{W})\) \citep{Roweis2000}. \gls{lle} requires the specification of the local neighbourhood size \(n\). 

\item\textbf{Laplacian Eigenmaps}: Laplacian Eigenmaps are based on an adjacency graph representing \tmat{X}. For adjacent data points (proximity measures as in Isomap), the edges of the graph are weighted based on a heat kernel of the euclidean distance: \(\matsub{H}{i,j}=exp(-\frac{\lVert \matsub{x}{i} - \matsub{x}{j}\rVert_2}{n})\). Edges for points that do not fall within the proximity threshold \(n\) are set to zero.  Based on the weight matrix \tmat{H}, a diagonal matrix \tmat{D} is constructed by \(D = \sum_{j} \matsub{H}{i,j}\) and the positive, semi-definite Laplacian matrix \tmat{L} computed as: \(\mat{L} = \mat{D} - \mat{H}\). The eigendecomposition of \(\mat{LV} = \mat{\lambda DV}\) and selection of the first \(K\) eigenvectors \(\mat{V}\) yields the \(K\)-dimensional embedding of \tmat{X} in \tmat{Z} \citep{Belkin2003}. For dimensionality reduction via Laplacian Eigenmaps, the threshold for the proximity criterion and \(n\), the free parameter in the heat-kernel have to be specified. Large values of \(n\) yield less weight to differences in distance, with \(n=\infty\) setting all non-zero distances to one. 
 
\item\textbf{DiffusionMaps}: As for all kernel eigenmap methods, DiffusionMaps first constructs a graph representation of \tmat{X} which is turned into the Markow matrix \tmat{M}, used for the low-dimensional embedding. The length of the edges between points on the graph are computed by a kernel \(k(\matsub{x}{i},\matsub{x}{i})\) normalised to the local connectivity of the graph, and in such capture the local geometry in the data. This normalised kernel can be interpreted as the transition kernel of \tmat{M}, representing the transition probability from point \tmatsub{x}{i} to \tmatsub{x}{j} in one time step. Based on the eigenvalues and eigenvectors of \tmat{M}, diffusion distances and maps between the data points can be computed. These are in turn used to map the data into a Euclidean space, where the distance describes the relationship between data points in terms of their connectivity. The dimensionality of the re-mapped data depends on the number of eigenvectors used for the embedding into Euclidean space. These are chosen based on the number of transitions \(t\) on \tmat{M} and an accuracy term \(\epsilon\), which specify the maximum eigenvalue considered informative in the mapping \citep{Coifman2005,Coifman2006}. Depending on the kernel function, additional parameters might have to be specified. Typical kernel functions are the Gaussian kernel and  heat kernels.

\item\textbf{t-Distributed stochastic neighbourhood embedding}: In \glsdisp{tsne}{tSNE}, the Euclidean distance of the data points in \tmat{X} are converted into joint probabilities \(p_{i,j}\). Similarly, for  a low-dimensional representation \tmat{Z} of \tmat{X}, the distance  \(\lVert \matsub{z}{i} -\matsub{z}{j}\rVert_2 \) is converted into the joined probabilities \(q_{i,j}\). The objective of \gls{tsne} is to find the configuration of \tmat{Z} which minimises the Kullback-Leibler divergence \(KL\) between the probability distributions \(P\) and \(Q\): \(KL(P\lVert Q) = \sum_i \sum_j p_{ij}\log\frac{p_{ij}}{q_{ij}}\). \(KL\) in general is a measure for how much one probability distribution diverges from another \citep{Kullback1951} and serves in \gls{tsne} as the criterion for finding a good low-dimensional representation. The mapping of similarities to probabilities in the low-dimensional space are based on a Student t-distribution with one degree of freedom, whereas the mappings in high-dimensional space are converted using a gaussian distribution. Depending on the data density around each \tmatsub{x}{i}, the standard deviation is adjusted for each gaussian \(p_i\) based on the specified perplexity, a smooth measure of the effective number of neighbours. In addition, parameters for the gradient descent function used to find the minimum \(KL\) have to be specified:  the number of iterations, the learning rate and the momentum. For details of these parameters refer to \citep{Maaten2008}.
\end{enumerate}

Despite the diversity of the dimensionality reduction techniques, there are a number of underlying features which define common properties and can give an indication for their applicability. Methods directly based on \gls{pca} (\gls{pca}, \gls{mds} and \gls{drr}), are easy to apply and the extracted features are interpretable (directions of variance). While \gls{pca} and \gls{mds} mainly work well for linear manifolds, \gls{drr} extends the applicability to non-linear manifolds. The ability to learn non-linear manifold structures in the data is also shared by the kernel eigenmap methods, \gls{nmds} and \gls{drr} \citep{Coifman2006}. However, non-linear models introduce a number of free parameters, whose choice requires prior assumptions about the manifold characteristics. Dimensionality reduction via kernel eigenmaps and \gls{tsne} depend on the assumption that distances of points for apart in the global space do not contain information and need not be preserved. Hence, these techniques are simply based on local neighbourhoods and preserve these in the low-dimensional space. This in turn requires dense data points in the low-dimensional space for these strategies to be a good estimation. 

There are two main purposes for dimensionality reduction, visualisation and feature selection.  For visualisation, \(K\) is commonly chosen in a range from one to three such that the data can be presented in a one, two or three dimensional graphic. The choice of dimensionality for feature selection is less trivial, as the dimension of the low-dimensional manifold is unknown. In general, choosing the dimensionality is easiest for \gls{pca} and \gls{pca}-based methods, where the principal components that cumulatively explain a certain fraction of the variance in the data define the dimensionality. For other methods, the task is less straight forward and different strategies have to be developed.  In the next section, I will show the results of applying the techniques described above for the visualisation of two small datasets with known structure.
