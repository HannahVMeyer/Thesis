\section{Visualisation of data structures by dimensionality reduction}
\label{section:visualisation}
In high-dimensional data analysis, one is often interested in finding a clear visualisation of the data, which leads to a minimal loss of information and is capable of summarising underlying data structures. Data can be of biological origin, representing features of interest like cell populations or tissue types, or of technical origin such as batch effects. In high-dimensional datasets, visualisation requires either \textit{a prior} selection of the dimensions of the original data to be displayed or the reduction to a dimension that can be represented. Common choices of dimensionality reduction for this task are \gls{pca} or \gls{tsne} \citep{Deng2014,Crowley2015,Corces2016,Martinez-Jimenez2017,Huisman2017}. In the following, I analyse the low-dimensional representations of two different datasets generated by the twelve dimensionality reduction techniques described above.

For each technique, I used corresponding functions already implemented in publicly available R-packages. \Cref{tab:dimRed-R} summarises the R packages, functions and their parameters used for the dimensionality reduction. Most functions require specification of the expected number of dimensions \(ndim\). For the purpose of visualisation in a Cartesian coordinate system, this parameter choice is straightforward (one, two or three) and was set to  \(ndim=2\). In the case of kernel eigenmap methods and \gls{tsne}, the number of \(n\) nearest neighbours used in the graph construction and probability function have to be provided. This task is less intuitive and different algorithms have been implemented to estimate the optimal number of neighbours for the reconstruction. Choosing a suitable \(n\) is important, as neighbourhoods chosen too large might eliminate fine structures in the data, while neighbourhoods too small can lead to the division of the continuous input space into smaller, unconnected sub-manifolds \citep{Kayo2006}.

For any method that required specification of \(n\), I provided \(n\) estimated according to the method proposed by \citet{Kayo2006}, implemented as the function \textit{calc}\_\textit{k} in the \textit{lle} package. Some methods require additional, specific parameters. These are either specified in \cref{tab:dimRed-R} or the default setting was chosen. For functions that required a distance matrix or metric for the local neighbourhood estimation (\gls{mds}, DiffusionMap, Isomap, \gls{nmds}), the default is the Euclidean distance. Methods that require a kernel function (\gls{drr}, \gls{kpca}) use a gaussian radial basis kernel by default. For \gls{ica} and \gls{drr}, I choose the default setting of the \gls{pca} pre-processing step. For \gls{peer}, the functions are implemented in an object-oriented manner and I followed the protocol described in \citet{Stegle2012}. I choose to include the optional parameter of adjusting for the mean.  
%
% Table generated by Excel2LaTeX from sheet 'HighDimR'
\begin{table}[htbp]
  \centering
  \caption[\textbf{R functions for dimensionality reduction methods and their parameters.}]{\textbf{R functions for dimensionality reduction methods and their parameters.} Most functions require \textit{a priori} specification of the number of \(n\) nearest neighbours and the expected intrinsic dimensionality \(ndim\). Any function-specific parameters different to the default settings are listed. The reference column specifies the publications the R packages are based on. LE: Laplacian Eigenmaps, DM: Diffusion Maps.}
  \begin{small}
    \begin{tabular}{lllr}
    \toprule
    Name  & R function & Parameters & Reference \\
    \midrule
    \gls{pca}   & stats::prcomp &  -    & \citep{Hotelling1933} \\
    \gls{peer} & peer  & ndim,  & \citep{Stegle2010} \\
    \multirow{2}[0]{*}{\gls{ica}} & \multirow{2}[0]{*}{fastICA::fastICA} & ndim, fun=logcosh, & \multirow{2}[0]{*}{\citep{Hyvarinen2000}} \\
          &       &  method="C" &  \\
    \gls{mds}   & stats::cmdscale & ndim  & \citep{Gower1966} \\
    \gls{nmds}  & vegan::metaMDS & ndim  & \citep{Ripley1996} \\
    \gls{drr}   & DRR::drr &  -    & \citep{Laparra2015} \\
    \gls{kpca}  & kernlab::kpca &  -    & \citep{Schoelkopf1998} \\
    \multirow{2}[0]{*}{Isomap} & \multirow{2}[0]{*}{vegan::isomap} & ndim, k, & \multirow{2}[0]{*}{\citep{Tenenbaum2000}} \\
          &       & fragmentedOK=TRUE &  \\
    \gls{lle}   & lle::lle & ndim, k & \citep{deRidder2002} \\
    LE & loe::LOE & ndim, k & \citep{Belkin2003} \\
    DM & diffusionMap::diffuse & k     & \citep{Lafon2006} \\
    \gls{tsne}  & Rtsne::Rtsne & ndim, k & \citep{Maaten2008} \\
    \bottomrule
    \end{tabular}%
 \end{small}
 \label{tab:dimRed-R}%
\end{table}%
%
The first dataset is a commonly used sample dataset for statistical functions in R and consists of \num{150} samples of three \textit{Iris} species (\textit{I. setosa}, \textit{I. versicolor}, \textit{I. virginica}) for which four phenotypes were measured:  sepal width, sepal length, petal width and petal length. One sample appears twice in the dataset and was removed for subsequent analyses. In order to get an understanding of the phenotype structure, I computed the pair-wise Pearson correlation coefficient across the three species and across the four phenotypes. The strongest correlation on species level is observed for  \textit{I. virginica} and \textit{I. versicolor} (\(r^2=0.9\)). On phenotype level, petal length and width correlate strongly across species (\(r^2=0.96\), \cref{fig:iris}). For the second dataset, I simulated \num{2000} data points uniformly distributed on a (x,y)-plane and transformed the plane into (x,y,z) coordinates by \(z = x \sin(x)\) and \(x = x \cos(x)\). The resulting `roll' structure is depicted in \cref{fig:roll}. These datasets represent two distinct types of data: the \textit{Iris} data is a four-dimensional dataset comprised of three subgroups, whereas the roll data is two-dimensional manifold non-linearly embedded in a three-dimensional space. 

Before applying dimensionality reduction functions to both datasets, I estimated the optimal number of neighbours for the dimensionality reduction techniques based on local neighbourhoods. For the\textit{Iris} data with \num{596} data points, the optimal number of neighbours is estimated to be \(n=26\). For the roll data with \num{2000} data points it was estimated to be \(n=36\). \Cref{fig:dimRed-iris} shows the two-dimensional representation of the \textit{Iris} data after dimensionality reduction by the four linear and eight non-linear dimensionality reduction techniques. Assuming that the allocation to species is the correct intrinsic low-dimensional representation of the \textit{Iris} dataset, I coloured the data points according to species to enable the visual comparison of the goodness of the dimensionality reduction.

\begin{figure}[h]
	\centering
			\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.7\textwidth]{Chapter4/Figures/iris.pdf}		
	\caption[\textbf{Correlation of flowering phenotypes.}]{\textbf{Correlation of flowering phenotypes.} For the \num{149} unique samples in the \textit{Iris} dataset, the pair-wise  Pearson correlation for the three different \textit{Iris} species across all measurements (A) and the four flowering phenotypes sepal width, sepal length, petal width and petal length across the three species (B) are depicted. The color scheme and shapes in the upper triangle of the matrix represent the strength and direction of the correlation, the lower triangle depicts the value of the correlation. Generated via R function \textit{corrplot::corrplot}.} 
		\label{fig:iris}
\end{figure}%

\begin{figure}[h!]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.7\textwidth]{Chapter4/Figures/DimReduction_iris.pdf}
	\caption[\textbf{Visualisation of the \textit{Iris} dataset in two dimensions.}]{\textbf{Visualisation of the \textit{Iris} dataset in two dimensions.} The number of dimensions in the \textit{Iris} dataset was reduced form four to two by the dimensionality reduction techniques described in \cref{tab:dimMethods} and computed with the functions and parameters listed in \cref{tab:dimRed-R}. The number of nearest neighbours provided to the local-proximity-based methods was estimated to be \(n=26\).} 
	 	\label{fig:dimRed-iris}
\end{figure}

\gls{pca}, i.e. the representation of the data based on the direction of highest variation in the data is able to clearly separate the \textit{I. setosa} from \textit{I. versicolor} and \textit{I. virginica} across the first principal component. However, the separation of the strongly correlated \textit{I. versicolor} and \textit{I. virginica} species based on the first two principal components alone is not possible. \gls{mds} with Euclidean distance is equivalent to \gls{pca} and the resulting \gls{mds} plot is a mirror image of the \gls{pca} result on the x-axis. \gls{ica} for this dataset shows the strong influence of the pre-processing via \gls{pca}, as it is the mirror image of the \gls{pca} result on the x- and y-axis. \gls{peer} is capable of separating \textit{I. setosa} from the other species, but similarily fails at completely separating \textit{I. versicolor} and \textit{I. virginica}. 
Visually the best results of the non-linear methods are obtained from \gls{drr}, Isomap and \gls{nmds} and perform similarly in their ability to separate the species as the linear methods. The other non-linear methods are able to separate \textit{I. setosa}, but do worse in separating the other two species.

\begin{figure}[p]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip,width=0.75\textwidth]{Chapter4/Figures/roll-viridis.pdf}\\
	\caption[\textbf{Three-dimensional embedding of datapoints lying on a two-dimensional plane.}]{\textbf{Three-dimensional embedding of data points lying on a two-dimensional plane.} Data points uniformly distributed on a (x,y)-plane (A) are transformed into (x,y,z) coordinates by \(z = x \sin(x)\) and \(x = x \cos(x)\). The color scheme simply represents the location in x-direction of the (x,y)-plane.  Generated via R function \textit{plot3D::scatter3D}}
 	\label{fig:roll}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.7\textwidth]{Chapter4/Figures/DimReduction_roll.pdf}
	\caption[\textbf{Visualisation of the roll dataset in two dimensions.}]{\textbf{Visualisation of the roll dataset in two dimensions.} The dimensionality reduction methods described in \cref{tab:dimMethods} were analysed for their ability to recover the original 2D plane embedded into 3D space (\cref{fig:roll}). The 2D-representation was computed with the functions and parameters listed in \cref{tab:dimRed-R}, with the number nearest neighbours provided to the local-proximity-based methods estimated to be \(n=36\). } 
	 	\label{fig:dimRed-roll}
\end{figure}


The results of the dimensionality reduction for the non-linear projection of the 2D manifold into 3D space demonstrate the difficulty of the linear methods to deal with non-linear structures (\cref{fig:dimRed-roll}). The color scheme of the original embedding simply represents the location of points in the 2D plane ordered in x-direction. In a good low-dimensional representation, one should be able to observe the gradient of the original (x,y)-plane linearly across either one of the dimensions. While the general order of the points is conserved in the low-dimensional representation for the linear methods, none are able to separate them linearly in either dimension (\cref{fig:dimRed-roll}\subfig{A}). \gls{peer} performs best in capturing the spread in y-direction compared to the other linear methods, but equally fails in separating the tight curvature x-direction. In contrast, the non-linear method Isomap completely recovers the original 2D plane. DiffusionMap and Laplacian Eigenmaps are able to separate the structure linearly, but underestimate the spread of the original data in y-direction. \gls{lle} recovers the spread in y-direction, but fails to find the order in x-direction for the tight curvature (dark colors) in the 3D space. \gls{drr}, \gls{nmds} and \gls{tsne} suffer from the same issues as the linear methods, with \gls{drr} additionally introducing non-smoothness. \gls{kpca} recovers the plane structure for the mid-section of the roll, but scrambles the order at both ends. 

The visualisations clearly demonstrate the difference in ability of the dimensionality reduction methods to find a good low-dimensional representation of the original, known data structures. As a generalisation and unsurprisingly, linear methods perform well in separating linear data structures (\textit{Iris} data) but fail in in recovering non-linear structures (roll data). Non-linear methods perform better in recovering the non-linear structure, but underperform on linear datasets compared to the linear methods. 

\section{Quantification of dimensionality reduction performance}
\label{section:Quantification-DimRed}
In addition to the visualisation, it would be desirable to have a quantitative assessment of the performance of the dimensionality reduction techniques. Lee and colleagues \citeyear{Lee2009} reviewed different methods for evaluating the quality of dimensionality reduction methods. Two criteria for the goodness of the low-dimensional representation contained in three out of the five methods reviewed are the closeness of neighbouring samples in the low-dimensional space compared to the original space (trustworthiness of the projection) and the conservation of original neighbourhoods in the low-dimensional space (continuity of the projection). Kaski and colleagues \citep{Kaski2003} proposed two metrics quantifying the extend of trustworthiness and continuity based on the ranking of \(k\) neighbours in the original and low-dimensional space. For trustworthiness, they define \(r(x_i, x_j)\) as the rank of the distance of \(x_j\) to  \(x_i\) in the original data space and \(U_k(x_i)\) as the set of \(x_{j \neq i}\) that are in the neighbourhood of \(x_i\) in the low-dimensional space but not in the original space. Similarly, continuity is based on \(\hat{r}(x_i, x_j)\), the rank of the distance of \(x_j\) to  \(x_i\) in the low-dimensional space and \(V_k(x_i)\) as the set of \(x_{j \neq i}\) that are in the neighbourhood of \(x_i\) in the original space but not in the low-dimensional space. The trustworthiness \(T\) and the continuity \(C\) are defined as:

\begin{equation}
T =  1- A(k)\sum^{N}_{i=1}\sum^{}_{x_j \in U_k(x_i)}(r(x_i, x_j) - k)
\label{eq:trustworthiness}
\end{equation}
and 
\begin{equation}
C =  1- A(k)\sum^{N}_{i=1}\sum^{}_{x_j \in V_k(x_i)}(\hat{r}(x_i, x_j) - k),
\label{eq:continuity}
\end{equation}

where \(A(k) = \frac{2}{Nk(2N -3k -1)}\) is introduced as a normalising parameter scaling the values between zero and one. The projection into low-dimensional space is considered trustworthy if the set of \(k\) closest neighbours of a sample in the low-dimensional space are also close in the original space. Continuity quantifies how well the original neighbourhoods are preserved, i.e. it measures if there are neighbourhoods of \(k\) points in the original space which are not preserved because of discontinuities in the low-dimensional space. 

I applied these metrics to the results of the low-dimensional projections obtained in \cref{section:visualisation}. Both metrics are dependent on the number of \(k\) neighbours that they are evaluated on, so I chose different neighbourhood sizes ranging from \numrange{1}{3}\% of samples (rows) in the dataset. The results are depicted in \cref{fig:TandC-iris} and \cref{fig:TandC-roll}. \gls{tsne}, \gls{lle}, the \gls{pca}-derived linear methods (\gls{pca}, \gls{ica}, \gls{mds}) and \gls{nmds} have a trustworthiness measure of more than \num{0.95} across all neighbourhood sizes in the \textit{Iris} data (\cref{fig:TandC-iris}\subfig{A}). The \gls{pca}-derived non-linear method \gls{drr} performs slightly worse, as do \gls{kpca} and Isomap. Laplacian Eigenmaps perform worst only reaching \num{0.9} for high neighbourhood sizes. In general, the dependency of the local methods on neighbourhood size becomes apparent, as the kernel-eigenmap methods' trustworthiness varies strongest across the different neighbourhood sizes. The six methods performing well in terms of trustworthiness for the \textit{Iris} data (\gls{tsne}, \gls{lle}, \gls{pca}, \gls{ica}, \gls{mds} and \gls{nmds}) also keep the level of discontinuities introduced in the low-dimensional space low as seen by high measures of continuity (\cref{fig:TandC-iris}\subfig{B}). To get an estimate for \(T\) and \(C\) for a poor representation of the original data, I randomly chose neighbourhoods in the original space and computed trustworthiness and continuity measures for these and the original \textit{Iris} data, leading to median measurements of \num{0.51} for both \(T\) and \(C\) (results not shown in graphic to allow for a clearer visualisation of the trustworthiness range \numrange{0.85}{1}).  For the roll data, Isomap has by far the best performance in terms of trustworthiness (\cref{fig:TandC-roll}\subfig{A}) and confirms the visual results (\cref{fig:dimRed-roll}\subfig{B}). \gls{lle} and \gls{nmds} also score above \num{0.9}. These three methods together with \gls{kpca} and DiffusionMaps are best in preserving continuities (\cref{fig:TandC-roll}\subfig{B}). The trustworthiness for all linear methods is similar and consistently lower than the best scoring non-linear methods. The worst results in terms of continuity are observed for \gls{tsne} and \gls{drr} and both methods show discontinuities in the visualisation (\cref{fig:dimRed-roll}\subfig{B}).  For the reference point of trustworthiness and continuity based on random neighbourhoods, results similar to those found for the \textit{Iris} dataset were observed, with median \(T=0.52\) and \(C=0.52\).

Overall, the trustworthiness and continuity measures reflect the results obtained from the visualisation of the data by their low-dimensional representation: linear methods are most suitable for linear data and non-linear methods for non-linear data. 

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.9\textwidth]{Chapter4/Figures/TandC_iris.pdf}
	\caption[\textbf{Quality of the dimensionality reduction in the \textit{Iris} dataset.}]{\textbf{Quality of the dimensionality reduction in the \textit{Iris} dataset.} The trustworthiness (A)  and Continuity (B) of the projections into the low-dimensional space for the \textit{Iris} dataset were computed according to \cref{eq:trustworthiness} and \cref{eq:continuity}. The neighbourhood sizes ranged from one to five neighbours, corresponding to \numrange{0.6}{3.4}\% of samples.}
	 	\label{fig:TandC-iris}
\end{figure}


\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.9\textwidth]{Chapter4/Figures/TandC_roll.pdf}
	\caption[\textbf{Quality of the dimensionality reduction on the 2D manifold embedded in 3D.}]{\textbf{Quality of the dimensionality reduction on the 2D manifold embedded in 3D.} The trustworthiness (A)  and Continuity (B) of the projections into the low-dimensional space for the 2D manifold were computed according to \cref{eq:trustworthiness} and \cref{eq:continuity}. The neighbourhood sizes ranged from \numrange{10}{50} neighbours, corresponding to \numrange{1}{5}\% of samples.} 
	 	\label{fig:TandC-roll}
\end{figure}

