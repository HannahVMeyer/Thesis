\section{Visualisation of data structures by dimensionality reduction}
\label{section:visualisation}
In high-dimensional data analysis, one is often interested in finding a clear visualisation of the data, which does not lead to a loss of information and is capable of summarizing underlying data structures. Data structures can be of biological origin, representing features of interest such as cell populations or tissue types or of technical origin such as batch effects. In high-dimensional datasets, visualisation requires either \textit{a prior} selection of the dimensions of the original data to be displayed or the reduction to a dimension that can be presented. Common choices of dimensionality reduction for this task are PCA or tSNE \citep{Deng2014,Crowley2015,Corces2016,Martinez-Jimenez2017,Huisman}. In the following, I analyse the low-dimensional representations of \num{2} different dataset generated by the \num{12} dimensionality reduction techniques described above.

For each of the techniques, I used corresponding functions already implemented in publically available R-packages. \Cref{tab:dimRed-R} summarises the R packages, functions and their parameters used for the dimensionality reduction. Most functions require specification of the expected number of dimensions \(ndim\). For the purpose of visualisation in a cartesian coordinate system, this parameter choice is straight-forward and was set to  \(ndim=2\). In the case of kernel eigenmap methods and tSNE, the number of \(k\) nearest neighbours used in the graph construction and probability function have to be provided. This task is less intuitive and different algorithms have been implemented to estimate the optimal number of neighbours for the reconstruction. For any method that required specification of \(k\), I provided \(k\) estimated according to the method proposed by Kayo \citep{Kayo2006}, implemented as the function \textit{calc}\_\textit{k} in the \textit{lle} package. In addition, for some of the methods, there are function-specific parameters which have to be provided. These are either specified in \cref{tab:dimRed-R} or the default setting was chosen. For functions that required a distance matix or metric for the local neighbourhood estimation (MDS, DiffusionMap, Isomap, nMDS), the default is the Euclidean distance. Methods that require a kernel function (DRR, kPCA) use a gaussian radial basis kernel by default. For ICA and DRR, I choose the default setting for doing a pre-processing step via PCA. For PEER, the functions are implemented in an object-oriented manner and I followed the protocol described in \citep{Stegle2012}. I choose to include the optional parameter of adjusting for the mean.  

% Table generated by Excel2LaTeX from sheet 'HighDimR'
\begin{table}[htbp]
  \centering
  \caption[\textbf{R functions for dimensionality reduction methods and their parameters.}]{\textbf{R functions for dimensionality reduction methods and their parameters.} Most functions require \textit{a priori} specification of the number of \(k\) nearest neighbours and the expected intrinsic dimensionality \(ndim\). Any function-specific parameters, which have to be provided or were chosen different from the default settings are listed. The reference column specifies the publications the R packages are based on.}
    \begin{tabular}{llll}
    \toprule
    Name  & R function & Parameters & Reference \\
    \midrule
    PCA   & stats::prcomp &  -    & \citep{Hoteling1933} \\
    PEER  & peer  & ndim,  & \citep{Stegle2010} \\
    \multirow{2}[0]{*}{ICA} & \multirow{2}[0]{*}{fastICA::fastICA} & ndim, fun=logcosh, & \multirow{2}[0]{*}{\citep{Hyvarinen2000}} \\
          &       &  method="C" &  \\
    MDS   & stats::cmdscale & ndim  & \citep{Gower1966} \\
    nMDS  & vegan::metaMDS & ndim  & \citep{Ripley1996} \\
    DRR   & DRR::drr &  -    & \citep{Laparra2015} \\
    kPCA  & kernlab::kpca &  -    & \citep{Schoelkopf1998} \\
    \multirow{2}[0]{*}{Isomap} & \multirow{2}[0]{*}{vegan::isomap} & ndim, k, & \multirow{2}[0]{*}{\citep{Tenenbaum2000}} \\
          &       & fragmentedOK=TRUE &  \\
    LLE   & lle::lle & ndim, k & \citep{Ridder2002} \\
    Laplacian Eigenmaps & loe::LOE & ndim, k & \citep{Belkin2003} \\
    DiffusionMaps & diffusionMap::diffuse & k     & \citep{Lafon2006} \\
    tSNE  & Rtsne::Rtsne & ndim, k & \citep{Maaten2008} \\
    \bottomrule
    \end{tabular}%
 \label{tab:dimRed-R}%
\end{table}%

The first dataset is a commonly used sample dataset for statistical functions in R and consists of \num{150} samples of \num{3} iris species (setosa, versicolor, virginica) for which \num{4} phenotypes were meassured:  sepal width, sepal length, petal width and petal length. One sample appears twice in the dataset and was removed for subsequent analyses. In order to get an understanding of the phenotype structure, I computed the pair-wise Spearman correlation coefficent across the \num{3} species and across the \num{4} phenotypes. The strongest correlation on species level is observed for viginica and versicolor (\(r^2=0.9\)). On phenotype level, petal length and width correlate strongly across species (\(r^2=0.96\), \cref{fig:iris}). For the second dataset, I simulated \num{2000} data points uniformly distributed on a (x,y)-plane and transformed the plane into (x,y,z) coordinates by \(z = x \sin(x)\) and \(x = x \cos(x)\). The resulting `roll' structure is depicted in \cref{fig:roll}. These datasets represent two distinct types of data: the iris data is a \num{4}-dimensional dataset comprised of \num{3} subgroups, whereas the roll data is two-dimensional manifold non-linearly embedded in a \num{3} dimensional space. 

Before applying dimensionality reduction functions to both datasets, I estimated the optimal number of neighbours for the dimensionality reduction techniques based on local neighbourhoods. For the iris data with \num{596} data points, the optimal number of neighbours is estimated to be \(k=26\). For the roll data with \num{2000} data points it was estimated to be \(k=36\). \Cref{fig:dimRed-iris} shows the two-dimensional representation of the iris data after dimensionality reduction by the \num{4} linear and eight non-linear dimensionality reduction techniques. The coloring of the data points according to species enables the visual comparison of the goodness of the dimensionality reduction.

\begin{figure}[h]
	\centering
			\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.6\textwidth]{Chapter4/Figures/iris.pdf}		
	\caption[\textbf{Correlation of flowering phenotypes.} Generated via R function \textit{corrplot::corrplot}]{\textbf{Correlation of flowering phenotypes.} For the \num{149} unique samples in the iris dataset, the pair-wise Spearman correlation for the \num{3} different iris species across all meassurements (A) and the \num{4} flowering phenotypes sepal width, sepal length, petal width and petal length across the \num{3} species (B) are depicted. The colorscheme and shapes in the upper triangle of the matrix represent the strength and direction of the correlation, the lower triangle depicts the value of the correlation.} 
		\label{fig:iris}
\end{figure}%

\begin{figure}[h!]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.7\textwidth]{Chapter4/Figures/DimReduction_iris.pdf}
	\caption[\textbf{Visualisation of the iris dataset in two dimensions.}]{\textbf{Visualisation of the iris dataset in two dimensions.} The number of dimensions in the iris dataset was reduced form \num{4} to two by the dimensionality reduction techniques described in \cref{tab:dimMethods} and computed with the functions and parameters listed in \cref{tab:dimRed-R}. The number of nearest neighbours provided to the local-proximity-based methods was estimated to be \(k=26\).} 
	 	\label{fig:dimRed-iris}
\end{figure}

 PCA, i.e. the representation of the data based on the direction of highest variation in the data is able to clearly separate the septosa species from versicolor and virginica across the first principal component. However, the separation of the strongly correlated versicolor and virginica species based on the first two principal components alone is not possible. MDS with Euclidean distance is equivalent to PCA and the resulting MDS plot is a mirror image of the PCA result on the x-axis. The ICA result for this dataset shows the strong influence of the pre-processing via PCA, as it is the mirror image of the PCA result on the x- and y-axis. PEER is capable of separating septosa from the other species, but similarily fails at completely separating versicolor and virginica. 
Visually the best results of the non-linear methods are obtained from DRR, Isomap and nMDS and perform similarly in their ability to separate the species as the linear methods. The other non-linear methods are able to separate setosa, but do worse in separating the other two species.


\begin{figure}[p]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip,width=0.75\textwidth]{Chapter4/Figures/roll-viridis.pdf}\\
	\caption[\textbf{\num{3}-dimensional embedding of datapoints lying on a two-dimensional plane.} Generated via R function \textit{plot3D::scatter3D}]{\textbf{\num{3}-dimensional embedding of data points lying on a two-dimensional plane.} Data points uniformly distributed on a (x,y)-plane (A) are transformed into (x,y,z) coordinates by \(z = x \sin(x)\) and \(x = x \cos(x)\). The colorscheme simply represents the location in x-direction.}
 	\label{fig:roll}
\end{figure}

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.7\textwidth]{Chapter4/Figures/DimReduction_roll.pdf}
	\caption[\textbf{Visualisation of the roll dataset in two dimensions.}]{\textbf{Visualisation of the roll dataset in two dimensions.} The dimensionality reduction methods described in \cref{tab:dimMethods} were analysed for their ability to recover the original 2D plane embedded into 3D space (\cref{fig:roll}). The 2D-representation was computed with the functions and parameters listed in \cref{tab:dimRed-R}, with the number nearest neighbours provided to the local-proximity-based methods estimated to be \(k=36\). } 
	 	\label{fig:TandC-roll}
\end{figure}


The results of the dimensionality reduction for the non-linear projection of the 2D manifold into 3D space demonstrate the difficulty of the linear methods to deal with non-linear structures (\cref{fig:dimRed-roll}). The color scheme of the original embedding simply represents the location of points in the 2D plane ordered in x-direction. In a good low-dimensional representation, one should be able to observe the gradient of the original (x,y)-plane linearly across either one of the dimensions. While the general order of the points is conserved in the low-dimensional representation for the linear methods, none are able to separate them linearly (\cref{fig:dimRed-roll}\subfig{A}). In contrast, the non-linear method Isomap completely recovers the original 2D plane. DiffusionMap and Laplacian Eigenmaps are able to separate the structure linearly, but underestimate the spread of the original data in y-direction. LLE recovers the spread in y-direction, but fails to find the order in x-direction for the tight curvature (dark colors) in the 3D space. DRR, nMDS and tSNE suffer from the same issues as the linear methods, with DRR additionally introducing non-smoothness. kPCA recoveres the plane structure for the mid-section of the roll, but scrambles the order at both ends. 

The visualisations clearly demonstrate the difference in abilty of the dimensionality reduction methods to find a good low-dimensional representation of the original, known data structures. As a generalisation, linear methods perform well in separating linear data structures (iris data) but fail in in recovering non-linear structures (roll data). Non-linear methods perform better in recovering the non-linear structure, but underperfom on linear datasets compared to the linear methods. 

\section{Quantification of dimensionality reduction performance}
\label{section:Quantification-DimRed}
In addition to the visualisation, it would be desirable to have a quantitative assessment of the performance of the dimensionality reduction techniques. Lee and colleagues \citeyear{Lee2009} reviewed different methods for evaluating the quality of dimensionality reduction methods. Two criteria for the goodness of the low-dimensional representation contained in \num{3} out of the \num{5} methods reviewed are the closeness of neighbouring samples in original and corresponding low-dimensional space (trustworthiness of the projection) and the continuity of the projection. Kaski and colleagues \citep{Kaski2003} proposed \num{2} metrics quantifying the extend of trustworthiness and continuity based on the ranking of \(k\) neighbours in the original and low dimensional space. For trustworthiness, they define \(r(x_i, x_j)\) as the rank of the distance of \(x_j\) to  \(x_i\) in the original data space and \(U_k(x_i)\) as the set of \(x_{j \neq i}\) that are in the neighborhood of \(x_i\) in the low-dimensional space but not in the original space. Similarily, continuity is based on \(\hat{r}(x_i, x_j)\), the rank of the distance of \(x_j\) to  \(x_i\) in the low-dimensional space and \(V_k(x_i)\) as the set of \(x_{j \neq i}\) that are in the neighborhood of \(x_i\) in the original space but not in the low-dimensional space. The trustworthiness \(T\) and the continuity \(C\) are defined as:

\begin{equation}
T =  1- A(k)\sum^{N}_{i=1}\sum^{}_{x_j \in U_k(x_i)}(r(x_i, x_j) - k)
\label{eq:trustworthiness}
\end{equation}
and 
\begin{equation}
C =  1- A(k)\sum^{N}_{i=1}\sum^{}_{x_j \in V_k(x_i)}(\hat{r}(x_i, x_j) - k),
\label{eq:continuity}
\end{equation}

where \(A(k) = \frac{2}{Nk(2N -3k -1)}\) is introduced as a normalising parameter scaling the values between \numrange{0}{1}. The projection into low dimensional space is considered trustworthy if the set of \(k\) closest neighbors of a sample in the low-dimensional space are also close in the original space. Continuity quantifies how well the original neighborhoods are preserved, i.e. it measures if there are neighborhoods of \(k\) points in the original space which are not preserved because of discontinuities in the low-dimensional space. 

I applied these metrics to the results of the low-dimensional projections obtained in \cref{section:visualisation}. Both metrics are dependent on the number of \(k\) neighbours that they are evaluated on, so I chose different neighbourhood sizes ranging from \numrange{1}{3}\% of samples (rows) in the dataset. The results are depicted in \cref{fig:TandC-iris} and \cref{fig:TandC-roll}. tSNE, LLE, the PCA-derived linear methods (PCA, ICA, MDS) and nMDS have a trustworthiness measure of more than \num{0.95} across all neighbourhood sizes in the iris data (\cref{fig:TandC-iris}\subfig{A}). The PCA-derived non-linear method DRR performs slightly worse, as do kPCA and Isomap. Laplacian Eigenmaps perform worst with only reaching \num{0.9} for high neighbourhood sizes. In general, the dependency of the local methods on neighbourhood size becomes apparent, as the kernel-eigenmap methods' trustworthiness varies strongest across the different neighbourhood sizes. The \num{6} methods performing well in terms of trustworthiness for the iris data (tSNE, LLE, PCA, ICA, MDS and nMDS) also keep the level of discontinuities introduced in the low-dimensional space low as seen by high meassures of continuity (\cref{fig:TandC-iris}\subfig{B}). To get an estimate for \(T\) and \(C\) for a poor representation of the original data, I randomly chose neighbourhoods in the original space and computed trustworthiness and continuity meassures for these and the original iris data, leading to median meassurements of \num{0.51} for both \(T\) and \(C\) (results not shown in graphic to allow for a clearer visualisation of the trustworthiness range \numrange{0.85}{1}).  For the roll data, Isomap has by far the best performance in terms of trustworthiness (\cref{fig:TandC-roll}\subfig{A}) and confirms the visual results (\cref{fig:dimRed-roll}\subfig{B}). LLE and nMDS also score above \num{0.9}. These \num{3} methods together with kPCA and DiffusionMaps are best in preserving continuities (\cref{fig:TandC-roll}\subfig{B}). The trustworthiness for all linear methods is similar and consistently lower than the best scoring non-linear methods. The worst results in terms of continuity are observed for tSNE and DRR and both methods show discontinuities in the visualisation (\cref{fig:dimRed-roll}\subfig{B}).  For the reference point of trustworthiness and continuity based on random neighbourhoods, results similar to those found for the iris dataset were observed, with median \(T=0.52\) and \(C=0.52\).

Overall, the trustworthiness and continuity meassures reflect the results obtained from the visualisation of the data by their low-dimensional representation: linear methods are most suitable for linear data and vice versa. 

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.9\textwidth]{Chapter4/Figures/TandC_iris.pdf}
	\caption[\textbf{Quality of the dimensionality reduction in the iris dataset.}]{\textbf{Quality of the dimensionality reduction in the iris dataset.} The trustworthiness (A)  and Continuity (B) of the projections into the low-dimensional space for the iris dataset were computed according to \cref{eq:trustworthiness} and \cref{eq:continuity}. The neighbourhood sizes ranged from \numrange{1}{5} neighbours, corresponding to \numrange{0.6}{3.4}\% of samples.}
	 	\label{fig:TandC-iris}
\end{figure}


\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.9\textwidth]{Chapter4/Figures/TandC_roll.pdf}
	\caption[\textbf{Quality of the dimensionality reduction on the 2D manifold embedded in 3D.}]{\textbf{Quality of the dimensionality reduction on the 2D manifold embedded in 3D.} The trustworthiness (A)  and Continuity (B) of the projections into the low-dimensional space for the 2D manifold were computed according to \cref{eq:trustworthiness} and \cref{eq:continuity}. The neighbourhood sizes ranged from \numrange{10}{50} neighbours, corresponding to \numrange{1}{5}\% of samples.} 
	 	\label{fig:dimRed-roll}
\end{figure}

