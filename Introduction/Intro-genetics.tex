\chapter{Introduction}
The field of quantitative genetics has come far since Fisher's initial studies on human growth traits in 1918. Although the concept of inheritance existed at this time, little was known about the molecule responsible. The discovery of the DNA structure in the 1950s and technical break-throughs in analysing its sequence in the following decades have allowed to investigate genetic variance on a detailed scale, moving from whole chromosomes and linkage studies to the analysis of DNA variation on a single-base pair level. 

The developments in genotyping and sequencing technologies in recent years have made large scale studies on genetic variation feasible. With the sinking costs of genotyping techniques, the number of samples has risen and studies investigating the effects of single DNA bases often comprise thousands of individuals, in particular in the field of human genetics.  Together with the increased number of samples, the number of phenotypes that are measured for each individual has grown from a few measurements to tens, hundreds or even thousands. The availability of these rich datasets provides great opportunities when studying the influence of genetic variation on phenotypic variance. However, it also poses technical challenges when analysing these datasets. 

In this thesis, I identified some of these challenges and propose new methods for the genetic analysis of high-dimensional datasets. These new methods are first explored on simulation studies and subsequently applied to real datasets. Specifically, I developed a new approach for the joint genetic association testing of a large number of phenotypic traits and applied this method to a publicly available dataset of yeast growth traits. I explored different dimensionality reduction methods for very high-dimensional datasets and propose a new measure to define the stability of the dimensionality reduction. Finally, I analysed human heart morphology data for genetic associations, applying the methods from the dimensionality reduction study on simulated data. In this introduction, I will first give a general overview of the history and methods in quantitative genetics, followed by the description of statistical models relevant for this thesis. In order to help with an understanding of the genetic association studies on the human heart morphology data, I also introduce basic concepts of cardiac structure and development and their underlying genetics. 

\section{From ancient ideas of inheritance to the birth of modern genetics}
The formulation of the concept of human inheritance --the passing on of traits from parents to offspring-- can already be found in works of Hippocrates and Aristotle. In addition to their theory of the inheritance of acquired traits, Hippocrates and Democritus also describe a possible mechanism of inheritance \citep{Zirkle1935}, a concept later formalised as \textquote{pangenesis} by the English naturalist Charles Darwin \parencite*{Darwin1868} and others such as the French Comte de Buffon \parencite*{Buffon1749} and Genevan naturalist Charles Bonnet \parencite*{Bonnet1779}. The theory of pangenesis -- which translates to whole (Greek: pan) origin (Greek: genesis) or birth (Greek: genos) -- describes how the entire parental organism participates in passing on traits to the offspring. In this developmental theory of heredity, all cells in an organism were believed to secrete small particles called gemmules, which circulate through the body to congregate in the gonads. While this theory was quickly refuted, Darwin became renowned for his ideas about trait variation and the link to inheritance. In his famous work \textit{On the Origin of Species} \parencite*{Darwin1859}, he postulates natural selection as the central concept of evolution, based on his observations of phenotypic variance in a population, differential fitness based on phenotype and the concept of heritability of this fitness \citep{Lewontin1970}. 
\\
\\
The milestones in genetics made since Darwin's work \textit{On the Origin of Species} as well as accompanying statistical models and techniques in molecular biology are depicted in \cref{fig:timeline-genetics}.

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=\textwidth]{Introduction/Figures/TimelineQuantGenetics.pdf}
	\caption[\textbf{Genetics over time. }]{\textbf{Genetics over time. } A. Statistical concepts and B. techniques in molecular biology crucial for the advances in genetics. C. The developments in genetics from its birth by Mendel's Laws of Inheritance to large databases cataloguing genetic variation of thousands of individuals. Whilst there are many independent studies in all three areas contributing to the successes in genetics that we observe today, I have attempted to depict all major events that lead to the specific field of human quantitative genetics in the GWAS era. The development of mathematical models is focused towards models used in this thesis. The legend below the timelines specifies the symbols of the organisms used in the respective studies. As references for each entry, the first author of the corresponding publication is shown. Discoveries where multiple authors are named indicate independent studies at the same time making the same discovery/developing techniques. } 
	 	\label{fig:timeline-genetics}
\end{figure}


\subsection{Mendelian Laws of Inheritance}
The Austrian friar Gregor Mendel was the first to systematically study the mechanisms of heritability. By cross-breeding different varieties of pea plants, he was able to follow the inheritance patterns of a number of visually-observable traits such as flower color, seed shape and plant height. In 1866, he presented his observations in the paper \textit{Versuche ueber Pflanzenhybride} (experiments on plant hybridisation) where he proposes three general concepts of inheritance which later became known as the Mendelian Laws of Inheritance: i) the Law of Independent Segregation (every individual contains two alleles for each trait which segregate in germ cells leading to a random transmission of alleles to the offspring), ii) the Law of Independent Assortment (traits are inherited independently of each other) and iii) the Law of Dominance (recessive alleles will be masked by dominant alleles and the trait corresponding to the dominant allele will be observed) \citep{Mendel1866}. Although his work stayed widely unnoticed during his lifetime, his meticulous studies and documentation ensured his recognition as the father of genetics. In 1900, his work was independently rediscovered by the Dutch botanist Hugo de Vries \citep[translation into English]{deVries1900,Hannah1950} and --although contested by some based on their seeming lack in understanding of Mendel's work \citep{Keynes2008,Monaghan1986,Monaghan1987}-- the German botanist Carl Correns \citep[translation into English]{Correns1900,Piernick1950} and the Austrian agronomist Erich Tschermak \parencite*{Tschermak1900}. 

Around the same time, the British geneticist William Bateson set out to make Mendel's work accessible to the scientists not proficient in Mendel's native language German. He translated Mendel's original papers on the Law's of inheritance \citep{Mendel1866} and cross-breeding studies in \textit{Hieracium} \citep{Mendel1869} into English and published them in \textit{Mendel's Principles of Heredity: a Defense} \citep{Bateson1902}. In \parencite*{Bateson1909}, Bateson published an extended version of his original book which allowed Mendel's work to become known in the greater scientific world \citep{Keynes2008}, more than 40 years after their original publication.

In addition to the rediscovery and translation of Mendel's ideas into English, two other branches of investigations contributed to the understanding of heredity and the identification of the molecular basis of the Laws of Inheritance from 1900 onward: biometrics and molecular biology. 

\subsection{Biometrics}
Inspired by Darwin's work on evolution, his half-cousin Francis Galton was interested in mathematically describing and analysing evolutionary concepts. In 1886, he published the \textit{Regression Towards Mediocrity in Hereditary} \textit{Stature}, offering a statistical approach towards understanding inheritance. Based on measurements of height in parents and their children, he observed that the \textquote{[t]he height-deviate of the offspring is, on the average, two-thirds of the height-deviate of its mid-parentage}. He achieved the quantification of the deviation from the mean by fitting straight lines to the observed heights and finding their slope, thereby developing the technique of linear regression analysis and introducing the concept of correlation \citep{Galton1886}. An extension of this work and descriptions of different statistical distributions and processes in heredity were published in his book \textit{Natural inheritance} \citep{Galton1889}. Karl Pearson formalised and extended Galton's statistical models for quantifying the effects of inheritance on trait variance by introducing the concept of p-values, the Chi-sqare test and principal component analysis \citep{Pearson1900,Pearson1901}. The marine biologist Walter Weldon applied these statistical concepts to data he had collected on shrimps and crabs \citep{Weldon1890, Weldon1892}, demonstrating selection in natural populations. Together, Galton, Pearson and Weldon are known as the founders of biometrics, the science of applying statistical methods to the study of evolution on quantitative traits, or as Galton described it: \textquote{The primary object of Biometry is to afford material that shall be exact enough for the discovery of incipient changes in evolution which are too small to be otherwise apparent.} \citep[editorial]{Galton1901}. Despite the progress in understanding evolution in the light of statistical concepts, their direct study of heredity was impeded by their reluctance to acknowledge the validity of Mendelian genetics \citep{Bulmer2003}.

\subsection{Molecular basis of inheritance}
Advances in understanding the molecule responsible for inheritance were made by the Swiss physican and biologist Johannes Miescher and the German anatomist Walther Flemming.  \citet{Miescher1871} was the first to successfully isolate a substance he called \textit{nuclein} --later known as DNA-- from the nucleus . Flemming's experiments on salamander cells lead to the discovery of structures that could easily be stained by basophilic dies and he named them \textit{chromatin} --  \textquote{coloured material} (greek:khrōmat). He later found chromatin to be originating from the cell nucleus and did further studies into understanding cell division and mitosis \parencite*{Flemming1878}. Although both Miescher's and Flemming's methods and discoveries were crucial in the later identification of DNA as the carrier of inheritance, neither of them made the connection at the time. 
\\
With these advances in molecular and statistical techniques and the rediscovery of the Mendelian laws, the new discipline of genetical research attracted much attention. 

\section{The Laws of Inheritance on a cellular level}
The first two scientists proposing how Mendel's Laws could work on a cellular level were the German biologist Theodor Boveri and the American Walter Sutton. By experimentally introduced double-fertilisations of sea urchin eggs and subsequent observations of developmental processes in the resulting embryos, Boveri claimed \foreignquote{german}{dass eine bestimmte Kombination von Chromosomen zur normalen Entwicklung notwendig ist, und dieses bedeutet nichts anderes, als dass die einzelnen Chromosomen verschiedene Qualit\"{a}ten besitzen m\"{u}ssen.}  i.e. \textquote{that a specific combination of chromosomes is necessary for a normal development which in turn means that each chromosome must harbour different qualities} \citep{Boveri1902}. 

At the same time, Sutton described his observations in reduction division (later known as meiosis) and postulated that different chromosomes play different roles in development. Similar to Boveri, he came to the conclusion that \textquote{the phenomena of germ cell division and of heredity are seen to have the same essential features \textelp{}, with purity of units (chromosomes, characters) and the independent transmission of the same} \parencite*{Sutton1903}. Both studies demonstrated the link between the Mendelian Laws of Inheritance and chromosomes as its carrier and are the basis for the chromosome theory of inheritance, also known as Boveri-Sutton Chromosome theory. 

Around the same time, Bateson worked together with Edith Saunders and Reginald Punnett on experiments similar to Mendel's pea hybrids to understand the physiology of heredity. While they confirmed Mendel's original observations, they also discovered traits whose segregation did not follow the Law of Independent Assortment. Although they could not explain the mechanism of these observations, their results lead them to propose the concept of coupling or co-inheritance of traits \citep{Bateson1905}. 

With the progress in understanding Mendelian Laws on a cellular level came the establishment of terms describing certain entities and properties that are still in use today. In addition to his scientific contributions and his translation of Mendel's works into English, Bateson became known for coining key terms in the field of genetics, even the term genetics itself \citep{Dunwell2007}. He defined the units of inheritance transmission as allelomorphs, which became later abbreviated as alleles and introduced the terms homozygote and heterozygote for individuals carrying the same or different allelomorphs \citep{Bateson1902}. The word gene as a term for the Mendelian factors or units of inheritance was introduced by the Danish botanist \citet{Johannsen1911}. He also introduced the terms phenotype as the outward appearance of an individual and genotype as their genetic traits. The terms polygenetic, for traits that are governed by multiple genes \citep{East1910} and pleiotrophic, for genes that affect multiple, seemingly unrelated phenotypes \citep[page 597]{Plate1910} also made their first appearance at that time. While these terms are standard in today's field of genetics, their use in that time only rose slowly over time. For simplicity, however, I will from now on refer to any description of Mendelian factors or units as genes. 

\section{Genetic linkage}
The American embryologist Thomas Morgan was critical of the ideas of Mendelian inheritance and chromosomes as its carrier \citep{Allen1968}, yet he would become a crucial figure in establishing the chromosomal theory of heredity and introducing other important concepts of inheritance. In his famous Fly Room at Columbia University, he worked on mutation and breeding experiments in the fruit fly \textit{Drosophila Melanogaster} aiming to discover mutations that would lead to the emergence of new species, as described in De Vries' mutation theory \citep{Allen1968}. Instead, his experiments on fruit fly mutants for eye color (white instead of red) showed that the pattern of inheritance of the mutant trait followed the Mendelian Law of Dominance. In addition, he discovered that the factor determining eye color was linked to the factor for sex determination \citep{Morgan1910,Morgan1911a} pointing towards the coupling of traits as observed by Bateson.

In subsequent years, Morgan and his students carried out extensive research on mutant fruit flies which lead to the discovery of crossing over (exchange of paternal and maternal chromosomal material during meiosis) and the formalisation of the concept of genetic linkage \citep{Morgan1911b}. Based on the hypothesis that the degree of linkage between phenotypes would be inversely correlated to the linear distance of their genes on a chromosome, they developed the technique of genetic mapping: the localisation of genes underlying phenotypes on the basis of correlation with inheritance patterns [DNA variation], without the need for prior hypotheses about biological function. Using this technique, where the recombination rate between traits is used to estimate the relative distance of their genes, his student \citet{Sturtevant1913} published the first genetic map describing the order of genes on the X chromosome of \textit{Drosophila Melanogaster}. Together with Herman Muller and Calvin Bridges, two other students of Morgan's, they published the book on \textit{The Mechanism of Mendelian Heredity} \parencite*{Morgan1915}, describing additional genetic maps for chromosome 2 and 3 and list groups of genes that are jointly inherited. 
%The inheritance pattern of the genes was determined by genetic linkage, where they mapped mutant genes to the phenotypes they induced, such as \textit{vestigial} (vestigial wings), \textit{eyeless} and \textit{white} (white eyes as compared to the standard red eyes). 

\section{Towards quantitative genetics}
With their development of genetic mapping and cross-breeding of \textit{D. melanogaster} lines, Morgan, Sturtevant, Muller and Bridges conducted the first genotype-phenotype analysis studies. As in Mendel's original experiments and later, similar work by Bateson, Saunders and Punett, the phenotypes they observed were predominantly categorical, such as color of seeds and flowers in pea plants or the white-eyed phenotype in \textit{Drosophila Melanogaster}. In contrast, biometricians like Galton and Pearson analysed quantitative traits such as height. Their models fit with the Darwinian model of gradual change through natural selection, but did not explain the mode of inheritance.  A great advance in genotype-phenotype mapping allowing for the analysis of quantitative traits came about with the work by the British statistician and biologist Ronald Aylmer Fisher. 

An undergraduate student at the University in Cambridge, \citet{Fisher1912} published his first paper on a criterion for fitting frequency curves where he outlined the fundamental ideas of maximum likelihood estimation. He later extended on this work and by 1922, he had established the properties of the maximum likelihood estimator such as consistency and minimum variability \citep{Fisher1922a} that is still used today \citep{Hald1999}. He demonstrated the utility of maximum likelihood estimation in genetics by solving a number of equations to elucidate a genetic map of eight \textit{Drosophila Melanogaster} genes based on their crossing over frequencies \citep{Fisher1922b}. In the same year and years to follow, he published a series of papers where he derived the distribution and significance testing of regression coefficients, correlation ratios and multiple regression coefficients \citep{Fisher1922c,Fisher1928}, an exact test for two-by-two contingency tables with small expectations (Fisher's exact test) \citep{Fisher1922d}, partial correlation coefficients \citep{Fisher1924a} and the variance ratio, later named after Fisher as the F statitistic \citep{Fisher1924b}. 

In 1918, the cornerstone for quantitative genetics was laid with his publication \textit{The correlation between relatives on the supposition of Mendelian inheritance} where he showed that biometrics and Mendelianism are not contradictory but complimentary \citep{Fisher1918}. Specifically, by analysing levels of phenotypic correlation between individuals of differing degrees of relatedness, he showed that the observed phenotypic variation can result from Mendelian inheritance. He further distinguishes between two different types of genetic components contributing to the phenotype, one simply ascribed to genotypes and the other to \textquote{essential genotypes}. Today, these components are known as broad-sense and narrow-sense heritability. Broad-sense heritability is the proportion of phenotypic variance explained by the entire genetic variation including additive, dominance (allelic interaction within loci) and epistatic (allelic interaction between loci) genetic effects, while narrow-sense heritability is defined as the ratio of additive genetic variance to total phenotypic variance. 

As an additional statistical concept, it was in this work that Fisher defined the term variance as \textquote{the square root of the mean squared error}. The analysis of variance in biological experiments would be of interest to Fisher in his appointment at Rothamsted Experimental Station where he analysed data from crop experiments with respect to different variance components and developed statistical techniques such as the analysis of variance (ANOVA) \citep{Fisher1921,Fisher1923,Eden1929}. 

Extending on his 1918 work on trait correlation in light of Mendel's Laws, Fisher published the book \textit{The Genetical Theory of Natural Selection} where he reconciled the long-standing ideas of Darwins evolutionary theory and Mendelian inheritance. He gives the first, comprehensive quantitative theory of sexual selection, evolution of recombination rates, polymorphism and many more concepts found in today's field of population genetics \citep{Fisher1930}. 

\section{Progress in deciphering the molecular mechanisms of inheritance}
Large steps forward in the molecular understanding of inheritance were the discovery of DNA as the genetic material in 1944 \citep{Avery1944} and its composition from the four bases adenine, thymine, cytosine and guanine \citep{Vischer1948,Chargaff1949,Chargaff1952} as well as the resolution of the DNA structure almost a decade later \citep{Watson1953}. These insights brought forward an understanding of other biological concepts such as protein synthesis and enabled Francis Crick to postulate the central dogma of biology:  information is transmitted from DNA and RNA to proteins, but information cannot be transmitted from a protein to DNA \citep{Crick1958}. The deciphering of the genetic code through Nirenberg and others followed a few years later \citep{Nirenberg1961,Crick1961,Matthaei1962}.  

\subsection{Novel genotype mapping techniques}
\label{subsection:mapping-techniques}
Three discoveries and novel techniques at the beginning of the 1970s opened the door for the development of new genetic mapping approaches: the discovery of restriction enzymes \citep{Smith1970,Morrow1972}, the ability to clone and amplify specific DNA sequences \citep{Jackson1972,Cohen1973}, and the detection of specific DNA sequences from a large pool of DNA fragments (Southern plot) \citep{Southern1975}. Based on these techniques, \gls{rflp} analysis was developed, which allows for the identification of variants from within a specific genomic region using restriction enzyme-digested DNA \citep{Grodzicker1974,Botstein1980}. Initially, \gls{rflp} anaylsis was used for genetic linkage maps in model organisms \citep{Goodman1977,Cameron1979} and target genes in human  \citep{Kan1978,Jeffreys1979,Tuan1979}.  Based on theoretical considerations of using \gls{rflp} analysis for a general, target-free genetic mapping in humans \citep{Botstein1980}, the first human genetic map was published in 1987 \citep{Donis-Keller1987}. 

\subsection{Deciphering DNA sequences}
While these mapping efforts were underway, the independent development of two different DNA sequencing techniques by two groups, one Frederik Sanger and the other Walter Gilbert together with Allan Maxwell, were a further big leap in understanding the biological basis of genetic variation \citep{Sanger1977,Maxam1977}. Sanger's method of DNA sequencing with chain-terminating inhibitors eventually became the standard for DNA sequencing and subsequent innovations lead to the development of automatic sequencing machines which allowed for sequencing lengths of about one kilobase \citep{Hunkapiller1991}. For sequencing longer stretches of DNA, a novel strategy named shotgun sequencing was developed \citep{Staden1979,Anderson1981}. In shotgun sequencing, the long DNA of interest is randomly broken up into shorter DNA fragments which are cloned and sequenced separately. The occurrence of overlapping DNA fragments given by the random nature of creating the short fragments allows for the \textit{in silico} reconstruction of longer DNA fragments.

In 1995, the first genome of a living organism --the bacteria \textit{Haemophilus influenzae}-- was sequenced and assembled by shotgun sequencing \citep{Fleischmann1995}. The genomes of other model organisms were to follow in subsequent years (yeast \citep{Goffeau1996}, \textit{C. elegans} \citep{C.elegans1998}, \textit{D. melanogaster} \citep{Adams2000}) until the first draft of the human genome was published in 2001\citep{Lander2001}. 

The sequence of the human genome, the development of faster, massively-parallel next-generation sequencing techniques (reviewed in \citep{Shendure2008,Heather2016}) and DNA microarrays that allow for the genotyping of hundreds of thousands of genetic markers simultaneously \citep{Wang1998}, started a new era of human genetic and genomic research. 

\section{From genetic linkage analysis to genome-wide association studies}
\subsection{Genotype-phenotype mapping until the 1990s}
Genotype-phenotype mapping approaches today can broadly be classified into genetic linkage analyses and population-based association studies. Genetic linkage analysis for human traits had already been applied in the 1930s \citep{Bernstein1930,Penrose1935}, while association studies only became known in the 1950s. For a clearer description of the methods and results, the following sections describe the developments in human quantitative genetics based on study type rather than in their chronological order. 

\subsubsection{Genetic linkage analysis}
Genetic linkage analysis investigates the relationship between a given locus and the trait or disease of interest.  As with Morgan's linkage studies in \textit{D. melanogaster}, today's methods are also based on the observation that genetic markers in close physical proximity on a chromosome remain mainly linked during meiosis. By following the segregation of a specific trait in family pedigrees, the recombination rates between genetic markers can be estimated and their relative genomic position determined. To quantify the likelihood of linkage, a variety of measures with different pedigree requirements have been developed. Some required full parent-offspring trios \citep{Bernstein1930,Haldane1934}, while others showed the possibility of determining genetic linkage based on sib-pairs alone \citep{Penrose1935}. A commonly used test allowing for different pedigree structures is the sequential probabilty ratio test for linkage \citep{Morton1955,Pulst1999}. In this test, the logarithm of the odds that the loci are linked is divided by the logarithm of the odds that the loci are unlinked. This \gls{lod} score serves as the measure for the likelihood of linkage. Genetic linkage studies often require strict assumptions about the underlying genetic models such as specification of penetrance and disease gene frequency \citep{Morton1955,Pulst1999} and have a number of potential confounding variables such as genetic heterogeneity and accurate diagnosis \citep{Bird1993}. Nevertheless, linkage studies have been successful in pinpointing genomic loci associated with disease.  Initially restricted to known genes or gene products such as haemoglobin (linked to sickle-cell thalassaemia \citep{Ingram1959}) or haemophilia and color-blindness \citep{Haldane1947}, the development of techniques such as \gls{rflp} mapping (\cref{subsection:mapping-techniques}) enabled to detect genetic markers in candidate genes. With these markers, linkage analysis could be extended to a greater number of candidate genes and  lead to the discovery of genetic links to diseases such as Huntington's disease \citep{Gusella1983}, cystic fibrosis \citep{Kerem1989} and bipolar disorder \citep{Baron1987}. 

\subsubsection{Association studies}
In contrast to linkage studies with the association between locus and trait in pedigrees, association studies investigate the relationship of a genetic marker frequency and the trait in a population. The frequencies of the genetic markers in individuals carrying the trait (cases) are compared to those in individuals without the trait (controls). Genetic markers whose frequencies are increased in cases compared to controls are thought to be associated with the risk for diseases. Often, the significance of the association is evaluated via a simple \(\chi ^2\)-test. As with linkage analysis, population association studies were initially limited to known genes or gene products such as in the association for blood antigens and stomach cancer \citep{Aird1953,Aird1954}. With the new techniques for determining genetic markers in candidate genes, association studies successfully identified gene-disease associations in for instance diastrophic dysplasia \citep{Hastbacka1992} and Alzheimers' Disease \citep{Strittmatter1996}\footnote{\citet{Spielman1993} reconciled linkage analysis and case-control association studies, by formally introducing the transmission/disequilibrium test which tests directly for linkage between a disease and marker locus which is known to show population association.}. \citet{Jiang1995} provided an extension to the population-association model, leaving the strict case-control design and proposing a method to detect association with multiple quantitative traits. In the quantitative association study, an individuals genotype is represented numerically and a model can be fit directly to the genotypes and the continuous trait without relying on case-control status. In the linear model framework introduced by Jiang and colleagues, multiple traits are jointly analysed for genetic association, testing different models such as pleiotropic effects i.e., and gene-environment interaction \citep{Jiang1995}. 



\subsection{\textquote{Common disease–common variant} hypothesis}
By the mid-1990s, genotype-phenotype mapping in humans was largely focused on candidate gene mapping in either linkage analysis or association studies. Linkage analysis had been very successful in identifying genes linked to Mendelian and monogenetic disorders with \num{671} genes for which at least one disease-related locus\footnote{Statistics extracted from Online Mendelian Inheritance in Man: \url{https://omim.org}; search parameters: \textquote{date\_updated:1981/1-1995/12}} was detected by 1995. Population-based association studies had so far detected about 250 genes associated with disease or dichotomous traits \citep{Hirschhorn2002}. However, the number of reproducible results was significantly lower and showed the difficulties associated with case-control population association studies. Major limitations were seen in the susceptibility to population stratification \citep{Lohmueller2003} and the low \textit{a priori} probability of the tested gene to be causal. In addition, for illnesses such as heart disease, diabetes or hypertension, the risk of being affected is likely a combination of multiple genetic and environmental factors \citep{Hunter2005}, which stands in stark contrast to the pattern observed in monogenetic diseases. In monogenetic diseases, the presence of a genetic factor or factors (dominant or recessive) almost completely predicts the presence of diseases such as cystic fibrosis or Huntington's Disease and these factors are generally of low frequency \citep{Sankaranarayanan1998}.  In the complex diseases, the genetic risk factor may be present in higher frequency and only lead to a small increase in disease risk \citep{Reich2001}. Based on these arguments, the \textquote{common disease–common variant} hypothesis had been proposed, stating that common polymorphisms may play a role in the susceptibility to common diseases \citep{Risch1996,Lander1996,Chakravarti1999,Reich2001}. For detecting common variants with small or moderate effect sizes, association studies are a more powerful tool than linkage analyses \citep{Ott2015} and became the method of choice to investigate common disease variants on a genome-wide level. To enable systematic genome-wide screens of common variants, three components were needed: a catalogue of common variation in the human population, experimental techniques to obtain these genotypes in large cohorts, and the computational techniques for the subsequent analyses. 

\subsection{Databases of human variation}
\label{subsection:databases}
%(other suchSHAPEIT \citep{Delaneau2012,Delaneau2013} techniques reviewed in \citep{Browning2011})
The first genome-wide database of common human sequence variation was created within the scope of the International HapMap project which was launched in 2002 \citep{HapMap2005,HapMap2007,HapMap2010}.
The HapMap project aimed at characterising the frequencies of \glspl{snp}, i.e. variation on a single base pair level, for different human populations. Based on their genome-wide \glspl{snp} frequencies, a comprehensive map for \gls{ld}  --the non-random association of alleles at different loci \citep{Lewontin1960}-- in different populations was created. By having included parent–offspring trios in the analysis, computational phasing \citep{Stephens2001} enabled determination of the \gls{snp} contribution from each parent and the combination in which they were inherited. This particular combination of \glspl{snp}along a chromosome is termed haplotype and was the inspiration for the name of the project. The HapMap collection contains 1.6 million common single nucleotide polymorphisms in \num{1184} reference individuals from 11 global populations. An extension of the work of the HapMap project, the 1000 Genome Project aimed to detect common human genetic variation by whole-genome sequencing of individuals from multiple populations. The project finished in 2015, providing genotypes and haplotypes at more than 88 million variants, including \glspl{snp}, \glspl{indel}, and structural variants for \num{2504} individuals from 26 populations \citep{1000Genomes2011,1000Genomes2012,1000Genomes2015}. The work of the UK10K consortium complemented the work of both previous projects and extends the spectrum of observed genetic variation to rare variants in nearly \num{10000} individuals from population-based and disease collections \citep{UK10KConsortium2015}. While the major focus of these consortia laid in the collection of comprehensive genotype data, a new resource combining both genotype and phenotype data of more than \num{500000} individuals has recently been published. Phenotypes collected within this resource, the UK Biobank, cover amongst others anthropometric, cardiac and disease phenotypes \citep{Sudlow2015}.

\subsection{Genotyping of large cohorts}
Genotype data of common variants is standardly obtained from DNA microarrays which allow for the genotyping of hundreds of thousands of common \glspl{snp} simultaneously \citep{Wang1998}. Based on the \gls{ld} structures found in the reference panels (described above), haplotypes of the individuals can be estimated. Comparing the estimated haplotypes of the individuals to haplotype patterns in the reference panel enables imputation of unobserved genotypes in the study cohort. A number of different methods for genotype imputation have been developed including IMPUTE2 \citep{Howie2009}, Beagle \citep{Browning2007} and MaCH \citep{Li2010} (reviewed in \citep{Marchini2010}). Via imputation, the number of genotypes per individual can be extended from the hundred thousands on the genotyping array to millions of observed variants in the reference datasets. Using these imputed genotypes for association studies can increase the power of the study and presents a high-resolution view of all \glspl{snp} in the associated region \citep{Marchini2010}.

\subsection{Genome-wide association studies}
\label{subsection:GWAS}
The first successful study to test the \textquote{common disease–common variant} hypothesis without gene-based selection of genetic markers was conducted in 2005. \citet{Klein2005} carried out a case-control \gls{gwas} for age-related macular degeneration and found a \gls{snp} in complement factor H to be associated with an increase in disease risk. Similar to population association studies of candidate genes, the significance of each \gls{snp}-disease association was tested via a \(\chi ^2\)-test and the resulting p-values subsequently corrected for multiple testing via Bonferroni correction (see \cref{subsection:multiple-testing}). Soon after, the \gls{wtccc} published large case-control \gls{gwas} for seven common diseases, including bipolar disorder, coronary heart disease and type I and II diabetes \citep{Burton2007}. In the same year, the first \gls{gwas} on quantitative traits followed. Two research groups investigated the genetic effects on body mass index and found links to the FTO gene. In addition, these BMI-associated \glspl{snp} also showed strong association to type II diabetes \citep{Frayling2007} and other \glspl{snp} within the FTO gene were also associated to weight and hip-circumference \citep{Scuteri2007}. Both studies used a simple linear model  (see \cref{section:LinearModels}) to find the association of the genetic marker as the explanatory variable and BMI as response variable. 

In the following years, the methods for \gls{gwas} were extended to enable the genotype-phenotype mapping for sets of \glspl{snp}  \citep{Wu2010,Casale2015}, the joint mapping of multiple traits \citep{Korte2012,Yang2011,Bottolo2013,Casale2015} and the use of more complex models to account for population stratification such as mixed model approaches \citep{Kang2010,Lippert2011,Zhang2010,Svishcheva2012} and general estimating equations \citep{Cupples2007}.

Based on these methods, thousands of \gls{gwas} have been conducted covering common diseases (e.g. asthma \citep{Noguchi2011,Pickrell2016}, coronary heart disease \citep{Wild2011,Takeuchi2012,Lu2012}, migraine \citep{Pickrell2016,Gormley2016}, blood pressure \citep{Kato2011,Franceschini2013}), anthropometric traits (e.g.height \citep{Lango2010,Wood2014}, weight \citep{Willer2009}, BMI \citep{Speliotes2010,Yang2012a}, waist-hip ratio \citep{Lindgren2009,Heid2010}) and other non-disease related quantitative phenotypes (e.g. eye color \citep{Eriksson2010,Candille2012,Zhang2013}, freckling \citep{Sulem2008}, facial morphology \citep{Paternoster2012}, hair greying \citep{Adhikari2016}). The results of these studies are collected in the \gls{gwas} catalogue, which currently contains \num{3092} publications and \num{49769} unique \gls{snp}-trait associations \citep[accessed 10.09.2017]{MacArthur2017}. 

%`[a] knowledge of sequences could contribute much to our understanding of living matter' Sanger \citep{Nobel1981}. 

\section{Linear models for genome-wide association studies}
\label{section:LinearModels}
Simple linear models and linear mixed models are widely applied in genetic association analysis. They offer great control for confounding factors and allow for the joint analysis of multiple traits. In the following sections, I will describe the general model specifications and parameters, their estimation and application to genetic studies. I will outline the challenges for linear models in \gls{gwas} and the approaches developed to overcome these challenges. 

For mathematical model descriptions throughout this thesis, I used the following notation: bold, small letters symbolise one-dimensional column vectors e.g \tmat{v} and bold capitalised letters matrices e.g \tmat{M}. A normal distribution is specified by \(\normal {\text{mean}}{\text{variance}}\), a multivariate normal by \(\multinormal {\text{r}} {\text{c}}{\text{mean}}{\text{variance}}\) and a matrix-variate normal by  \(\matrixnormal {\text{r}} {\text{c}}{\text{mean}}{\text{variance}_\text{rows}}{\text{variance}_{\text{columns}}}\), where r and c are the row and column dimensions respectively.

\subsection{Linear regression}
In the linear model, the continuous response variable (e.g. phenotype) is described as a linear function of one or more explanatory variables (e.g. genotype and covariates). With \(N\) representing the number of samples, \(y_i\) the response variable for sample \(i\), \(\left\{x_{i1}, x_{i2}, \dots, x_{iF}\right\}\)  the \(F\) explanatory variables for sample \(i\) and \(\beta_{f}\) their corresponding weights, the linear model can be cast as
\begin{equation}
y_i = \sum^{F}_{f=1}x_{if}\beta_{f} + \psi_i, \,\,\,\, \text{with }  \psi_i \sim \normal 0 {\sigma^2_e}.
\label{eq:linear-regression}
\end{equation}
%
In this model, the residual term \(\psi_i\) captures measurement noise and other unaccounted factors that influence the response variable. \(\psi_i\) is modelled to follow a normal distribution with mean 0 and variance \(\sigma^2_e\) and to be independent across 
samples, i.e. with covariance equals to zero: \(\text{cov}\left(\psi_i,\psi_j\right)=0\).

Alternatively, \cref{eq:linear-regression} can be written in matrix form
\begin{equation}
\mat{y} = \mat{X}\mat{\beta} + \mat{\psi}, \,\,\,\, \text{with }  \mat{\psi} \sim  \normal {\mat{0}}{\sigma^2_e\matsub{I}{N}},
\label{eq:linear-regression-matrix}
\end{equation}
%
where the \(N \times N\) identity matrix \tmatsub{I}{N}, the response vector \tmat{y}, the matrix of explanatory variables \tmat{X}, the weight vector \tmat{\beta} and the vector of residuals \tmat{\psi} are defined as:

\begin{equation}
 \mat{y} =
  \begin{bmatrix} 
 	y_{1} \\
 	y_{2} \\
 	\vdots \\
 	y_{N} 
 \end{bmatrix}, \,
%
 \mat{X} =
  \begin{bmatrix}
   x_{11} & x_{12} & \cdots & x_{1F} \\
   x_{21} & x_{22} & \cdots & x_{2F} \\
   \vdots & \vdots & \ddots & \vdots \\
   x_{N1} & x_{N2} & \cdots & x_{NF} 
   \end{bmatrix}, \,
%
 \mat{\beta} =
  \begin{bmatrix} 
 	\beta_{1} \\
 	\beta_{2} \\
 	\vdots \\
 	\beta_{N} 
 \end{bmatrix}\,\text{and} \, 
%
\mat{\psi} =
  \begin{bmatrix} 
 	\psi_{1} \\
 	\psi_{2} \\
 	\vdots \\
 	\psi_{N} 
 \end{bmatrix}.
\end{equation}



\subsubsection{Maximum likelihood estimation}
The model in \cref{eq:linear-regression-matrix} describes the probability distribution of the response variable, given the explanatory variables and corresponding parameter estimates \tmat{\beta} and \(\sigma^2_e\). This probability is also known as the likelihood function or likelihood \(\mathcal{L}\) and plays a key role in statistical inference of the model parameters. Casting \cref{eq:linear-regression-matrix} as the likelihood of the model parameters  \tmat{\beta} and \(\sigma^2_e\) yields 
\begin{equation}
\mathcal{L} \left( \mat{\beta}, \sigma^2_e \right) = p \left(\mat{y} \mid \mat{X}, \mat{\beta}, \sigma^2_e \right) = \normal {\mat{y} \mid \mat{X}\mat{\beta}} {\sigma^2_e\matsub{I}{N}}
\end{equation}
%
or directly expressed in terms of the response variable
\begin{equation}
\label{eq:normal-matrix}
\mat{y} \sim \normal {\mat{X}\mat{\beta}} {\sigma^2_e\matsub{I}{N}}.
\end{equation}
%
The parameter estimates \(\hat{\mat{\beta}}\) and \(\hat{\sigma}^2_e\) that maximise the likelihood function are the \glspl{mle} of \tmat{\beta} and \(\sigma^2_e\). In order to improve numerical stability, the log likelihood\footnote{Since the logarithm is monotonically increasing, maximisation of the log-likelihood is equivalent to maximising the likelihood itself, but offers mathematically convenient properties.} is commonly used instead of the likelihood. The full log-likelihood is expressed as
%
\begin{align}
\log\mathcal{L} \left( \mat{\beta}, \sigma^2_e \right) & = \log p \left(\mat{y} \mid \mat{X}, \mat{\beta}, \sigma^2_e \right) \\
& = \log \prod^N_{i=1} p \left(y_i \mid \mat{X}, \mat{\beta}, \sigma^2_e \right) \\
& =  -\frac{N}{2}\log\left(2\pi\right) - \frac{N}{2}\log\sigma^2_e  \frac{1}{2\sigma^2_e } \left(\mat{y} - \mat{X}\mat{\beta}\right)^T  \left(\mat{y} - \mat{X}\mat{\beta}\right).
\label{eq:log-likelihood}
\end{align}
%
and the \gls{mle}
%
\begin{equation}
\hat{\mat{\beta}},\hat{\sigma}^2_e = \text{argmax}_{\mat{\beta},\sigma^2_e} \log\mathcal{L}\left(\mat{\beta},\sigma^2_e\right).
\label{eq:argmax-log}
\end{equation}
%
The \gls{mle} of \tmat{\beta} and \(\sigma^2_e\) are found by finding the maxima of the partial derivates of \cref{eq:log-likelihood} 
%
\begin{align}
\left(\frac{\partial\log\mathcal{L}\left(\mat{\beta},\sigma^2_e\right)}{\partial \mat{\beta}}\right)_{\mat{\beta}=\hat{\mat{\beta}},\sigma^2_e=\hat{\sigma}^2_e} &= 0 \\
\left(\frac{\partial\log\mathcal{L}\left(\mat{\beta},\sigma^2_e\right)}{\partial \sigma^2_e}\right)_{\mat{\beta}=\hat{\mat{\beta}},\sigma^2_e=\hat{\sigma}^2_e} &= 0, 
\end{align}
%
yielding
%
\begin{align}
\hat{\mat{\beta}} & = \left(\mat{X}^T\mat{X}\right)^{-1}\mat{X}^T\mat{y} \\
\label{eq:beta-MLE}
\hat{\sigma}^2_e &= \frac{1}{N}\left(\mat{y} - \mat{X}\hat{\mat{\beta}}\right)^T\left(\mat{y} - \mat{X}\hat{\mat{\beta}}\right) \\
&= \frac{1}{N}\left(\mat{y} -\mat{X}\left(\mat{X}^T\mat{X}\right)^{-1}\mat{X}^T\mat{y}\right)^T\left(\mat{y} -\mat{X}\left(\mat{X}^T\mat{X}\right)^{-1}\mat{X}^T\mat{y}\right).
\label{eq:sigma-MLE}
\end{align}

\subsubsection{Restricted maximum likelihood}
\label{subsubsection:REML}
In Gaussian models as in \cref{eq:normal-matrix}, the \gls{mle} of the mean estimate \(\hat{\mat{\beta}}\) is unbiased whereas the \gls{mle} of the variance component \(\hat{\sigma}^2_e \) suffers from a downward bias. The bias of \(\hat{\sigma}^2_e \) originates from the loss in the degrees of freedom as a consequence of estimating \(\hat{\mat{\beta}}\) from the data. \Citet{Patterson1971} proposed a solution for a \(\mat{\beta}\)-free estimation of \(\hat{\sigma}^2_e \) via \gls{reml}. In short, for a linear regression model with 
\begin{equation}
\mat{y} = \mat{X}\mat{\beta} + \mat{\phi}, \,\,\,\, \text{with }  \mat{\phi} \sim  \normal {\mat{0}}{H\left(\theta\right)},
\label{eq:linear-regression-general}
\end{equation}
%
where the covariance term is now described as a general covariance matrix \(H\left(\theta\right)\) parameterised by \(\theta\), the \gls{reml} is based on the projection \tmat{w} of \tmat{y} by a matrix \tmat{A} with:
\begin{equation}
\label{eq:orthoganol-projection}
\mat{A}\mat{X}=0.
\end{equation}
%
Using \cref{eq:orthoganol-projection} and rewriting \cref{eq:linear-regression-general} in terms of the projection \tmat{w}
\begin{equation}
\mat{w} = \mat{A}\mat{y} = \mat{A}\left(\mat{X}\mat{\beta} + \mat{\phi}\right) = \mat{A}\mat{\phi} 
\end{equation}
%
yields an expression of \tmat{y} that is free of \tmat{\beta}. By directly estimating \(\mathcal{L} \left(\theta \mid \mat{A}\mat{y}\right)\), the unbiased estimate for \(\theta\) can be found. In case of the linear regression in \cref{eq:normal-matrix} with \(H\left(\theta\right) = \sigma^2_e \matsub{I}{N}\), the \gls{reml} estimate of variance component \(\sigma^2_e\) is
\begin{align}
\hat{\sigma}^2_e &= \frac{1}{N-F}\left(\mat{y} -\mat{X}\left(\mat{X}^T\mat{X}\right)^{-1}\mat{X}^T\mat{y}\right)^T\left(\mat{y} -\mat{X}\left(\mat{X}^T\mat{X}\right)^{-1}\mat{X}^T\mat{y}\right).
\label{eq:sigma-REML}
\end{align}
%
Comparing \cref{eq:sigma-MLE} and \cref{eq:sigma-REML}, it becomes evident that the \gls{mle} and \gls{reml} for the variance component only differ in the denominator where \(N\) is replaced by \(N-F\), reflecting the loss in \(F\) degrees of freedom (number of explanatory variables in the model). 

In more complex linear models such as linear mixed models (\cref{subsection:lmm}), the estimation of the variance component is equally more complex depending on the covariance structure of the residual effects. The detailed derivation of the \gls{reml} estimators of parameters from the linear model framework used  throughout this thesis can be found in \citep[Supplementary material]{Casale2015}. 

\subsection{Simple linear model for genotype associations with a single trait}
\label{subsection:lm-uv}
In genetic association studies, the simple linear model describes the phenotype of interest as the sum of the genetic effect and often additional covariate effects such as height or sex:
\begin{equation}
\mat{y} = \mat{x}\beta + \mat{F}\mat{\alpha} + \mat{\psi}, \,\text{ with }
\mat{\psi} \sim \normal 0 {\sigma_e^2\matsub{I}{N}}
\label{eq:lm-uv}
\end{equation}
%
and
%
\begin{align*} 
& \text{the phenotype vector for $N$ samples } \mat{y} \inR N 1,\\
& \text{the genetic profile of the SNP being tested }\mat{x} \inR N 1,\\
& \text{the effect size of the SNP } \beta \inR 1 1 \\
& \text{the matrix of $K$ covariates }\mat{F} \inR N K \text{ and}\\
& \text{the effect of covariates } \mat{\alpha} \inR K 1.
\end{align*} 
%
The residual noise \(\mat{\psi}\) is assumed to follow a normal distribution that is independent across the \(N\) samples.

In order to model the genotypes quantitatively, they have to be encoded numerically. For genetic association studies in diploid organisms, there are different inheritance models based on the combination of parental alleles \(a\) and \(b\) (for bi-allelic loci). In a recessive inheritance model (with respect to \(b\)), the phenotype is only observed in the presence of two \(b\) alleles and the genotypes are encoded as \(aa=0\), \(ab=0\) and \(bb=1\). In the dominant model for \(b\), where only one copy of the allele is necessary to confer the phenotype, the genotypes are \(aa=0\), \(ab=1\) and \(bb=1\). The additive, or allelic dosage, model for \(b\) assumes that the effect on the phenotype is proportional to the allele count of \(b\) with \(aa=0\), \(ab=1\) and \(bb=2\) \citep{Bush2012}. For association testing without prior knowledge or assumptions about the mode of inheritance, the additive model has been widely adapted and will be used throughout this thesis. It shows reasonable performance across all three models for the majority of effects, however, may suffer from a loss in power for recessive traits with a low causal allele frequency \citep{Lettre2007}. 

As \cref{eq:normal-matrix} shows, the phenotype is assumed to follow a normal distribution. In order to avoid model misspecification when testing for genetic association, it is common practice to ensure approximate normality by transforming the observed phenotypes via methods such as Cox-Box \citep{Etzel2003,Yang2006} or inverse normal transformation \citep{Scuteri2007,Guan2008,Anttila2010,Casale2015}. For any association tests conducted throughout this thesis, a rank-based inverse normal transformation is applied to each phenotype.

\subsection{Testing the genotype association for significance}
\label{subsection:hypothesis-testing}
The significance of the association between phenotypes and the genetic markers can be assessed by testing that the genetic variant has an effect (\(\beta \neq 0\)) versus the null hypothesis \tnullH of not having an effect (\(\beta = 0\)) on the phenotype. The\gls{llr} test statistic \(\Lambda\) is a commonly used statistic to compare the likelihood of the full model \taltH to the one of the Null model \tnullH:
%
\begin{align}
\altH&: \mat{y} \sim \normal {\mat{x}\beta + \mat{F}\mat{\alpha}} {\sigma_e^2\matsub{I}{N}},\label{eq:lm_alt} \\
\nullH&: \mat{y} \sim \normal {\mat{F}\mat{\alpha}} {\sigma_e^2\matsub{I}{N}}. \label{eq:lm_null}
\end{align}
%
The \gls{llr}test statistic \(\Lambda\) is defined as
\begin{equation}
\Lambda  =  \mathcal{L} (\hat{\beta}, \hat{\mat{\alpha}}, \hat{\sigma_{e}}) -  \mathcal{L} (0, \bar{\mat{\alpha}}, \bar{\sigma_{e}})
\label{eq:llr}
\end{equation}
%
where \(\mathcal{L} (\hat{\beta}, \hat{\alpha}, \hat{\sigma_{e}})\) are the \gls{reml} of \taltH and \(\mathcal{L} (0, \bar{\alpha}, \bar{\sigma_{e}})\) the \gls{reml} of \tnullH. \(2\Lambda\) follows a \(\chi^2_{d}\)-distribution with \(d\) degrees of freedom equal to the number of tested parameters \citep{Wilks1938} and allows for the calculation of the p-value as :
\begin{equation}
P(\Lambda) = \int_{2\Lambda}^{\infty} \chi^2 \left(x;d\right)dx = 1- F_{\chi^2}\left(2\Lambda; d\right),
\label{eq:pvalue}
\end{equation}
where \(F_{\chi^2}\left(2\Lambda; d\right)\) is the cumulative density function of the \(\chi^2\)-distribution with \(d\) degrees of freedom. For a single-variant single-phenotype test, the degrees of freedom are \(d=1\) (\cref{fig:GWAS-stats}\subfig{A}, blue). The p-values derived from the \(\chi^2\)-distribution can be used to interpret the association. It is defined as the probability of finding the observed, or more extreme, results when \tnullH is true. The p-values are compared to a predefined significance level \(\alpha\), which specifies the probability of rejecting a true null hypothesis. If \(p < \alpha\), the null hypotheses is rejected and the association considered statistically significant. Falsely rejected null hypothesis are classified as Type I errors, or false positives, and depend on the stringency of the \(\alpha\) threshold. For instance, with  \(\alpha=0.05\), 5\% of all rejected null hypotheses might be true. Type II errors, or false negatives, occur when the null hypothesis is falsely accepted, i.e. a true association is not detected. The power in a \gls{gwas} is the proportion of true positives associations that can be detected, which corresponds to \(\text{power} = 1 - \text{Type II error rate}\)  \citep{Krzywinski2013a,Krzywinski2013b}.

In a \gls{gwas}, \(S\) genome-wide \glspl{snp} are assessed under \tnullH (\cref{eq:lm_null}). With the assumption that the wide majority of \tnullH are true and potential confounding has been properly adjusted for (\cref{subsection:population-structure}), the genome-wide p-values follow a uniform distribution in \(\left(0,1\right]\) (\cref{fig:GWAS-stats}\subfig{B}). To visually examine the p-value distribution, p-values are often depicted in quantile-quantile (qq) plots where the expected \(-\log_{10} \text{p-values}\)\footnote{In practice, the expected \(-\log_{10} \text{p-values}\) are obtained through \(S\) equally spaced numbers in \(\left(0,1\right]\)} are plotted against the observed \(-\log_{10} \text{p-values}\) (both sorted in increasing order). A \gls{gwas} is well-calibrated if the expected and observed p-value distribution only show deviations for \glspl{snp} significantly associated with the phenotype (\cref{fig:GWAS-stats}\subfig{C}). Deviations of the observed from the expected p-value distribution are commonly observed in GWAS with confounding factors such as population structure and relatedness which can create spurious associations or highly polygenic traits \citep{Spielman1993,Lander1994,Marchini2004,Balding2006}. Strategies to adjust for these confounding effects and to tell them apart from the true polygenetic effects are described in (\cref{subsection:population-structure}). 

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=\textwidth]{Introduction/Figures/GWASstats.pdf}
	\caption[\textbf{Distributions in LLR testing in GWAS.}]{\textbf{Distributions in LLR testing in GWAS. }A. Cumulative density functions of \(\chi^2\)-distributions with different numbers of degrees of freedom (d). The higher the number of degrees of freedom, the higher the \(\chi^2\)-statistic (x-axis) has to be to obtain statistically significant p-values (indicated as dotted lines and shaded regions under the curves for \(\alpha=0.05\)). B. P-value distribution of a well-calibrated \gls{gwas}. P-values are derived from the associations of \num{50000} bi-allelic \glspl{snp} from \num{1000} individuals with a single quantitative phenotype. Out of the \num{50000} \glspl{snp}, five \glspl{snp} were simulated to have an effect \(\beta \neq 0\). The phenotype was simulated with default parameters as described in \cref{chapter:simulation}.  C. Quantile-quantile plot of the p-values from the associations in B. The five \glspl{snp} with \(\beta \neq 0\) are indicated in green.}
	 	\label{fig:GWAS-stats}
\end{figure}

\subsection{Correcting for multiple hypotheses testing in GWAS}
\label{subsection:multiple-testing}
The underlying assumption of a \gls{gwas} is that the large majority of \glspl{snp} will have no impact on the phenotypes, i.e. for each \gls{snp}, one tests the null hypothesis of no effect versus the alternative hypothesis of a \gls{snp} effect that is different from zero and expects to accept the vast majority of these null hypotheses. However, when testing a large number of null hypotheses, it is likely to observe some significant results even if all null hypotheses are true. In a well-calibrated test, the number of false positive results depends on the \textit{a priori} specified significance level \(\alpha\). For instance, with  \(\alpha=0.05\) and ten million genome-wide \glspl{snp}, \(5 \times 10^5\) tests would be expected to be false positives. Methods to correct for multiple hypotheses testing, i.e. reduce the number of Type I errors are reviewed in detail in \citep{Shaffer1995}. The most commonly applied methods based on false discovery rate and family-wise error rate are described below.

\paragraph{False discovery rate}
The \gls{fdr} corrects for multiple testing based on the expected proportion of false discoveries. The \gls{fdr} was introduced by Benjamini and Hochberg in \parencite*{Benjamini1995} and a number of other \gls{fdr}-based correction methods were developed thereafter e.g. \citep{Storey2002,Donoho2006,Sarkar2007}. The original method by Benjamini and Hochberg set out to control the expected values of the \gls{fdr} based on the ratio of wrongly rejected \(N\) and total number of rejected null hypotheses \(R\):
\begin{equation}
\text{FDR} = \mathbb{E} \left[\frac{N}{max\left(R,1\right)} \right],
\end{equation}
where the maximum in the denominator protects against division by zero. The procedure works as follows: for a total number of \(m\) tests, with p-values \(p_1, p_2, \dots, p_m\) ordered in increasing order by their ranks  \(k_1, k_2, \dots, k_m\) (smallest p-value \(p_1\) with \(k_1=1\)), the adjusted p-value \(p'_i\) is determined as \(p'_i = \frac{mp_i}{k_i}\). Choosing to accept all null hypotheses with \(p'_i > \alpha\) ensures \(\text{FDR} < \alpha\). 

\paragraph{Family-wise error rate}
The \gls{fwer} controls for the probability of observing at least one false positive result within a given experiment (family of tests) \citep{Shaffer1995}. Among the \gls{fwer}-based tests, the most simple procedure to adjust for multiple testing is multiplying all observed p-values \(P=p_1, p_2, \dots, p_m\) by the total number of tests \(m\): \(P' = mP\). This method to compute the adjusted p-values \(P'\) was proposed by Olive Dunn in 1961, based on properties of Bonferroni's inequalities \citep{Dunn1961} and the method is commonly referred to as Bonferroni correction. Accepting all null hypotheses with \(p_i' > \alpha\) ensures controlling for \(\text{FWER} < \alpha\). The main assumption in Bonferroni-based adjusting for multiple testing is the independence of the conducted tests. In genome-wide tests of association, \gls{ld} structure in the genome induces dependence of tests and correction for multiple testing by a strict multiplication of the total number of tests is too conservative. 

\paragraph{Permutation-based adjusting for FWER}
In order to account for the dependency of the statistical tests in genetic association studies, one can use permutation-based approaches to control the \gls{fwer}. In these approaches, the link between the parameter of interest i.e. the genotype and the observed phenotype is broken by random permutation of the genotype data across individuals.  The association study is conducted \(T\) times on \(T\)  random permutations of the data and the p-values of the permutation experiments \(\bar{P}\) compared to the observed p-values of association study. For each \(p_i\),   \(p'_i\)  is calculated by recording the number of times \(p_i\) is smaller than any \(\bar{p}_i\) and subsequently dividing this number by the total number of permutations. Permutation-based approaches have been employed in whole-genome association studies (about \num{10000} genotypes) for yeast \citep{Brem2002,Ehrenreich2010,Bloom2013} and human genotype to gene expression association studies for adjusting on gene level \citep{1000Genomes2015}. In these studies the computational burden is moderate, whereas permutation studies for human \gls{gwas} with millions of \glspl{snp} might become impractical. 

\paragraph{LD-corrected genome-wide significance threshold}
As an alternative to adjusting each p-value individually, a new \(\alpha '\) can be specified which controls for the same level of type I errors as \(\alpha\) but takes the number of tests that are conducted into account. For the conservative Bonferroni correction, which does not consider the genomic \gls{ld} structure \(\alpha ' = \frac{\alpha}{m}\). 
For human \gls{gwas}, the multiplication factor for \(\alpha \) has been estimated based on the estimated number of independent variants in the genome.  It is is based on an observation of the HapMap project \citep{HapMap2005} where about \num{150} independent, common variants were found per \num{500}~kb region.  Extrapolating this number to the human genome size of \(\sim 3.3\)~Gb, for \(\alpha ' =0.05\) the genome-wide significance threshold was estimated as  \(\alpha '= 0.05 \times 150 \times \left(500\text{kb} \times 3.3 \text{Gb}\right)^{-1}= 5.05 \times 10^{-8}\). This estimate was later confirmed in a study using different methods for estimating the number of independent variants \citep{Fadista2016} and is the commonly employed threshold in today's human \gls{gwas}. However, note this threshold can be different in genetic studies of rare variants (for example \citep{Xu2014}).

\subsection{Accounting for population structure and genetic kinship}
\label{subsection:population-structure}

Confounding of association results based on genotypic differences between cases and controls had been a known challenge before the \gls{gwas} era \citep{Spielman1993} and remains a critical issue still. If population structure is not taken into account when testing for genotype-phenotype associations, statistically significant associations might be observed that simply reflect the underlying population structure and lead to an increase in false positive results. Equally, real effects might be masked and genuine associations missed \citep{Marchini2004}. In the case-control setting, this problem arises easily when the study consists of (undetected) subpopulations which are not evenly distributed among cases and controls. For \glspl{snp} where the allele proportions differ between the hidden subgroups, a false positive significant association will be recorded \citep{Marchini2004,Balding2006}. Quantitative trait association studies can be subject to similar issues. If the study cohort is comprised of individuals from different ethnicities, spurious associations can be detected that reflect ethnicity rather than causal variation. \citet{Campbell2005} demonstrated in an association study for height in a European-ancestry cohort that significant association could simply be attributed to differences in \gls{snp} frequencies across European ancestry subpopulations.  Other studies confirmed allele-frequency differences within cohorts of the same ethnicity \citep{Tian2008a,Tian2008b}, thus emphasising the need for proper control of population structure. Similar issues arise for a more fine-scaled structure in the cohort induced by samples with different degrees of relatedness. When related individuals are present in the cohort, their genotypes do not reflect random and independent draws from the population frequencies. While this generally does not affect the allele frequency estimates, their variance might be greater than expected, leading to an overdispersed test statistic and increased false positive rate, as demonstrated by Bacenu, Devlin and Roeder for case-control settings and quantitative traits \citep{Devlin1999, Bacanu2002}. In addition to population structure and relatedness, spurious associations might arise in studies with recently admixed populations, as described by \citet{Lander1994} and \citet{Ewens1995} where false positive disease associations were found due to allele frequency differences in the parent populations. A number of different methods have been developed to correct for confounding genotype structures.

\subsubsection{Post-hoc adjusting}
The first methods to adjust for genetic background structure was proposed by \citet{Devlin1999}. \textit{Genomic Control} is based on the hypothesis that genetic background structure generates an inflation of the test statistics. Adjustment for population structure is achieved by estimating the inflation factor \(\lambda\) and dividing the test statistic of each association by \(\lambda\). Extensions to their initial approach for case-control studies included partially modified approaches for estimating \(\lambda\) \citep{Reich2001}, its application for quantitative traits \citep{Bacanu2002}, and an adjusted approach to take the number of \glspl{snp} for the estimation of \(\lambda\) into account \citep{Devlin2004}. The observation that inflation and sample size seemed to correlate lead \citet{Yang2011} to systematically study different parameters influencing inflation and they found \(\lambda\) to be a function of sample size, \gls{ld} structure and narrow-sense heritability. Importantly, they showed that \(\lambda\) is also correlated with the number of causal variants, thus studies on traits with polygenetic inheritance can show inflation independent from confounding. Based on this observation, \citet{Bulik-Sullivan2015} developed \gls{ld} Score regression, a regression method for distinguishing confounding structures from polygenicity in \gls{gwas}. As with \textit{Genomic Control}, the estimated inflation factor from \gls{ld} Score regression can be used for the post-hoc adjusting of the test statistic. 
%For dense genotyping data, genetic markers that are in \gls{ld} with the causal marker will also show an elavation in the test statistic and in sum contribute to an inflation of the genome-wide test statistic. Hence, inflation of a markers test statistic dues to polygenicity will be correlated with \gls{ld} structure, while confounding due to population structure and relationship is not correlated with \gls{ld} patterns. }
\subsubsection{Adjusting by subsampling}
Shortly after the introduction of \textit{Genomic Control}, \citet{Pritchard2000} proposed the concept of \textit{Structured Associations}, where genetic markers unlinked to the phenotype are used to identify subpopulations of samples. Assigning the samples to their respective unstructured subpopulations and testing for association within subpopulations will essentially overcome the problem of population structure present in the overall study population. While useful and employed in association studies for a moderate number of genetic markers and samples \citep{Li2004,Stein2009,Kulbrock2013}, it is computationally expensive for large datasets \citep{Price2006}. In addition, human genetic diversity is better approximated by continuous measures or gradients rather than discrete cluster membership \citep{Serre2004,Price2006}.

\subsubsection{Relatedness and population estimates as model variables}
\label{subsubsection:relatedness-model-variables}
In contrast to the post-hoc and subsampling approaches, adjusting for population structure and relatedness within the association model is possible by estimating the genetic relationship of the samples and using these estimates as additional variables. Studies on genotype variation in relation the geographical distance have demonstrated that geographic ancestries of individuals can be inferred from genetic markers \citep{Rosenberg2002,Tang2005}. Sample clustering based on the genetics is thereby largely correlated with their geographic regions \citep{Rosenberg2005}. In addition to capturing large scale population structure, genetic markers have also been employed to estimate shared ancestry and relatedness in natural populations \citep{Lynch1999,Ritland2000,Thomas2005}. \citet{Price2006} proposed to use genome-wide genetic markers to estimate a genetic sample-by-sample covariance matrix. The \glspl{snp}  of this genetic covariance matrix represent continuous axes of genetic variation and can be used to adjust for population structure, either by \textit{a priori} regression of the principal components from both the genotype and phenotype data, or by including them as additional covariates in the model. They showed that principal components correctly identified and corrected for population structure based on geographic differences. However, \glspl{pc} perform poorly in modelling family structure or cryptic relatedness  \citep{Yu2006,Zhao2007,Kang2010,Casale2015}. \citet{Yu2006} have proposed to use a linear mixed model approach to control for population structure and relatedness. The key assumption in this approach is that phenotypic covariance between individuals based on population structure or relatedness is proportional to their relative relatedness. They showed together with \citet{Malosetti2007} and \citet{Zhao2007} that linear mixed models in the analysis of structured samples yield higher power while controlling better for type I errors than \textit{Genomic Control}, \textit{Structured Associations} and principal components.

\subsection{Linear mixed models}
\label{subsection:lmm}
\gls{lmm} describe to linear relationship between the response vector and a number of fixed (deterministic) effects and random (unknown) effects. While fixed effects are modelled by estimating the effect sizes of known explanatory variables (\cref{eq:linear-regression-matrix}), random effects model a random variable for which distribution parameters are estimated. Specifically, for the response vector of \(N\) samples \(\mat{y} \inR N 1\), the design matrix of \(F\) fixed effects \(\mat{X} \inR N F\) and their respective effect size vector \(\mat{\beta} \inR F 1\), the design matrix of \(U\) random effects \(\mat{Z} \inR N U\) and the random effect \tmat{b}, the linear mixed model is cast as
%
\begin{equation}
\mat{y} =\mat{X}\mat{\beta} +\mat{Z}\mat{b}+\mat{\psi},\, \text{with } \mat{b} \sim \normal 0 {\sigma_b^2\mat{\Sigma}}\,\text{and  }\mat{\psi} \sim\normal 0 {\sigma_e^2\mat{I}_N}.
\label{eq:lmm}
\end{equation}
%
As in the simple linear model (\cref{eq:linear-regression-matrix}), the residual noise is assumed to follow a normal distribution with mean zero and variance parameter \(\sigma_b^2\). In the formulation considered here, the covariance of the random effect is described by a known covariance matrix \tmat{\Sigma} and its variance parameter \(\sigma_b^2\). 
%
\Cref{eq:lmm} can be expressed as the likelihood  of the joint probability distribution of \tmat{y} and \tmat{b}
\begin{align}
p\left(\mat{y}, \mat{b}\mid \mat{\beta},\sigma_b^2, \sigma_e^2\right) &= p\left(\mat{y} \mid \mat{\beta}, \mat{b}, \sigma_e^2\right)p\left(\mat{b}\mid \sigma_b^2\right) \label{eq:lmm-prob} \\
&= \normal {\mat{y} \mid \, \mat{X\beta} + \mat{Zb}}{\sigma_e^2\matsub{I}{N}} \normal {\mat{b} \mid \,\mat{0}}{\sigma_b^2\mat{\Sigma}}\label{eq:lmm-norm}
\end{align}
%
To find the estimates of the unknown parameters \(\mat{\beta}, \sigma_b^2, \sigma_e^2\), one can first marginalise out \tmat{b} 
%
\begin{align}
p\left(\mat{y} \mid \mat{\beta},\sigma_b^2, \sigma_e^2\right) &= \int p\left(\mat{y} \mid \mat{\beta}, \mat{b}, \sigma_e^2\right)p\left(\mat{b}\mid \sigma_b^2\right) db \label{eq:lmm-marginal-prob} \\
&= \normal {\mat{y} \mid  \, \mat{X\beta}}{\sigma_b^2\mat{Z}\mat{\Sigma}\matsub{Z}{T} + \sigma_e^2\matsub{I}{N}}  \label{eq:lmm-marginal-norm} 
\end{align}
%
and then find the estimates that maximise the marginal likelihood \(\mathcal{L}\left(\mat{\beta}, \sigma_b^2, \sigma_e^2\right) = p\left(\mat{y}\mid \mat{\beta},\sigma_b^2, \sigma_e^2\right)\). 
Estimates are usually found by \gls{reml} instead of \gls{mle} to avoid bias in the estimation of the variance components \(\sigma_b^2\) and \( \sigma_e^2\). In contrast to the simple linear model (\cref{subsubsection:REML}), the \gls{reml} of parameters in linear mixed models cannot be solved in closed-form. Different methods for the efficient estimation of the model parameters have been proposed e.g. \citep{Lippert2011}, but will not be described in detail here. In this thesis, the \gls{lmm} framework LIMIX and accompanying methods (mtSet) were used to build the association models. Within this framework, the \gls{reml} of the model parameters are found via Broyden's method \citep{Broyden1965}. Details of the implementation can be found in \citep[Supplementary material]{Casale2015}.

\subsubsection{Linear mixed models in genetic association studies}
\label{subsection:lmm-genetics}
In genetic association studies, \glspl{lmm} describe the trait of interest as the sum of genetic fixed and random effects, i.e. single variants and background genetic effects, possible additional covariates and residual noise: 
%
\begin{equation}
\mat{y} =\mat{x}\beta + \mat{F}\mat{\alpha} + \mat{g}+\mat{\psi} \, \text{ with } \mat{g} \sim \normal 0 {\sigma_g^2\mat{R}} \text{ and } \mat{\psi} \sim\normal 0 {\sigma_e^2\mat{I}_N}.
\label{eq:lmm-uv}
\end{equation}
%
\begin{align*} 
\text{with }\,\,\,\,& \text{the phenotype vector }\mat{y} \inR N 1,\\
& \text{the genetic profile of the SNP being tested }\mat{x} \inR N 1,
\end{align*} %
\begin{align*}
& \text{the effect size of the SNP } \beta \inR 1 1\\ 
& \text{the matrix of $K$ covariates }\mat{F} \inR N K\\ 
& \text{the effect of covariates } \mat{\alpha} \inR K 1 \text{ and}\\
& \text{the genetic relatedeness matrix }\mat{R} \inR N N.
\end{align*} 
%
In analogy to \cref{eq:lmm-marginal-norm}, the random effect \tmat{g} can be marginalised out, leading to the likelihood expression for \cref{eq:lmm-uv} as
\begin{align}
\mat{y} &\sim \normal {\mat{F}\mat{\alpha} +\mat{x}\beta}{\sigma_g^2\mat{R} + \sigma_e^2\mat{I}_N}.
\label{eq:lmm-uv-matrix}
\end{align}
%
In \cref{eq:lmm-uv-matrix}, the genetic covariance structure of the samples, as expressed by the genetic relatedness matrix \tmat{R}, is integrated in the overall covariance structure of the model. As discussed in the next section, the covariance structure introduced by \tmat{R} captures population structure and polygenic background and leads to well-behaved statistics under the null model \citep{Yu2006,Kang2008}.

\subsubsection{Estimating the kinship between samples}
 \label{subsubsection:grm}
Traditionally, \glspl{lmm} have been widely used in association studies with pedigrees of known relationship \citep{Eu-ahsunthornwattana2014}. The pedigree relationship between two individuals was used to estimate their predicted proportion of the genome that is \gls{ibd}. The concept of \gls{ibd} is based on the random Mendelian sampling of chromosomes during successive meiosis from a common ancestor. As such, \gls{ibd} as a measure is always relative to the founders in the pedigree. \gls{ibd} estimates can also be generated for a population, where they have to be defined relative to some ancestral population or time point \citep{Browning2010,Glazner2012}. A matrix of pair-wise \gls{ibd} estimates is then used as the genetic relatedness matrix \tmat{R} in the linear mixed model (\cref{eq:lmm-uv-matrix}).

Alternatively, the genetic relatedness matrix can be estimated from genome-wide genetic marker information. \citet{Nejati-Javaremi1997} showed in simulations that if all loci contributing to a given trait were known, the accuracy of phenotype predictions based on this relatedness matrix estimated from those loci would be higher than for matrices estimated on pedigree information. Similarly, \citet{Villanueva2005} showed that the accuracy of breeding values from relationship matrices computed based on genetic markers is higher than for matrices derived from pedigree information. Extending these simulations, \citet{Hayes2009} showed that the increased prediction accuracy also holds when the relatedness matrix is estimated for a cohort of unknown pedigree using dense genetic markers instead of all true, but unknown causal loci. The use of such a \gls{rrm} is now widely employed in \gls{gwas} of large cohorts and plant and animal breeding studies as it is able to capture small differences in the proportion of genetic markers that are shared between seemingly unrelated individuals \citep{Lee2010,Lopes2013}. A common strategy for the estimation of the \gls{rrm}, which is used in this thesis, is to compute the average allelic correlation matrix
%
 \begin{equation}
 \mat{R} = \frac{1}{S}\mat{XX}^T
 \label{eq:relatedness}
 \end{equation}
 %
where \(S\) is the number of \glspl{snp} used for the estimation and \tmat{X} is the \(N \times S\) matrix of standardised genotypes of the samples \(N\) \citep{Patterson2006,Yang2011}. To derive the standardisation of the genotypes based on their allele frequency \citep{Patterson2006,Yang2011,Casale2015}, consider the bi-allelic genotype at the \(i\)th sample \tmatsub{x}{i} in Hardy-Weinberg equilibrium i.e. with the allele frequencies of the alleles  \(p + q =1\) and the genotype frequencies \(p_i^2 + 2p_iq_i + q_i^2 = 1\). Here,  \(p\) is defined as the reference allele and  \(q\) as the alternative allele. In the additive genotype model (\cref{subsection:lm-uv}), the genotypes can be described in terms of allele dosage \(d\), with \(d(p_i,p_i) = 0\), \(d(p_i,q_i) = 1\) and \(d(q_i,q_i) = 2\). Based on allele dosages, the expected value of the genotype is defined as 
\begin{align}
E(x_i) &=  d(p_i,p_i) \times p_i^2 + d(p_i,q_i) \times 2p_iq_i + d(q_i,q_i) \times  q_i^2 \\
		  &= 2p_iq_i + 2q_i^2 = 2(1-q_i)q_i + 2q_i^2 = 2q_i.
\end{align}
%
With the expected value of the genotype, its variance and standard deviation can be computed
\begin{align}
\text{Var}(x_i) &= E(x_i^2) - E(x_i)^2 \\
&= d(p_i,p_i)^2 \times  p_i^2 + d(p_i,q_i)^2  \times 2p_iq_i + d(q_i,q_i)^2  \times q_i^2 - (2q_i)^2 \\
&=  2q_i(1-q_i) \\
\sigma(x_i) &= \sqrt{\text{Var}(x_i)} = \sqrt{2q_i(1-q_i)}
\end{align}
%
and the genotypes standardised as
\begin{equation}
\bar{x}_i = \frac{x_i - 2q}{\sqrt{2q(1-q)}}.
\end{equation}
%
Different strategies have been proposed for the selection of genetic markers in the \gls{rrm} estimation, including a two-stepped analysis approach allowing for preselection of phenotype-specific variants \citep{Lippert2013} and grouping \glspl{snp} by haplotype \citep{Zhao2007,Kang2008}. The latter avoids the bias introduced by the potentially unequal number of experimentally genotyped/imputed \glspl{snp} per haplotype \citep{Speed2017}. Similarly, choosing only \glspl{snp} which are in approximate linkage equilibrium can avoid this bias \citep{Browning2008}. As described in \citep{Eu-ahsunthornwattana2014}, \glspl{snp} in approximate linkage equilibrium can be found by strict \gls{ld} pruning in genomic windows of appropriate size (depending on the organism and study design). Throughout this thesis, \gls{rrm} estimates are always based on \gls{ld}-pruned \gls{snp} sets.  

\subsection{Joint analysis of multiple phenotypes}
\label{subsection:joint-analysis}
Many cohort studies today, ranging from studies in model organism such as yeast and \textit{Arabidopsis thaliana} to human, have rich, high-dimensional datasets including molecular, morphological or imaging derived traits \citep{Bloom2013,Atwell2010,Astle2009,Shaffer2016,Stein2010}. However, these traits have often been analysed separately,  partly for simplicity and partly because of a paucity of models suitable for the analysis of high-dimensional phenotype data. A variety of multi-trait models have been developed which can be broadly grouped into three different classes: i) dimensionality reduction techniques, ii) meta-analysis approaches and iii) multivariate regression models (reviewed in \citep{Shriner2012,Yang2012b}). 

\paragraph{Dimensionality reduction techniques} Dimensionality reduction methods in genotype\-phenotype mapping seek to find a suitable projection of high-dimensional phenotypes into a lower dimensional space. Two  commonly used dimensionality reduction methods are \gls{pca}  and \gls{cca}. An overview of other methods and a more detailed description of methods in this section will be given in \cref{chapter:DimReduction}. 

In \gls{pca} , the phenotype data is projected into its principal components - the eigenvectors of the empirical covariance matrix. The amount of variance that each component explains is proportional to its corresponding eigenvalue. The dimensionality reduction is achieved by using all those principal components (in increasing order) until the cumulative sum of the eigenvalues reaches a predefined threshold of total phenotypic variance that should be retained. \gls{pca}  as a dimensionality reduction technique has for instance been used in studies to find links between genotypes and facial features or obesity phenotypes \citep{Liu2012,Claes2014,He2008}. Recently, Aschard and colleagues \citep{Aschard2014} demonstrated that simply focusing on the principal components with the highest variance might not exploit the full potential of using \gls{pca}  for genetic association. They propose a model of combined \gls{pca} where the \glspl{pc} are grouped based on the level of variance they explain and show a power gain in detecting genetic associations using this approach.
% Principal components reflect the internal structure of the data in terms of the phenotypic variance that they explain: the highest amount of phenotypic variance explained lies in the first component, the second highest variance in the second component and so forth.

While the \gls{pca}  dimensionality reduction approach focuses on the phenotype space and subsequent association with the genotypes, \gls{cca}   seeks to maximise the canonical (ordered) correlation between the transformed phenotypes and genotypes i.e. finding the optimal linear transformation of the phenotypes while simultaneously testing for the association with the genotypes.  For a single genetic marker, \gls{cca}  finds the linear phenotype transformation that explains the maximum amount of covariance between this genotype and all traits by solving the eigendecomposition of a complex phenotype-genotype covariance term \citep{Yang2012b}. Ferreira and Purcell \parencite*{Ferreira2009} showed in simulations that \gls{cca}  with multiple traits and one genetic marker controls well for type I errors and has increased power compared to univariate tests. In order to extend \gls{cca}  to more than one marker, the genotypes also have to undergo a linear transformation and the maximum canonical correlation is found by solving two eigenvalue problems. As the number of genotype markers in \gls{gwas}  exceeds the number of samples, estimates of the genotype covariance term becomes unreliable \citep{Schaefer2005}. Several methods have been developed to circumvent this issue, making use of sparse matrices \citep{Parkhomenko2009} or a priori grouping of the genotypes \citep{Naylor2010}. 
%The transformation of the phenotypes and the maximal canonical correlation are found by eigendecomposition of a complex covariance term containing the empirical sample covariance matrices of the phenotypes and the genotypes as well as the cross-covariances of the phenotypes and genotypes. %and the eigenvector corresponding to the largest eigenvalue is used for transformation of the phenotypes

\paragraph{Meta-analysis approaches} Meta-analysis approaches combine the simplicity of the univariate approaches with the advantages of the multivariate approach. For each phenotype, a univariate association study is conducted and the summary statistics of these tests are combined. Many methods for combining the summary statistics \citep{Xu2003,Yang2010,Bolormaa2014} go back to the work by O'Brien \citep{OBrien1984}, who proposed to use a linear combination of the observed test statistics for each univariate test as the new statistics to be evaluated for significance. Subsequent studies proposed different methods for choosing the weights in the linear combination of the univariate test statistic or keeping the same principle computation but re-formulating the alternative hypotheses \citep{Yang2010,Xu2003}. These studies showed an increase in power for applying the combined statistic on small marker sets or numbers of phenotypic traits. \citet{Bolormaa2014} showed that the power gains also hold for genotype to phenotype mapping of 32 traits across all genome-wide markers. 

%\(\mat{T} =(T_1, \dots, T_P)^T\) as the new statistics to be evaluated for significance. It is defined as: \(S = \mat{J}^T \mat{\Sigma}\mat{T}\), with \(\mat{J} = (1, 1, \ldots, 1)^T\).  The statistic has been modified in a number of studies, by adapting either the weighting matrix \tmat{J}, the covariance matrix \tmat{\Sigma} or both. Xu and colleagues \parencite*{Xu2003} optimised \(S\) to allow for testing against a general \(\mu\) rather then for a case where  \(\mu_1= \ldots =\mu_P\) by allowing for flexible, but restrained weights in \tmat{J}. Similarily, \citet{Yang2010} proposed non-uniform weights to reflect heterogeneity in the means and use a sample splitting and cross-validation approach to determine the optimal weights. 
%\tmat{T} is asymptoctically normal distributed with mean \(\mat{\mu} = (\mu_1,\ldots, \mu_P)^T\) and covariance matrix \(\mat{\Sigma}\). O'Brien stastistic allows for testing the Null hypothesis \(H_0: \mu = 0\) against the alternative hypothesis of  \(H_1: \mu_p \ge 0, p=1, \ldots , P \) and is most powerful if \(\mu_1= \ldots =\mu_P\) \citep{Xu2003}. 

\paragraph{Regression models} There are a number of different regression models that allow for the multivariate analysis of phenotypes. Among them are graphical models, generalised estimation equations and frailty models, for which a summary of methods and application can be found in \citep{Shriner2012,Yang2012b}. Here, I will focus on describing the development of multivariate linear regression models for genotype-phenotype mapping. 

Before the era of \gls{gwas}, Jiang and colleagues \parencite*{Jiang1995} proposed a multi-trait model where the phenotypes are jointly modelled as the sum of the fixed genetic effects of interest, fixed effects for genetic background variation and residual noise. They show that the joint analysis of traits can increase power to detect the underlying genetics and can increase the precision of the parameter estimates. The significance of the association is determined via a likelihood ratio test of the parameter estimates under the Null model, where the fixed genetic effect is zero, and the parameter estimates under the alternative model. The alternative model design depends on the underlying biological hypothesis regarding the effect of the genetic variant. Here, Jiang and colleagues differentiate between hypotheses for a simple joint mapping of phenotypes, pleiotropy and gene-environment interactions.% Joint mapping does not make any assumptions about the underlying genetic architecture and simply tests if an association can be found when both traits are analysed jointly, i.e. the effect of the genetic variant is non-zero for at least one of the traits. This hypothesis can be extended in requiring that the effect on both traits is unequal to zero. In this case, the genetic variant is considered to be pleiotrophic. To test for gene-environment interaction, the different conditions a trait was studied in can be treated as different traits and be jointly mapped. If the effect size estimates of the genetic variant are not equal, the variant is considered to have environmental interactions.  

Methods developed thereafter often use the same underlying hypotheses for the mapping, but different techniques for the evaluation of the significance. For instance, two other groups developed methods for the joint analysis of traits based specifically on the \gls{rss} matrix of the standard linear model estimated at each locus tested \citep{Knott2000,Korol2001}. In the model proposed by Knott and Haley, the different properties and descriptors of the \gls{rss} are used to determine the significance of the association. To test for pleiotropy for instance, the determinant of the \gls{rss} at the test locus is compared to the \gls{rss} of the null model of no association. In contrast, Korol and colleagues propose to use the \gls{rss} of the multi-trait mapping as a means for trait transformation and dimensionality reduction. The resulting one-dimensional trait per sample is fitted in a single-trait test for significance testing.  

While methods described so far have only used fixed genetic effects, Korte and colleagues \parencite*{Korte2012} were the first to introduce a random genetic effect into the model. Based on the original model by Jiang, they substituted the fixed effect accounting for background genetics by a random effect, turning the multivariate linear model into a multivariate linear mixed model. Since this initial model for multi-trait testing, a number of publicly available linear mixed model frameworks for the genome-wide mapping of a moderate number of traits have been developed. \citep{Yang2014,Lippert2014,Zhou2014,Casale2015}. 

Out of the different approaches described above, multivariate linear mixed models have the additional advantage that they can control for complex relatedness and population structure (\cref{subsubsection:relatedness-model-variables}). 

\subsection{Linear mixed models for the joint analysis of multiple phenotypes}
The multivariate linear mixed model with \(P=\left\{1,2,\dots,p\right\}\) phenotypes for \(N\) samples can be derived as an extension of the univariate model with \(P=1\) phenotype for \(N\) samples described in \cref{eq:lmm-uv-matrix}. Consider \cref{eq:lmm-uv-matrix} as the model description for the \(i\)th phenotype (omitting covariates for simplicity) :
\begin{align} 
\mat{y}_i &\sim \normal {\mat{x}\beta_i}{{\sigma_g^2}_i\mat{R} + {\sigma_n^2}_i\mat{I}_N},
\label{eq:lmm-mv-p}
\end{align}
%
with \(\mat{x} \inR N 1\) the genotype, \(\beta_i\) the effect size of the genotype for trait \(i\),  \({\sigma_g^2}_i\) and \({\sigma_n^2}_i\) the covariance terms of the genetic and noise random effect for trait \(i\), \(\mat{R} \inR N N\)  the realised relationship matrix estimated from the genotype data and \(\mat{I}_N\) the identity matrix. As described by \citet{Henderson1976}, multivariate LMMs model the covariance between trait \(i\) and \(j\) as
\begin{align}
\text{Cov}\left(\mat{y}_i, \mat{y}_j\right) = {\rho_g}_{ij}{\sigma_g^2}_i{\sigma_g^2}_j\mat{R} +  {\rho_n}_{ij}{\sigma_n^2}_i{\sigma_n^2}_j\mat{I}_N,
\label{eq:cov-lmm-mv}
\end{align}
%
with \( {\rho_g}_{ij}\) and \( {\rho_n}_{ij}\) the genetic and noise correlation between trait \(i\) and \(j\), respectively. 

Using the multivariate linear model described for trait \(i\) in \cref{eq:lmm-mv-p} and the expression of the \(ij\)-trait-trait covariance term in \cref{eq:cov-lmm-mv}, the multivariate \gls{lmm} for all traits \(P\) can be expressed as a matrix-normal distribution:
%
\begin{equation}
\mat{Y} = \mat{x}\mat{\beta}^T + \mat{G} + \mat{\psi}
\label{eq:lmm-mv}
\end{equation}
%
with the phenotype matrix \tmat{Y} and the effect size vector \tmat{\beta}
%
\begin{align}
 \mat{Y} &=
  \begin{bmatrix} 
 	\mat{y}_{1} & \cdots & \mat{y}_{P} 
 \end{bmatrix} \inR N P,\\
%
 \mat{\beta} &=
  \begin{bmatrix} 
 	\mat{\beta}_{1} & \cdots & \mat{\beta}_{P} 
 \end{bmatrix} \inR P 1,\\
 %
 \end{align}
 %
the random genetic effect \tmat{G} and the random noise effect \tmat{\psi} following a matrix-variate normal distribution with row covariance \tmat{R} and \(\mat{I}_N\) and column covariance \(\mat{C}_g\) and  \(\mat{C}_n\)
\begin{align}
 \mat{G} &= \matrixnormal N P 0 {\mat{R}}{\mat{C}_g},\\
%
 \mat{\psi} &= \matrixnormal N P 0 {\mat{I}_N}{\mat{C}_n},
 %
 \end{align}
%
and the genetic and noise trait-by-trait covariance matrices \(\mat{C}_g\) and  \(\mat{C}_n\) 
%
 \begin{align}
 \mat{C}_g &=
  \begin{bmatrix}
  {\sigma_g^2}_1 &  {\rho_g}_{12} {\sigma_g}_1 {\sigma_g}_2 & \cdots & {\rho_g}_{1P} {\sigma_g}_1 {\sigma_g}_P \\
  {\rho_g}_{12} {\sigma_g}_1 {\sigma_g}_2 & {\sigma_g^2}_2 & \cdots & {\rho_g}_{2P} {\sigma_g}_2 {\sigma_g}_P \\
   \vdots & \vdots & \ddots & \vdots \\
   {\rho_g}_{1P} {\sigma_g}_1 {\sigma_g}_P  &  {\rho_g}_{2P} {\sigma_g}_2 {\sigma_g}_P  & \cdots &{\sigma_g^2}_P 
   \end{bmatrix}, \\ \\
 %
 \mat{C}_g &=
  \begin{bmatrix}
  {\sigma_n^2}_1 &  {\rho_n}_{12} {\sigma_n}_1 {\sigma_n}_2 & \cdots & {\rho_n}_{1P} {\sigma_n}_1 {\sigma_n}_P \\
  {\rho_n}_{12} {\sigma_n}_1 {\sigma_n}_2 & {\sigma_n^2}_2 & \cdots & {\rho_n}_{2P} {\sigma_n}_2 {\sigma_n}_P \\
   \vdots & \vdots & \ddots & \vdots \\
   {\rho_n}_{1P} {\sigma_n}_1 {\sigma_n}_P  &  {\rho_n}_{2P} {\sigma_n}_2 {\sigma_n}_P  & \cdots &{\sigma_n^2}_P 
   \end{bmatrix}.
%
\end{align}
% 
The matrix-variate distribution of the phenotype matrix \tmat{Y} can be expressed in terms of a multivariate normal distribution (for details refer to \cref{eq:mvn} to \cref{eq:G-vec})
%
\begin{equation}
\text{vec}\left(\mat{Y}\right) \sim \multinormal N P {\text{vec}\left(\mat{x}\mat{\beta^T}\right)}{\mat{C}_g \otimes \mat{R} + \mat{C}_n \otimes\mat{I}_N}.
\label{eq:lmm-mv-multin}
\end{equation}
%
where the Kronecker products \(\otimes\) of \(\mat{C}_g \otimes \mat{R}\) and \(\mat{C}_n \otimes\mat{I}_N\)  follow the definition of the Kronecker product for any two matrices as:
\begin{align*}
 \mat{C}_g \otimes R &=
  \begin{bmatrix}
  {\mat{C}_g}_{11}\mat{R}  &  \cdots &  {\mat{C}_g}_{1P}\mat{R}  \\
   \vdots &  \ddots & \vdots \\
  {\mat{C}_g}_{P1}\mat{R}  &  \cdots &  {\mat{C}_g}_{PP}\mat{R}  \\
   \end{bmatrix} \text{ and }
   %
    \mat{C}_n \otimes  \mat{I}_N&=
  \begin{bmatrix}
  {\mat{C}_n}_{11}\mat{I}_N &  \cdots &  {\mat{C}_n}_{1P}\mat{I}_N  \\
   \vdots &  \ddots & \vdots \\
  {\mat{C}_n}_{P1}\mat{I}_N  &  \cdots &  {\mat{C}_n}_{PP}\mat{I}_N  \\
   \end{bmatrix}.
\end{align*}
%
The likelihood of the multivariate linear mixed model is
%
\begin{equation}
\mathcal{L}\left(\mat{\beta}^T, \mat{C}_g, \mat{C}_n\right) = \normal {\text{vec}\left(\mat{Y}\right) \mid \text{vec}\left(\mat{x}\mat{\beta}\right)}{\mat{C}_g \otimes\mat{R} + \mat{C}_n \otimes\mat{I}_N}.
\label{eq:lmm-mv-likelihood}
\end{equation}
%
Maximising \(\mathcal{L}\left(\mat{\beta}, \mat{C}_g, \mat{C}_n\right)\) requires \(P\) parameter estimates for the fixed effect \tmat{\beta} and \(\frac{1}{2}P \left( P + 1\right)\) parameter estimates for each of the \(P \times P\) covariance matrices \(\mat{C}_g\) and  \(\mat{C}_n\). Due to the large number of parameters, \gls{reml} for multivariate \gls{lmm} often relies on gradient-based optimisation methods. Different schemes have been used in \gls{lmm} for genetic analysis, including average information \gls{reml} \citep{Gilmour1995} (used in \citep{Yang2011}) quasi-Newton methods like Broyden's method \citep{Broyden1965} (used in \citep{Casale2015}), and Brent's algorithm \citep{Brent1971} (used in \citep{Lippert2011,Svishcheva2012}). The \gls{reml} implementation of the framework used in this thesis builds on Broyden's method and the detailed derivation can be found in \citep[Supplementary material]{Casale2015}.


\subsubsection{Hypothesis testing in multi-trait association studies}
\label{subsubsection:model-design}
As described by \citet{Jiang1995} and \citet{Korte2012} (summarised in \cref{subsection:joint-analysis}, \subfig{regression models}), when testing the association of a genetic marked across multiple phenotypes, different hypotheses for the underlying genetic trait architecture can be formulated. In the most simple case, one can test if the genetic variant has an effect on any of the traits \(P\) (any effect test) i.e. the effect size of the fixed effect \tmat{\beta} is unequal to zero for at least one trait : \(H_\text{A}: \mat{\beta} \ne \mat{0}_P\).  In this \(P\)-degrees of freedom test, the corresponding null hypothesis of no association is that the effect size of the fixed effect is equal to zero: \(H_0:\mat{\beta}  = \mat{0}_P\). In the common effect model, the variant has the same effect size across all traits with \(H_\text{A}: \mat{\beta}  = \mat{1}_P\beta\) and is tested for significance in a one degree of freedom model versus the null hypothesis of no association (\(\beta  = 0\)). A more complicated model allows to test for specific effects of the variant on a given trait \(p\). This can be tested with a one degree of freedom test where a model containing a common effect across all traits and a specific effect for trait \(p\) is compared against the common effect model.