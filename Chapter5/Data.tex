\section{Data}
\subsection{Genotypes}
\label{subsec:gentoypes}
\paragraph{Quality Control.} Genotyping and genotype calling were carried out at the Genotyping and Microarray facility at the Wellcome Trust Sanger Institute, UK and Duke-NUS Medical School, Singapore. Genotypes were assessed in five batches using Illumina HumanOmniExpress- 12v1-1 (Sanger, two batches), Illumina HumanOmniExpress- 24v1-0 (Duke-NUS, two batches) and Illumina HumanOmniExpress- 24v1-1 chips (Duke-NUS). SNPs were called via the GenCall software for clustering, calling and scoring of genotypes \citep{Teo2007}. For batches run on the same platform, genotype signals were combined and called in a single analysis, leading to three independent genotype batches: sanger12 (\num{1344} samples), Duke-NUS12 (\num{284} samples), Duke-NUS3 (\num{96} samples). I carried out the quality control (QC) of the raw genotype calls, the phasing and the imputation, all on a per-batch level. The final QC of the imputed data was conducted across all batches and only SNP passing the control in every batch were used in subsequent analyses. 

Prior to QC, I matched the rsID descriptions (chromosome, chromosomal positions and allele order) of the three batches to the reference set I would use for phasing and  imputation, a combined UK10K \citep{UK10KConsortium2014} and \num{1000} Genomes \citep{Abecasis2012} reference panel. For rsIDs not included in the reference panel I retrieved location and allele order from the ensembl human variation annotation (GRCh37p13, 15.04.2016). rsIDs that matched to neither reference were removed from further analyses (\num{4681} across all chips). In order to avoid batch effects in SNP calling simply based on probe sequences, I confirmed that probes targeting the same SNP on different chip versions had the same sequence. As this was the case, no SNPs had to be removed at this stage. 
I followed an adapted quality control protocol by Anderson and colleagues \citep{Anderson2010} to asses the quality of the genotyping on a per-individual and per-marker level. Unless stated otherwise, the PLINK software (version 1.9) \citep{Purcell2007, Chang2015} was used for all QC analyses. In summary, the per-individual QC included the identification of individuals with discordant sex information, missing SNP rates (more than \num{3}\% of SNPs not called) and outlying heterozygosity rates (outside three standard deviations of mean heterozygosity rate). Population substructures arising due to different ethnical origins of samples were examined by comparison of sample genotypes to genotypes of the HapMap Phase III study \citep{HapMap2005} from four ethnic populations. Samples that clustered with HapMap III individuals of caucasian ancestry were kept for further analyses. The per-marker QC included filtering of SNPs with missing call rate in more than \num{1}\% of the samples and SNPs which significantly deviate from Hardy-Weinberg equilibrium (HWE, \(p < 0.001\)). After removal of samples and SNPs that failed QC, I confimed that any pattern of missing genotype information was not batch-specific. To analyse these patterns, I treated each pair-wise combination of batches as a case-control set-up and computed the differential missingness of SNPs common to all batches. None of the \num{631877} common SNPs had to be removed due to significant differential missingness (\(p < 1e-5\)). Table~\ref{tab:genoOverview} shows an overview of sample and SNP numbers before and after the QC described above. The QC plots for each step can be found in \cref{suptab:geno-QC}. 
\\
% Table generated by Excel2LaTeX from sheet 'GenotypesSummary'
\begin{table}[htbp]
  \centering
  \caption[\textbf{Sample and SNP numbers before and after the QC. }]{\textbf{Sample and SNP numbers before and after the QC. } For each batch (first column), the number of male (m)/female (f) samples and SNPs before and after QC is listed. Rate specifies the genotyping rate of samples within one batch after QC. }
    \begin{tabular}{lrrrrrr}
    \toprule
          & \multicolumn{2}{c}{pre-QC} &       & \multicolumn{3}{c}{post-QC} \\
\cmidrule{2-3}\cmidrule{5-7}          & samples (m/f) & SNPs  &       & samples (m/f) & SNPs  & Rate \\
\cmidrule{2-7}    Sanger12 & \num{1344}  (\num{614}/\num{730}) & \num{719665} &       & \num{998} (\num{463}/\num{535}) & \num{677036} & \num{0.998} \\
    Duke-NUS12 & \num{284} (\num{118}/\num{166}) & \num{716503} &       & \num{179} (\num{68}/\num{111}) & \num{682016} & \num{0.998} \\
    Duke-NUS3 & \num{96} (\num{48}/\num{48}) & \num{7713014} &       & \num{62} (\num{34}/\num{28}) & \num{657497} & \num{0.998} \\
    \bottomrule
    \end{tabular}%
    \label{tab:genoOverview}%
\end{table}%

\paragraph{Phasing and imputation.} Phasing and imputation were conducted in two separate steps. The SHAPEIT software (version 2) \citep{Delaneau2012,Delaneau2013} was used to generate a set of estimated haplotypes based on the combined \num{1000} Genomes \citep{Abecasis2012} and UK10K \citep{UK10KConsortium2014} reference panel as genetic map. 

shapeit parameters
window_size=2
thread=8
states=200

impute parameters
chunk_size=3000000
buffer_size=250
k_hap=1000
Ne 20000

snptest -summary_stats_only -hwe

The same reference panel was used for the imputation of the samplesâ€™ genome-wide SNP genotypes via the IMPUTE2 software (version 2.3.0) \citep{Marchini2007, Howie2009}. 

\paragraph{Combining dataset.} I combined the three genotype batches after imputation and filtered them again on a per-marker and per-sample level. SNPs with a certainty score of less than \num{0.4} in at least \num{1} of the batches were excluded from further analyses. After combining the three datasets, SNPs were filtered for deviation from HWE (\(p <0.001\)) and a minor allele count of at least \num{25} alleles (corresponding to a minor allele frequency of \num{0.01}).  On sample level, related individuals were excluded from the dataset in order to remove bias towards genotypes shared within a family and to accurately represent the allele frequency of the entire population. Relatedness was estimated by the proportion of SNPs shared between two individuals and subsequent calculation of identity by descent estimated as PI\_HAT via PLINK as described in \citep{Anderson2010}. From any pair of individuals with a PI\_HAT of greater than \num{0.125}, the individual with the higher SNP calling rate was retained in the analysis. 

After imputation and imputation quality control, the data set contains \num{9233118} SNPs. IMPUTE2 yields imputed genotypes encoded in triplets of posterior probabilities for the possible allele combinations \((AA, AB, BB)\). These genotypes were converted into expected genotypes \(G\) by the dosage model \citep{Howie2011}:

\begin{equation}
	G = 0 \times p(AA) + 1 \times p(AB) + 2 \times p(BB) = p(AB) + 2 \times p(BB)
\end{equation}


\subsection{Phenotypes}
The phenotype work was done by my collaborators, in particular Antonio de Marvo. CMR imaging and generation of 3D models of the left ventricle derived from these images were conducted at Hammersmith Hospital, London. In the following, I will briefly describe the methodology of the automatic phenotyping approach. The technical details of the image aquisiton, the analysis and their improved performance over standard methods are described in detail in \citep{DeMarvao2014}. 

The aim of an automated analysis of the CMR images is the accurate extraction of cardiac structures from the raw images (\cref{fig:segmentation}\subfig{1}) to generate 3D models of the samples' hearts. The cardiac structures of interest in this study were left ventricular cavity, myocardium and right ventricular bloodpool at end-diastole and end-systole. The automatic approach developed by Antonio de Marvo and colleagues uses a local database of segmented and quality controled CMR images to which each newly acquired image is compared. The database was created by Antonio, who initially selected \num{20} subjects and manually classfied the approximately \num{140000} voxels per image into the \num{3} categories named above. In the database generation phase, subsequent successful segmentations of new images described by the method below were added to database, yielding a total of \num{1072} of images in the final database. In addition to serving as a database for the segmentation algorithm, the database images were used to generate a template image of average heart size, position and orientation. For each new image, six landmarks are manually placed on the image, which enables the subsequent non-rigid image registration between the target and the atlas images. After registration, a multi-atlas PatchMatch algorithm is used which finds corresponding patches of adjacent voxels within the atlas and taget images (\cref{fig:segmentation}\subfig{2}). Each patch in the target image is given the label of the closest matching atlas patches and combining the labels for all patches produces the final segmentation. Finally, the segmented image is registered to the template image to make the spatial coordinates in the 3D models consistent betwen all samples.  Using a surface rendering algorithm allows for the extraction of information from a segmentation volume into a surface representation. Through such an algorithm, the wall thickness, curvature and  fractional wall thickening at \num{27623} positions in the left ventricle were extracted for each sample (\cref{fig:segmentation}\subfig{2}). In addition, the left ventricular mass was computed based on the volume and density of the myocard. 
\\

\begin{figure}[h]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip,width=\textwidth]{Chapter5/Figures/Segmentation.pdf}
	\caption[\textbf{Cardiac phenotyping based on CMR images. }]{\textbf{Cardiac phenotyping based on CMR images. } 1. Detailed single breath-hold-3D images of the heart were acquired in the left ventricular short axis (LVSA) plane from base to apex using a 1.5T Philips Achieva system images. 2. The images were segmented  left ventricular myocardium (green), left ventricular blood pool (red) and right ventricular blood pool (yellow) and registered to a common template image via a multi atlas-based technique. 3. Through a surface rendering algorithm of the registered segmentations, a 3D model of the heart was generated and wall thickness meassurements derived at \num{27623} positions of the left ventricle. The left ventricle is shown in solid colors, with the colorscheme representing average wall thickness, increasing from light to darker colors. As a reference, the right ventricle is depicted as a mesh.}
 	\label{fig:segmentation}
\end{figure}

To assess the reproducibility of the phenotyping approach, \num{1} individual was scanned \num{8} times and the images segmented as described above. These repeat scans allowed for the quantification of variation in the segmentations by the coefficient of variation (CV). The CV is a standardised measure of dispersion and is defined as the ratio of the standard deviation to the mean value. I computed the CV for each of the \num{27623} positions in the 3D heart model across the \num{8} scans and projected the results onto the template image (\cref{fig:reproducibility}). Overall, the dispersion is very low i.e. the reproducibility high. Only at the base of the left ventricle in proximity to the right ventricle can a slight increase in dispersion be observed (\cref{fig:reproducibility}, red area).  

The low dispersion shows the accuracy of the segmentation and justifies their use as phenotypes in subsequent analyses. 
\\ 

\begin{figure}[h]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip,width=0.7\textwidth]{Chapter5/Figures/Dispersion.pdf}
	\caption[\textbf{Phenotype reproducibility. }]{\textbf{Phenotype reproducibility. }The dispersion in left ventricular wall thickness at \num{27623} positions was computed as the standard deviation over the mean across \num{8} segmentations derived from independent scans of one individual. The right ventricle as shown as reference (mesh structure). }
 	\label{fig:reproducibility}
\end{figure}
