\section{LiMMBo: Linear mixed modeling with bootstapping}
\label{section:limmbo}
Linear mixed models have become a workhorse in genetic assocation studies as they allow to control for complex sample-to-sample covariance structures that can reflect population structure and relatedness. LMM can broadly be grouped into two categories, based on the estimation of their random effect covariance terms. Exact methods estimate the covariance term anew for each SNP, while approximate methods rely on the assumption that the effect sizes of the fixed effects are sufficiently small \citep{Kang2010,Zhang2010} and an estimate of the covariance terms under the null is a good approximation. Hence, in these methods, the covariance terms are only estimated once under the null model of no fixed genetic effect and are then used as estimates in the genome-wide associations. Within these categories, one can further distinguish between methods only applicable as univariate test or tests that allow for multivariate testing. Table~\ref{tab:lmmframeworks} summarizes commonly used frameworks and describes their computational complexity.  Amongst the exact methods, FaST-LMM reduces the complexity best in terms of sample size by selecting the number of SNPs to use for the estimation of the GRM.  However, it can only be applied in univariate analyses while MTMM and GEMMA extend to multivariate cases.  BOLT-LMM scales best with increasing samples sizes in the group of approximate test, by directly using the genotypes and not computing or storing the GRM. All other methods have an upfront \(O(N^3)\) operation for the eigendecomposition of the GRM. TASSEL reduces this complexity based on grouping of the samples and thereby effectively reducing the size of the GRM.

% Table generated by Excel2LaTeX from sheet 'LMMOverview'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Linear mixed model frameworks for genetic association studies.} A list of popular LMM frameworks, grouped by their usage of covariance estimates when fitting the alternative model (method). \(P\) indicates the maximum trait sizes that the model can be applied to. Models with specific parameters are described in more detail in the text (FaST-LMM and TASSEL). \(N\): number of samples; \(P\): number of traits; \(S_c\): number of SNPs used for singular value decomposition; \(c\):  compression factor with \(c=\frac{N}{g}\) for \(g\) individuals per group; \(t, t_1 \text{and} t_2\): average number of iterations needed to find parameter estimates. GRAMMAR-Gamma, FaST-LMM:\(t\) for Brenth's algorithm;  GEMMA, GCTA, MTMM: \(t_1\) for EM algorithm, 
 \(t_2\) for NR algorithm; BOLT-LMM: \(t\) for variational Bayes and conjugate gradients; TASSEL: \(t\) for ProcMixed algorithm in SAS; mtSet: \(t\) for LFBGS.}
    \begin{tabular}{llrrr}
    \toprule
    Method & Framework & LMM Complexity per SNP & \(P\) & Reference \\
    \midrule
    \multirow{3}[1]{*}{exact} & FaST-LMM & \(O(NS_c^2 + N^2 + tN)\) & 1     & \citep{Lippert2011} \\
          & MTMM  & \(O(t_1N^3P^3 + t_2N^3P^7 + N^2P^2)\) & 2     & \citep{Korte2012} \\
          & GEMMA & \(O(N^3 + N^2P  +  t_1NP^2 + t_2NP^6)\) & 10    & \citep{Zhou2014} \\
    \addlinespace[1.5ex]
    \multirow{6}[1]{*}{approximate} & EMMAX & \(O(N^3 + tN + N^2)\) & 1 & \citep{Kang2010} \\
          & TASSEL & \(O(\frac{1}{c^3}N^3)\) & 1     & \citep{Zhang2010} \\
          & GCTA  & \(O(t_1N^3P^3 + t_2N^3P^7)\) & 2     & \citep{Yang2011} \\
          & GRAMMAR-Gamma & \(O(N^3 + tN + N)\) & 1     & \citep{Svishcheva2012} \\
          & BOLT-LMM & \(O(tN)\) & 1     & \citep{Loh2015} \\
          & mtSet/LiMMix & \(O(N^3 + N^2 + t(NP^2 + NP^4)\) & \(10\sim 30\) & \citep{Lippert2014,Casale2015} \\
    \bottomrule
    \end{tabular}%
	\label{tab:lmmframeworks}%
\end{table}%

Eu-Ahsunthornwattana and colleagues \citeyear{Eu-ahsunthornwattana2014} analysed several LMM frameworks including FaST-LMM, GEMMA and EMMAX with respect to their control for type I error and estimation of kinships and compared the results obtained from each method. They find that the results of most methods tested are in concordance and their performance similar in terms of power and calibration when applied to real and simulated data. In conclusion, they recommend to choose a framework based on complexity and data structure requirements. 

With the generation of ever-increasing cohort sizes in genetic assocation studies, most LMM frameworks are optimised for the number of samples as described above for BOLT-LMM and TASSEL. While the remaining methods still have the upfront cubic computation of the GRM's eigendecomposition, subsequent steps have been adapted to scale linearly or quadratically with the number of samples for the majority of the applications. The optimisation in scaling for the sample dimenison comes as a trade-off for the scaling in the number of traits that can be analysed. From the multivariate methods listed above, the complexity in terms of trait number ranges from \(O(P^4)\) to \(O(P^7)\) which originates from the estimation of the trait-by-trait covariance component of the random effect and, in practice, limits these models to moderate trait numbers.

To extend the range of LMMs for high-dimensional phenotype sets, I chose to build on an approximate model in order to avoid the repeated estimation of the trait covariance matrices. The multivariate LMM developed by Lippert, Casale and colleagues (mtSet) \citeyear{Lippert2014,Casale2015} is the only approximate model that is computationally efficient for 10-30 traits, depending on the sample and trait covariance structures. Within this framework, the genetic variance \(\mathbf{G} \sim\mathcal{N_{N\times P}}\left(0, \mathbf{C_g} \otimes \mathbf{R}_N\right)\) and the noise variance component \(\mathbf{\Psi} \sim\mathcal{N_{N\times P}}\left(0, \mathbf{C_n} \otimes \mathbf{I}_N\right)\) are estimated by fitting the null model of the mvLMM (Eq.~\ref{eq:mvLMM}; omitting covariates for simplicity):
\begin{equation}
\mathbf{Y} \sim \mathcal{N }_{N\times P}\left(0, \mathbf{C_g} \otimes \mathbf{R} + \mathbf{C_n} \otimes \mathbf{I_N}\right)
\label{eq:vd}
\end{equation}
The covariance structure of \(\mathbf{G}\) and \(\mathbf{\Psi}\) is described by the Kronecker product $\otimes$ of the genetic trait-to-trait covariance matrix $\mathbf{C_g}$ with the genetic sample-to-sample relationship matrix $\mathbf{R}$ and of the noise trait-to-trait covariance matrix $\mathbf{C_n}$ with the identity matrix $\mathbf{I_N}$, respectively (Fig.~\ref{fig:vd}).  $\mathbf{R}$ is estimated from the SNP genotypes of the samples and captures the kinship of the samples, while the noise sample-to-sample covariance is assumed to be constant. The variance decompositon (VD) of $\mathbf{Y}$ into $\mathbf{G}$ and $\mathbf{\Psi}$ is achieved by estimating \(\mathbf{C_g}\) and \(\mathbf{C_n}\) via restricted marginal likelihood (RML). The complexity of the VD is \(O(N^2 + t(NP^2 + NP^4))\) with \(N\) the number of samples, \(P\) the number of traits, and \(t\) the number of iterations of Broyden's method for optimising the RML of the parameter estimates. From this equation, it becomes evident that as the number of traits increases, the complexity increases by a power of four and explains why this LMM set-up is not feasable for large trait sets. To overcome the bottleneck of estimating the trait-to-trait covariance matrices as a whole which is the reason for the complexity of \(P^4\), I developed a simple heuristic that efficiently uses a linear mixed model bootstrapping (LiMMBo) approach to estimate \(\mathbf{C_g}\) and \(\mathbf{C_n}\).

\subsection{Covariance estimation via bootstrapping}
The key innovation of LiMMBo is to perform the variance decomposition on \(b\) bootstrap samples of \(s\) traits instead of on the whole dataset, and use those bootstrap samples to reconstruct the full \(\mathbf{C_g}\) and \(\mathbf{C_n}\) matrices (Fig.~\ref{fig:vd}). In detail, from the total phenotype set with \(P\) traits, \(b\) subset of \(s\) traits are randomly selected. \(b\) depends on the overall trait size \(P\) and the sampling size \(s\) and is chosen such that each two traits are drawn together at least \(c\) times (default: 3). For each subset, the variance decomposition is estimated via the null model of the mvLMM, i.e. without the fixed genetic effect \(\mathbf{x}\) (Eq.~\ref{eq:mvLMM} and \ref{eq:vd}) and the $s \times s$ covariance matrices $\mathbf{C^s_g}$ and $\mathbf{C^s_n}$ recorded.  For each trait pair, their covariance estimate is averaged over the number of times they were drawn. The challenge lies in combining the bootstrap results in such a way, that the resulting \(\mathbf{C_g}\) and \(\mathbf{C_n}\) matrices are true covariance matrices i.e. positive semi-definite and serve as good estimators of the true covariance matrices. This is achieved by fitting (least-squares estimate) the covariance estimates of the \(b\) subsets to the closest positive-semidefinite matrices via the Boyden-Fletcher-Goldfarb-Shanno algorithm (BFGS)\citep{Byrd1995}, using the average estimates to initiate the matrices. 

\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=\textwidth]{Chapter1/Figures/LiMMBoScheme.pdf}
	\caption{\textbf{Variance decomposition.} On the left-hand side, the phenotype set of \(P\) traits and \(N\) samples is decomposed into its \(P \times P\) trait-to-trait covariances \(\mathbf{C_g}\) and \(\mathbf{C_n}\), based on the provided genetic sample-to-sample kinship estimate matrix \tmat{R}. The noise sample-to-sample matrix is assumed to be constant (Identity matrix). Standardly, this is done by restricted maximum likelihood estimation of the null model of the mvLMM (Eq.~\ref{eq:vd}). However, this direct variance decomposition (VD) via RML only works for moderate number of phenotype sizes. For higher trait-set sizes, LiMMBo serves as an alternative to the standard RML (right-hand side). Here, the phenotypes' variance components are estimated on \(b\) \(s\)-sized subsets of \(P\) which are subsequently combined into the overall \(P \times P\) covariance matrices \(\mathbf{C_g}\) and \(\mathbf{C_n}\).} 
	 	\label{fig:vd}
\end{figure}


\subsection{Data simulation}
I simulated a number of different phenotype datasets to evaluate LiMMBo in terms of scalability, model calibration and power. The datasets differed in their overall trait size \(P\), the percentage of variance explained by genetics \(h_2\) (sum of fixed and random genetic effects) and the number of different phenotype components simulated to create the final phenotype. The phenotypes were simulated as described in Section~\ref{section:simulation}, based on the parameters and parameter values described in Tables~\ref{tab:pardescription} and \ref{tab:parvalues}. Parameter values were chosen based on \red{...}.

% Table generated by Excel2LaTeX from sheet 'SimulateDataParameters'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Parameters for phenotype simulation.} The genetic and noise effects have both a random and fixed effect. For each, the total variance is the sum of their random and fixed effect variance and has to add to 1. Each fixed and random component has a certain percentage of its variance that is shared across traits, while the rest is independent.}
    \begin{tabular}{clrrr}
    \toprule
          &       & variance explained & shared & independent \\
    \midrule
    \multirow{3}[1]{*}{genetic effects} & total & \(h_2\) &       &  \\
          & fixed & \(h_2^s\) & \(\theta\) & 1-\(\theta\) \\
          & random & \(h_2^g\) & \(\eta\) & 1-\(\eta\) \\
   \addlinespace[1.5ex]
    \multirow{3}[1]{*}{noise effects} & total & (1-\(h_2\)) &       &  \\
          & fixed & (1-\(h_2\))\(\delta\) & \(\gamma\) & 1-\(\gamma\) \\
          & random & (1-\(h_2\))(1-\(\delta\)) & \(\alpha\) & 1-\(\alpha\) \\
    \bottomrule
    \end{tabular}%
  \label{tab:pardescription}%
\end{table}%

% Table generated by Excel2LaTeX from sheet 'SimulateDataValues'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Parameter values of simulated phenotypes for assessing scalability, calibration and power.} The `genotype' parameter specifies the simulated genotype cohort which was used to simulate fixed and random genetic effects (described in Section~\ref{subsection:genotypes}). \(P\) are the different trait set sizes that were simulated. The parameters that follow are described in Table~\ref{tab:pardescription} and specify the variance explained by each of the phenotype components. A variance explained equals zero means that this component was not simulated and corresponding non-applicable variance terms are designated with `-'.} 
    \begin{tabular}{lrr}
    \toprule
          & \multicolumn{2}{c}{Parameter values} \\
\cmidrule{2-3}    Parameter & Power  & Calibration \\
    \midrule
    \multicolumn{1}{c}{\multirow{3}[1]{*}{Genotypes}} & \multicolumn{1}{c}{\multirow{3}[1]{*}{relatedNoPopstructure}} & relatedNoPopstructure \\
          &       & unrelatedNoPopstructure \\
          &       & unrelatedPopstructure \\
          \addlinespace[1.5ex]
    \(P\) & 10, 50, 100 & 10, 20, \(\ldots\), 100 \\
   \addlinespace[1.5ex]
    \(h_2^s\) & 0.48, 0.3, 0.12 & 0 \\
    \(h_2^g\) & 0.32, 0.2, 0.08 & 0.8, 0.5, 0.2 \\
    \(h_2\) & 0.8, 0.5, 0.2 & 0.8, 0.5, 0.2 \\
    (1-\(h_2\))\(\delta\) & 0.08, 0.2, 0.32 & 0 \\
    (1-\(h_2\))(1-\(\delta\)) & 0.12, 0.3, 0.48 & 0.2, 0.5, 0.8 \\
    (1-\(h_2\)) & 0.2, 0.5, 0.8 & 0.2, 0.5, 0.8 \\
    \(\theta\) & 0.6   &  - \\
    \(\eta\) & 0.8   & 0.8 \\
    \(\gamma\) & 0.6   &  - \\
    \(\alpha\) & 0.8   & 0.8 \\
    \bottomrule
    \end{tabular}%
\label{tab:parvalues}%
\end{table}%

\subsection{Scalability of LiMMBo}
The complexitiy of the variance decomposition of the LMM framework that LiMMBo builds on is \(O(N^2 + t(NP^2 + NP^4))\). The second term depends on the overall trait size and describes the complexity of estimating the trait-by-trait covariance matrices $\mathbf{C_g}$ and $\mathbf{C_n}$. By bootstrapping \(s\)-sized samples from the overall trait size, this complexity term changes to \(bt(Ns^2 + Ns^4)\), with the covariance estimation carried out for \(b\) bootstraps. In addition to the estimation of the covariance terms, the overall complexity of LiMMBo also depends on the fitting the BFGS algorithm \(n\) times to the full traitset of size \(P\). LiMMBo makes use of a cholesky decomposition of the matrices to be fitted, resulting in $\frac{1}{2}P(P+1)$ model parameters to be fitted for both $\mathbf{C_g}$ and $\mathbf{C_n}$ .Thus, the overall complexity of LiMMBo is \(O(N^2 + bt(Ns^2 + Ns^4) + n(\frac{1}{2}P(P+1))\), which is the sum of the complexity of the bootstrap variance decompositions and the complexity of fitting the BFGS algorithm.  

In order to assess and compare how LiMMBo scales, I performed VD both with LiMMBo and the standard RML approach on phenotypes with trait sizes ranging from 10 to 100 traits (parameters for phenotype simulation as described in Table~\ref{tab:parvalues}, total of ten simulated datasets per setup). IFor phenotypes with 10 traits, the sampling datasize \(s\) was set to 5, otherwise  \(s=10\).  Fig.~\ref{fig:proctime} shows the overall time taken by the standard RML approach, LiMMBo and its two main components, the bootstrapping and the combination of the bootstrap results. The majority of the run time of LiMMBo is taken by the VD of the bootstrapped subsets, which accounts for at least 85\%  (70 traits) and on average 97\%  of the total runtime. As a comparison, the time taken by the standard RML approach quickly exceeds the time of LiMMBo and becomes unfeasable for more than 30 traits. 

I implemented LiMMBo as a python module where the individual variance decomposition estimations can be distributed across multiple cores on a single computer or multiple nodes on a computing network via the \textit{Parallel Python Software} \cite{PPSoftware}, allowing for the parallelisation of the most time-consuming step. LiMMBo can be freely accessed via \red{my github/PMB github/python repo?}.


\begin{figure}[hbtp]
	\centering
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=1\textwidth]{Chapter1/Figures/proctime.pdf}
	\caption{\textbf{Scalability of LiMMBo  compared to standard RML}. Empirical run times for LiMMBo and the standard RML approach on three simulated datasets per phenotype size, with \(N=1,000\) individuals each and different amount of variance explained by the genetic background signal (0.2, 0.5, 0.8). The boxplots summarise the results across the different set-ups. Lines were fitted for the bootstrapping step (orange): \(n(s^2 + s^4)\); the combination of the bootstrapping (blue): \(\frac{1}{2}P(P+1)\) and their combined runtime (turquoise):  \(n(s^2 + s^4) + \frac{1}{2}P(P+1)\). \(b\): number of boostraps, \(s\): bootstrap size, \(P\): phenotype size. The majority of the runtime is required for the bootstrapping. The runtime for the standard RML results (red) are only depicted up to \(P=40\) when they already exceed the runtimes for \(P=100\) in the LiMMBo approach. The complexity of the RML and LiMMBo algorithm (or their reseptive parts) were used for fitting the lines to the observed data: sum across boostraps: \(bt(Ns^2 + Ns^4)\); combining of bootstraps: \(n(\frac{1}{2}P(P+1))\). total runtime of LiMMBo: \(O(N^2 + bt(Ns^2 + Ns^4) + n(\frac{1}{2}P(P+1))\); RML: \(O(N^2 + t(NP^2 + NP^4))\). \red{Need to doublecheck with Paolo, since the line fit for the RML is perfect when assuming \(O(N^2 + t(NP^2 + NP^6))\) but doesn't work when fitting to the power of 4.}}
	 	\label{fig:proctime}
\end{figure}

\subsection{LiMMBo yields covariance estimates consistent with RML estimates for moderate trait numbers}
I evaluated the suitability of LiMMBo for covariance estimation of \(\mathbf{C_g}\) and \(\mathbf{C_n}\) on simulated datasets with different strength of background genetic effects. I choose to simulate traitset sizes between ten and thirty traits, as this is the regime where the standard RML approach is still feasible and allows for a comparison of the two methods. I simulated phenotype sets composed of background genetic effects $\mathbf{G}$ and noise effects $\mathbf{\Psi}$ only, omitting any specific genetic effects (additional parameters as described in Table~\ref{tab:parvalues}) and estimated these variance components subsequently with LiMMBo and standard RML. Variance estimation on simulated datasets allows for the comparison of the estimated covariance matrices to the true covariance matrices based on which the phenotypes were simulated. By computing the root mean squared deviation (RMSD) between the true and estimated covariance matrices from both methods, I obtain a measure that is directly comparable and is independent of the traitset size: 
\begin{equation}
\text{RMSD}=\sqrt{\frac{\sum_{t=1}^n (C_{\text{true}} - C_{\text{estimate}})^2}{n}}
\label{eq:rmsd}
\end{equation}
Fig.~\ref{fig:covsimilarity} shows the comparison of both standard RML and LiMMBo-derived covariance matrices compared to the simulated, true covariance matrices. Both methods provide consistent estimates across trait sizes with little difference between the methods.

\begin{figure}[hbtp]
	\centering	
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.6\textwidth]{Chapter1/Figures/covarianceSummary.pdf}\\
	\caption{\textbf{Comparison of trait-by-trait covariance estimates derived from standard RML and LiMMBo.} For moderate trait set sizes ranging from 10 to 30 traits, phenotypes with different percentage of variance explained by genetics were simulated. The genetic and noise trait-to-trait covariance matrices \(\mathbf{C_g}\) and \(\mathbf{C_n}\) were then estimated both via LiMMBo and standard RML. These estimates were compared to the true (simulated) covariance matrix by computing their root mean squated deviation (RMSD; equation~\ref{eq:rmsd}). The boxplots summarize the RMSD across different variance levels. Across all trait sizes, LiMMBo yields covariance estimates with RMSD consistent to the RML approach.}
	  \label{fig:covsimilarity}%
\end{figure}

\subsection{mtGWAS with LiMMBo-derived covariance matrices are well calibrated across all phenotype sizes}
One key aspect in statistical method development is to ensure that the method is well-calibrated under the null model. Apart from gaining knowledge about the genetic and noise trait-to-trait covariance structure of a phenotype, variance decompostion into different random effect components yields estimates that can be supplied as known parameters to approximate mvLMM methods and multi-trait GWAS (mtGWAS). As introduced by Jiang and colleagues \citeyear{Jiang1995} and adapted by Korte and colleagues \citeyear{Korte2012}, there are different model designs for mvLMM, depending on the underlying biological hypothesis regarding the effect of the genetic variant. In the most simple case, one can test if the genetic variant has an effect on any of the traits \(P\) (any effect test) i.e. the effect size of the fixed effect is unequal to zero for at least one trait : \(H_\text{A}: \mat{\beta} \ne \mat{0}_P\).  In this \(P\)-degrees of freedom (df) test, the corresponding null hypthesis of no association is that the effect size of the fixed effect is equal to zero: \(H_0:\mat{\beta}  = \mat{0}_P\). In the common effect model,  the variant has the same effect size across all traits (\(\mat{\beta}  = \mat{1}_P\beta)\) and is tested for significance in a 1 df model versus the null hypothesis of no association (\(\mat{\beta}  = \mat{0}_P\)). A more complicated model allows to test for specific effects of the variant on a given trait \(p\). This can be tested with a 1 df test where a model containing a common effect across all traits and a specific effect for trait \(p\) is compared against the common effect model. Both, for the calibration and power analysis I chose to apply an any effect test.

In order to test if LiMMBo-derived covariance estimates yield well calibrated test statistics, I simulated phenotype sets composed of random genetic and noise effects only with 10, 20, 30, 50 and 100 traits and parameters Table~\ref{tab:parvalues}. For trait sizes of up to thirty traits, I compared the calibration of mtGWAS for LiMMBo- and RML-derived covariance matrices. As shown in Figure~\ref{fig:calibration}, both methods show sufficient calibration across all phenotype sizes and variance explained by genetics. 

For higher trait sizes, I compared the calibration of mtGWAS using a mvLMM to a simple multi-variate linear model (mvLM, Eq.~\ref{eq:mvLM}). The mvLM does not require the variance decompostion into different random effects, i.e. avoids the computational bottleneck, but simply uses pricipal components of the genotypes as fixed effects to adjust for population structure. I assessed the calibration of the mvLM and mvLMM in mtGWAS by testing all simulated SNPs against the different phenotypes and subsequently estimating the type I error rates. For each mtGWAS, I counted the number of tests that exceeded a given threshold divided by the overall number of tests conducted, hence the number of genome-wide SNPs. I tested for calibration at two  significance thresholds, 5E-5 and 5E-8.  The latter is the common GWAS significance threshold, which is based on the number of independent variants in the genome. It was first proposed in the scope of the HapMap project \citeyear{HapMap2005} who found about 150 independent, common variants per 500~kb region. At a significance threshold of \(p < 0.05\) and extrapolated to the genome of \(\sim 3.3\)~Gb, by Bonferroni correction this yields  \(  0.05/(\frac{150}{500\text{kb}} \times 3.3 \text{Gb}= 5.05 \times 10^-8\). This estimate was later confirmed in a study using different methods for estimating the number of independent variants \citep{Fadista2016}.  Tab.~\ref{tab:calibration} shows the type I error estimates for both mtGWAS approaches. The mvLMM performs well across all trait sizes and thresholds of significance. In contrast, the mvLM is poorly calibrated and clearly demonstrates the difficulty of adjusting for population structure via fixed effects in highly structured populations.  
 
\begin{figure}[hbtp]
	\centering	
	\includegraphics[trim = 0mm 0mm 0mm 0mm, clip, width=0.6\textwidth]{Chapter1/Figures/calibrationSummaryQQModerate}\\
	\caption{\textbf{Calibration of mtGWAS based on covariance estimates from standard RML and LiMMBo.} Phenotypes  ranging from 10 to 30 traits with different percentage of variance explained by genetics were simulated. The genetic and noise trait-to-trait covariance matrices \(\mathbf{C_g}\) and \(\mathbf{C_n}\) were then estimated both via LiMMBo and standard RML and used as estimates for mvLMM across all genome-wide SNPs. Both methods show a unifrom distribution of observed p-values across all phenotype sizes and variance explained by genetics }
	  \label{fig:calibration}%
\end{figure}


% Table generated by Excel2LaTeX from sheet 'CalibrationSummary'
\begin{table}[htbp]
  \centering
  \caption{\textbf{Type I error estimates for mtGWAS.} Type I errors estimates for mvLMM and mvLM across all genome-wide SNPs for three trait set sizes assesed at two different levels of significance.  For the mvLMM,  covariance estimates were derived via LiMMBo. In the mvLM, population structure was adjusted for via the first ten PCs of the genotype data. The mvLMM controls well for Type I errors at both thresholds, while the mvLM leads to inflated test statistics.}
    \begin{tabular}{crrr}
    \toprule
          &       & \multicolumn{2}{c}{Type I Error estimates} \\
\cmidrule{3-4}    \multicolumn{1}{r}{Traits} & Significance level & mvLM & mvLMM \\
    \midrule
    \multirow{2}[1]{*}{10} & 5.00E-05 & 2.17E-03 & 4.51E-05 \\
          & 5.00E-08 & 2.32E-05 & \(<\)5E-08 \\
             \addlinespace[0.5ex]
    \multirow{2}[0]{*}{50} & 5.00E-05 & 1.09E-02 & 1.93E-05 \\
          & 5.00E-08 & 2.08E-04 & \(<\)5E-08 \\
             \addlinespace[0.5ex]
    \multirow{2}[1]{*}{100} & 5.00E-05 & 3.17E-02 & 2.28E-05 \\
          & 5.00E-08 & 1.01E-03 & 4.56E-08 \\
    \bottomrule
    \end{tabular}%
  \label{tab:calibration}%
\end{table}%





\subsection{Multi-trait genotype to phenotype mapping increases power for high-dimensional phenotypes}
